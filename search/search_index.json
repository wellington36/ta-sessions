{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Monitorias Este website foi desenvolvido para concentrar as informa\u00e7\u00f5es sobre as monitorias que dei ao longo da minha vida acad\u00eamica. Em particular durante a Gradua\u00e7\u00e3o na Escola de Matem\u00e1tica Aplicada na Funda\u00e7\u00e3o Getulio Vargas (EMAp/FGV). Eu espero que a informa\u00e7\u00e3o aqui contida seja de interesse! Para sugest\u00f5es e altera\u00e7\u00f5es, voc\u00ea pode usar o Issues do Github. T\u00f3picos abordados \u00c1lgebra Linear (2019.2) Professor Eduardo Wagner Equa\u00e7\u00f5es Diferenciais Ordin\u00e1rias (2020.1) Professora Maria Izabel Camacho Infer\u00eancia \u00e0 Estat\u00edstica (2020.2) Professor Luiz Max de Carvalho Curvas e Superf\u00edcies (2021.1) Professora Asla Medeiros e S\u00e1 Introdu\u00e7\u00e3o \u00e0 An\u00e1lise Num\u00e9rica (2021.2) Professor Hugo A. de la Cruz Cancino Equa\u00e7\u00f5es Diferenciais Parciais (2021.2) Professor Moacyr Alvim Horta Barbosa da Silva Refer\u00eancias interessantes de matem\u00e1tica SageMath : software matem\u00e1tico que facilita c\u00e1lculos. \u00c9 uma possibilidade alternativa ao WolframAlpha , por\u00e9m com c\u00f3digo em Python. Manin : ferramenta para criar v\u00eddeos matem\u00e1ticos explicativos com anima\u00e7\u00e3o program\u00e1tica, uma biblioteca para Python. Seeing Theory : visualiza\u00e7\u00e3o de conceitos de probabilidade e estat\u00edstica. Softwares de Geometria Alg\u00e9brica : lista de softwares relacionados com Geometria Alg\u00e9brica.","title":"Home"},{"location":"#monitorias","text":"Este website foi desenvolvido para concentrar as informa\u00e7\u00f5es sobre as monitorias que dei ao longo da minha vida acad\u00eamica. Em particular durante a Gradua\u00e7\u00e3o na Escola de Matem\u00e1tica Aplicada na Funda\u00e7\u00e3o Getulio Vargas (EMAp/FGV). Eu espero que a informa\u00e7\u00e3o aqui contida seja de interesse! Para sugest\u00f5es e altera\u00e7\u00f5es, voc\u00ea pode usar o Issues do Github.","title":"Monitorias"},{"location":"#topicos-abordados","text":"\u00c1lgebra Linear (2019.2) Professor Eduardo Wagner Equa\u00e7\u00f5es Diferenciais Ordin\u00e1rias (2020.1) Professora Maria Izabel Camacho Infer\u00eancia \u00e0 Estat\u00edstica (2020.2) Professor Luiz Max de Carvalho Curvas e Superf\u00edcies (2021.1) Professora Asla Medeiros e S\u00e1 Introdu\u00e7\u00e3o \u00e0 An\u00e1lise Num\u00e9rica (2021.2) Professor Hugo A. de la Cruz Cancino Equa\u00e7\u00f5es Diferenciais Parciais (2021.2) Professor Moacyr Alvim Horta Barbosa da Silva","title":"T\u00f3picos abordados"},{"location":"#referencias-interessantes-de-matematica","text":"SageMath : software matem\u00e1tico que facilita c\u00e1lculos. \u00c9 uma possibilidade alternativa ao WolframAlpha , por\u00e9m com c\u00f3digo em Python. Manin : ferramenta para criar v\u00eddeos matem\u00e1ticos explicativos com anima\u00e7\u00e3o program\u00e1tica, uma biblioteca para Python. Seeing Theory : visualiza\u00e7\u00e3o de conceitos de probabilidade e estat\u00edstica. Softwares de Geometria Alg\u00e9brica : lista de softwares relacionados com Geometria Alg\u00e9brica.","title":"Refer\u00eancias interessantes de matem\u00e1tica"},{"location":"alglin/info/","text":"Informa\u00e7\u00f5es Gerais Monitoria de \u00c1lgebra Linear correspondente ao per\u00edodo de 2019.2. Os temas abordados s\u00e3o: Matrizes Sistemas lineares Espa\u00e7os vetoriais e com produto interno Bases Transforma\u00e7\u00f5es lineares N\u00facleo e imagem de uma transforma\u00e7\u00e3o Autovalores e autovetores Proje\u00e7\u00f5es Monitorias Monitorias Monitoria 4 Monitoria 6 Monitoria 7 Monitorias 10 e 11 Monitorias 12 e 13 Decomposi\u00e7\u00e3o LU","title":"\u00c1lgebra Linear"},{"location":"alglin/info/#informacoes-gerais","text":"Monitoria de \u00c1lgebra Linear correspondente ao per\u00edodo de 2019.2. Os temas abordados s\u00e3o: Matrizes Sistemas lineares Espa\u00e7os vetoriais e com produto interno Bases Transforma\u00e7\u00f5es lineares N\u00facleo e imagem de uma transforma\u00e7\u00e3o Autovalores e autovetores Proje\u00e7\u00f5es","title":"Informa\u00e7\u00f5es Gerais"},{"location":"alglin/info/#monitorias","text":"Monitorias Monitoria 4 Monitoria 6 Monitoria 7 Monitorias 10 e 11 Monitorias 12 e 13 Decomposi\u00e7\u00e3o LU","title":"Monitorias"},{"location":"alglin/files/decompositionLU/","text":"Decomposi\u00e7\u00e3o LU Fatoriza\u00e7\u00e3o de uma matriz A como um produto P\\cdot A = L\\cdot U , onde a primeira matrix \u00e9 triangular inferior (lower), enquanto a segunda \u00e9 triangular superior (upper). A matriz P \u00e9 uma matriz de permuta\u00e7\u00e3o que garante a exist\u00eancia dessa fatoriza\u00e7\u00e3o. import scipy.linalg A = scipy . array ([ [ 7 , 3 , - 1 , 2 ], [ 3 , 8 , 1 , - 4 ], [ - 1 , 1 , 4 , - 1 ], [ 2 , - 4 , - 1 , 6 ] ]) print ( A ) [[ 7 3 -1 2] [ 3 8 1 -4] [-1 1 4 -1] [ 2 -4 -1 6]] P , L , U = scipy . linalg . lu ( A ) print ( P ), print ( L ), print ( U ) [[1. 0. 0. 0.] [0. 1. 0. 0.] [0. 0. 1. 0.] [0. 0. 0. 1.]] [[ 1. 0. 0. 0. ] [ 0.42857143 1. 0. 0. ] [-0.14285714 0.21276596 1. 0. ] [ 0.28571429 -0.72340426 0.08982036 1. ]] [[ 7. 3. -1. 2. ] [ 0. 6.71428571 1.42857143 -4.85714286] [ 0. 0. 3.55319149 0.31914894] [ 0. 0. 0. 1.88622754]] (None, None, None)","title":"Decomposi\u00e7\u00e3o LU"},{"location":"alglin/files/decompositionLU/#decomposicao-lu","text":"Fatoriza\u00e7\u00e3o de uma matriz A como um produto P\\cdot A = L\\cdot U , onde a primeira matrix \u00e9 triangular inferior (lower), enquanto a segunda \u00e9 triangular superior (upper). A matriz P \u00e9 uma matriz de permuta\u00e7\u00e3o que garante a exist\u00eancia dessa fatoriza\u00e7\u00e3o. import scipy.linalg A = scipy . array ([ [ 7 , 3 , - 1 , 2 ], [ 3 , 8 , 1 , - 4 ], [ - 1 , 1 , 4 , - 1 ], [ 2 , - 4 , - 1 , 6 ] ]) print ( A ) [[ 7 3 -1 2] [ 3 8 1 -4] [-1 1 4 -1] [ 2 -4 -1 6]] P , L , U = scipy . linalg . lu ( A ) print ( P ), print ( L ), print ( U ) [[1. 0. 0. 0.] [0. 1. 0. 0.] [0. 0. 1. 0.] [0. 0. 0. 1.]] [[ 1. 0. 0. 0. ] [ 0.42857143 1. 0. 0. ] [-0.14285714 0.21276596 1. 0. ] [ 0.28571429 -0.72340426 0.08982036 1. ]] [[ 7. 3. -1. 2. ] [ 0. 6.71428571 1.42857143 -4.85714286] [ 0. 0. 3.55319149 0.31914894] [ 0. 0. 0. 1.88622754]] (None, None, None)","title":"Decomposi\u00e7\u00e3o LU"},{"location":"alglin/files/monitoria10/","text":"Monitorias 10 e 11 Defini\u00e7\u00f5es e Teoremas Lembrando: Uma transforma\u00e7\u00e3o linear T: \\mathbb{R}^n \\to \\mathbb{R}^m fica inteiramente determinada por uma matriz A = [a_{ij}] \\in M(m \\times n) . Os vetores coluna dessa matriz s\u00e3o as imagens A \\cdot e_j dos vetores da base can\u00f4nica. Definimos A como matriz de transforma\u00e7\u00e3o. Assim, A \\cdot e_j = \\sum_{i=1}^m a_{ij}e_i (j=1,...,n) , onde e_i \\in \\mathbb{R}^m . Simetrias: Matrizes de tranforma\u00e7\u00e3o referentes \u00e0 simetria em rela\u00e7\u00e3o aos eixos x e y, e em rela\u00e7\u00e3o \u00e0 origem, respectivamente: S_x = \\left[ \\begin{array}{cc} 1 & 0 \\\\ 0 & -1 \\end{array} \\right] S_y = \\left[ \\begin{array}{cc} -1 & 0 \\\\ 0 & 1 \\end{array} \\right] S_o = \\left[ \\begin{array}{cc} -1 & 0 \\\\ 0 & -1 \\end{array} \\right] Dilata\u00e7\u00f5es: Basta multiplicar uma coluna que se quer dilatar por r . Podemos chamar r de coeficiente de dilata\u00e7\u00e3o. Rota\u00e7\u00e3o: Para montar essa matriz, basta conhecer a transforma\u00e7\u00e3o dos vetores (1,0) e (0,1) . R_{\\theta} = \\left[ \\begin{array}{cc} \\cos \\theta & - \\sin \\theta \\\\ \\sin \\theta & \\cos \\theta \\end{array} \\right] A rota\u00e7\u00e3o tem algumas propriedades: R_{\\theta}^{-1} = R_{-\\theta} R_{\\alpha}R_{\\beta} = R_{\\alpha + \\beta} (R_{\\theta})^n = R_{n\\theta} Proje\u00e7\u00f5es: Podemos considerar a transforma\u00e7\u00e3o que projeta os vetores sobre a reta y = ax . P = \\frac{1}{1+a^2} \\left[ \\begin{array}{cc} 1 & a \\\\ a & a^2 \\end{array} \\right] Se quisermos que a proje\u00e7\u00e3o sobre um eixo e paralelo a uma reta, temos que P_p = \\left[ \\begin{array}{cc} 1 & -\\frac{1}{a} \\\\ 0 & 0 \\end{array} \\right] N\u00facleo de A : N(A) = \\{v \\in E | Av = 0\\} . \u00c9 o espa\u00e7o anulado da matriz A . Imagem de A : Im(A) = \\{Av | v \\in E\\} \\implies \\exists v \\in E; Av = w \\implies w \\in Im(A) . Notemos que posto(A) = dim~Im(A) = dim~col(A) . Isto ocorre, pois w \\in Im(A) \u00e9 combina\u00e7\u00e3o linear das colunas da matriz A . Transforma\u00e7\u00e3o Injetiva: A: E \\to F \u00e9 injetiva se \\forall v, v', v \\neq v' \\implies Av \\neq Av' . Uma transforma\u00e7\u00e3o \u00e9 injetiva se, e s\u00f3 se, transforma vetores LI em vetores LI. Para essa demonstra\u00e7\u00e3o, \u00e9 necess\u00e1rio mostrar que uma transforma\u00e7\u00e3o \u00e9 injetiva se, e s\u00f3 se, seu n\u00facleo possui apenas o vetor nulo. Transforma\u00e7\u00e3o Sobrejetiva: Ocorre quando Im(A) = F , onde F \u00e9 o espa\u00e7o vetorial contradom\u00ednio. Teorema do N\u00facleo e da Imagem: Como dim~Im(A) = posto(A) , podemos usar no teorema do posto. Podemos alterar n para dim~E , sendo E o dom\u00ednio da transforma\u00e7\u00e3o. Laplace: Escolhe-se uma linha uma coluna e para cada elemento, calcula-se o seu cofator. A_{ij} = (-1)^{i+j}D_{ij} . Propriedades Importantes: det(A) = det(A^{T}) ; trocar duas linhas ou colunas inverte o sinal do determinante; duas linhas proporcionais indica determinante 0; multiplicar uma linha por \\alpha implicar\u00e1 multiplicar o determinante pelo mesmo fator; determinante do produto de matrizes \u00e9 o produto dos determinantes; o determinante de uma matriz com a opera\u00e7\u00e3o de somar com m\u00faltiplo de outra linha \u00e9 id\u00eantico; determinante da inversa \u00e9 o inverso do determinante Lembretes para exerc\u00edcios: Para calcular uma matriz de tranforma\u00e7\u00e3o, precisamos apenas saber a transforma\u00e7\u00e3o linear de uma base do dom\u00ednio. Com essa transforma\u00e7\u00e3o, precisamos obter a transforma\u00e7\u00e3o da base can\u00f4nica, para que a matriz seja constr\u00edda nessa base. Essa matriz de tranforma\u00e7\u00e3o tamb\u00e9m pode ser obtida por T = AP^{-1} , onde P tem como colunas os vetores da base, e A os vetores da base ap\u00f3s a transforma\u00e7\u00e3o. Para mostrar injetividade, podemos usar a contrapositiva da defini\u00e7\u00e3o. Voc\u00ea sabe encontrar uma base para o n\u00facleo e uma base para a imagem de uma transforma\u00e7\u00e3o? A base da imagem \u00e9 basicamente a base para o espa\u00e7o coluna (consegue enxergar o porqu\u00ea? Tente representar um vetor da imagem como combina\u00e7\u00e3o linear das colunas. E a base para o n\u00facleo? Exerc\u00edcios: Reflex\u00e3o em torno de uma reta: Seja S: \\mathbb{R}^2 \\to \\mathbb{R}^2 a transforma\u00e7\u00e3o que reflete um veotr em torno da reta y = ax . Assim, a reta \u00e9 a bissetriz do \u00e2ngulo entre v e Sv e \u00e9 perpendicular \u00e0 reta que liga v a Sv . Solu\u00e7\u00e3o: Seja P a matriz de proje\u00e7\u00e3o. Projetamos ortogonalmente v sobre a reta y = ax . Assim, teremos que v + Sv = 2Pv \\implies I + S = 2P \\implies S = 2P - I . Outra forma \u00e9 fazer as tranforma\u00e7\u00f5es dos vetores da base can\u00f4nica. Considere 5 l\u00e2mpadas, cada uma com um bot\u00e3o. Cada bot\u00e3o muda o estado da l\u00e2mpada e das vizinhas. Todas est\u00e3o apagadas. Como deixar a primeira, terceira e quinta acesas. Encontre os n\u00fameros a,b,c,d de modo que o operador A: \\mathbb{R}^2 \\to \\mathbb{R}^2 , dado por A(x,y) = (ax + by, cx + dy) tenha como n\u00facleo a reta y = 3x . A transforma\u00e7\u00e3o A: \\mathbb{R} \\to \\mathbb{R}^n; A(x) = (x,2x,...,nx) \u00e9 uma transforma\u00e7\u00e3o injetiva? E B(x,y) = (x + 2y, x + y, x - y) ? Considere uma transforma\u00e7\u00e3o A: E \\to F na base can\u00f4nica. Considere V uma base de vetores de E . Determine a matriz de transforma\u00e7\u00e3o A' nessa base. Ou seja, se Av = w \\to A'v_V = w_V . Ache uma transforma\u00e7\u00e3o A: \\mathbb{R}^2 \\to \\mathbb{R}^2 tal que a imagem e o n\u00facleo sejam o eixo x.","title":"Monitorias 10 e 11"},{"location":"alglin/files/monitoria10/#monitorias-10-e-11","text":"","title":"Monitorias 10 e 11"},{"location":"alglin/files/monitoria10/#definicoes-e-teoremas","text":"Lembrando: Uma transforma\u00e7\u00e3o linear T: \\mathbb{R}^n \\to \\mathbb{R}^m fica inteiramente determinada por uma matriz A = [a_{ij}] \\in M(m \\times n) . Os vetores coluna dessa matriz s\u00e3o as imagens A \\cdot e_j dos vetores da base can\u00f4nica. Definimos A como matriz de transforma\u00e7\u00e3o. Assim, A \\cdot e_j = \\sum_{i=1}^m a_{ij}e_i (j=1,...,n) , onde e_i \\in \\mathbb{R}^m . Simetrias: Matrizes de tranforma\u00e7\u00e3o referentes \u00e0 simetria em rela\u00e7\u00e3o aos eixos x e y, e em rela\u00e7\u00e3o \u00e0 origem, respectivamente: S_x = \\left[ \\begin{array}{cc} 1 & 0 \\\\ 0 & -1 \\end{array} \\right] S_y = \\left[ \\begin{array}{cc} -1 & 0 \\\\ 0 & 1 \\end{array} \\right] S_o = \\left[ \\begin{array}{cc} -1 & 0 \\\\ 0 & -1 \\end{array} \\right] Dilata\u00e7\u00f5es: Basta multiplicar uma coluna que se quer dilatar por r . Podemos chamar r de coeficiente de dilata\u00e7\u00e3o. Rota\u00e7\u00e3o: Para montar essa matriz, basta conhecer a transforma\u00e7\u00e3o dos vetores (1,0) e (0,1) . R_{\\theta} = \\left[ \\begin{array}{cc} \\cos \\theta & - \\sin \\theta \\\\ \\sin \\theta & \\cos \\theta \\end{array} \\right] A rota\u00e7\u00e3o tem algumas propriedades: R_{\\theta}^{-1} = R_{-\\theta} R_{\\alpha}R_{\\beta} = R_{\\alpha + \\beta} (R_{\\theta})^n = R_{n\\theta} Proje\u00e7\u00f5es: Podemos considerar a transforma\u00e7\u00e3o que projeta os vetores sobre a reta y = ax . P = \\frac{1}{1+a^2} \\left[ \\begin{array}{cc} 1 & a \\\\ a & a^2 \\end{array} \\right] Se quisermos que a proje\u00e7\u00e3o sobre um eixo e paralelo a uma reta, temos que P_p = \\left[ \\begin{array}{cc} 1 & -\\frac{1}{a} \\\\ 0 & 0 \\end{array} \\right] N\u00facleo de A : N(A) = \\{v \\in E | Av = 0\\} . \u00c9 o espa\u00e7o anulado da matriz A . Imagem de A : Im(A) = \\{Av | v \\in E\\} \\implies \\exists v \\in E; Av = w \\implies w \\in Im(A) . Notemos que posto(A) = dim~Im(A) = dim~col(A) . Isto ocorre, pois w \\in Im(A) \u00e9 combina\u00e7\u00e3o linear das colunas da matriz A . Transforma\u00e7\u00e3o Injetiva: A: E \\to F \u00e9 injetiva se \\forall v, v', v \\neq v' \\implies Av \\neq Av' . Uma transforma\u00e7\u00e3o \u00e9 injetiva se, e s\u00f3 se, transforma vetores LI em vetores LI. Para essa demonstra\u00e7\u00e3o, \u00e9 necess\u00e1rio mostrar que uma transforma\u00e7\u00e3o \u00e9 injetiva se, e s\u00f3 se, seu n\u00facleo possui apenas o vetor nulo. Transforma\u00e7\u00e3o Sobrejetiva: Ocorre quando Im(A) = F , onde F \u00e9 o espa\u00e7o vetorial contradom\u00ednio. Teorema do N\u00facleo e da Imagem: Como dim~Im(A) = posto(A) , podemos usar no teorema do posto. Podemos alterar n para dim~E , sendo E o dom\u00ednio da transforma\u00e7\u00e3o. Laplace: Escolhe-se uma linha uma coluna e para cada elemento, calcula-se o seu cofator. A_{ij} = (-1)^{i+j}D_{ij} .","title":"Defini\u00e7\u00f5es e Teoremas"},{"location":"alglin/files/monitoria10/#propriedades-importantes","text":"det(A) = det(A^{T}) ; trocar duas linhas ou colunas inverte o sinal do determinante; duas linhas proporcionais indica determinante 0; multiplicar uma linha por \\alpha implicar\u00e1 multiplicar o determinante pelo mesmo fator; determinante do produto de matrizes \u00e9 o produto dos determinantes; o determinante de uma matriz com a opera\u00e7\u00e3o de somar com m\u00faltiplo de outra linha \u00e9 id\u00eantico; determinante da inversa \u00e9 o inverso do determinante","title":"Propriedades Importantes:"},{"location":"alglin/files/monitoria10/#lembretes-para-exercicios","text":"Para calcular uma matriz de tranforma\u00e7\u00e3o, precisamos apenas saber a transforma\u00e7\u00e3o linear de uma base do dom\u00ednio. Com essa transforma\u00e7\u00e3o, precisamos obter a transforma\u00e7\u00e3o da base can\u00f4nica, para que a matriz seja constr\u00edda nessa base. Essa matriz de tranforma\u00e7\u00e3o tamb\u00e9m pode ser obtida por T = AP^{-1} , onde P tem como colunas os vetores da base, e A os vetores da base ap\u00f3s a transforma\u00e7\u00e3o. Para mostrar injetividade, podemos usar a contrapositiva da defini\u00e7\u00e3o. Voc\u00ea sabe encontrar uma base para o n\u00facleo e uma base para a imagem de uma transforma\u00e7\u00e3o? A base da imagem \u00e9 basicamente a base para o espa\u00e7o coluna (consegue enxergar o porqu\u00ea? Tente representar um vetor da imagem como combina\u00e7\u00e3o linear das colunas. E a base para o n\u00facleo?","title":"Lembretes para exerc\u00edcios:"},{"location":"alglin/files/monitoria10/#exercicios","text":"Reflex\u00e3o em torno de uma reta: Seja S: \\mathbb{R}^2 \\to \\mathbb{R}^2 a transforma\u00e7\u00e3o que reflete um veotr em torno da reta y = ax . Assim, a reta \u00e9 a bissetriz do \u00e2ngulo entre v e Sv e \u00e9 perpendicular \u00e0 reta que liga v a Sv . Solu\u00e7\u00e3o: Seja P a matriz de proje\u00e7\u00e3o. Projetamos ortogonalmente v sobre a reta y = ax . Assim, teremos que v + Sv = 2Pv \\implies I + S = 2P \\implies S = 2P - I . Outra forma \u00e9 fazer as tranforma\u00e7\u00f5es dos vetores da base can\u00f4nica. Considere 5 l\u00e2mpadas, cada uma com um bot\u00e3o. Cada bot\u00e3o muda o estado da l\u00e2mpada e das vizinhas. Todas est\u00e3o apagadas. Como deixar a primeira, terceira e quinta acesas. Encontre os n\u00fameros a,b,c,d de modo que o operador A: \\mathbb{R}^2 \\to \\mathbb{R}^2 , dado por A(x,y) = (ax + by, cx + dy) tenha como n\u00facleo a reta y = 3x . A transforma\u00e7\u00e3o A: \\mathbb{R} \\to \\mathbb{R}^n; A(x) = (x,2x,...,nx) \u00e9 uma transforma\u00e7\u00e3o injetiva? E B(x,y) = (x + 2y, x + y, x - y) ? Considere uma transforma\u00e7\u00e3o A: E \\to F na base can\u00f4nica. Considere V uma base de vetores de E . Determine a matriz de transforma\u00e7\u00e3o A' nessa base. Ou seja, se Av = w \\to A'v_V = w_V . Ache uma transforma\u00e7\u00e3o A: \\mathbb{R}^2 \\to \\mathbb{R}^2 tal que a imagem e o n\u00facleo sejam o eixo x.","title":"Exerc\u00edcios:"},{"location":"alglin/files/monitoria12/","text":"Monitorias 12 e 13 Autovalores e Autovetores Defini\u00e7\u00e3o: Autovetor \u00e9 um vetor ( \\neq \\vec{0} ) que tem como imagem de uma transforma\u00e7\u00e3o linear um vetor proporcional. A propor\u00e7\u00e3o \u00e9 chamada de autovalor. Polin\u00f4mio Caracter\u00edstico: Polin\u00f4mio cujas ra\u00edzes s\u00e3o os autovalores de uma transforma\u00e7\u00e3o linear. Subespa\u00e7o invariante: Tamb\u00e9m conhecido como auto-espa\u00e7o, \u00e9 formado pela combina\u00e7\u00e3o dos autovetores associados ao mesmo autovalor. Teorema 1: Seja A um operador linear, \\lambda um autovalor e v um autovetor. Av = \\lambda v \\implies A^nv = \\lambda^n v . Teorema 2: A autovalores diferentes do mesmo operador correspondem autovetores linearmente independentes. Mudan\u00e7a de Base Considere as seguintes bases: E = \\{e_1, ..., e_n\\} U = \\{u_1, ..., u_n\\} V = \\{v_1,...,v_n\\} Considere w = (x_1,...,x_n) . Isso significa que w \u00e9 escrito como uma combina\u00e7\u00e3o linear dos vetores da base E , can\u00f4nica, e os coeficientes s\u00e3o x_1, ..., x_n . Imagine que queiramos escrever na base U . Para isso, basta encontrarmos os coeficientes de cada vetor da base U . Para isso, basta resolver o sistema linear onde cada vetor de U \u00e9 uma coluna e o vetor restultado \u00e9 o vetor na base can\u00f4nica. Assim, a matriz formada pelos vetores da base U formam uma matriz que transforma vetores da base U em vetores da base can\u00f4nica. A inversa faz o processo contr\u00e1rio. Se quis\u00e9ssemos mudar da base E para a base U sem o uso da inversa, s\u00f3 precisamos saber a transforma\u00e7\u00e3o dos vetores da base can\u00f4nica. Para fazer a transforma\u00e7\u00e3o de uma base em outra, basta transformarmos na can\u00f4nica como interm\u00e9dio. Matrizes Semelhantes e Diagonaliza\u00e7\u00e3o Defini\u00e7\u00e3o: Duas matrizes s\u00e3o semelhantes se existe P invert\u00edvel tal que B = P^{-1}AP (AP = PB) , que tem o mesmo polin\u00f4mio caracter\u00edstico e o mesmo determinante. Diagonaliza\u00e7\u00e3o: Uma matriz \u00e9 diagonaliz\u00e1vel se existe uma matriz semelhante que seja diagonal. A \u00e9 diagon\u00e1liz\u00e1vel se, e s\u00f3 se, tiver n autovetores LI. Nesse caso P \u00e9 a matriz cujas colunas s\u00e3o os autovetores de A e D os autovalores correspontes. Observe que P^{-1}AP = D , logo para transformar um vetor na base V em outro na base V correspondente a imagem desse vetor na base can\u00f4nica da matriz A , basta usar a transforma\u00e7\u00e3o D . Recorr\u00eancias Podemos utilizar matrizes para representar recorr\u00eancias. Um exemplo famoso \u00e9 a sequ\u00eancia de Fibonight. Produto Interno Seja E um espa\u00e7o vetorial e u, v \\in E . Define-se produto interno com <u,v> com um n\u00famero real que satisfaz as seguintes condi\u00e7\u00f5es: <u,v + v'> = <u,v> + <u,v'> e <u + u',v> = <u,v> + <u',v> <u,v> = <v,u> <\\alpha u,v> = \\alpha<u,v> e <u,\\alpha v> = \\alpha<u,v> Se u \\neq 0 , <u,u> > 0 Proje\u00e7\u00f5es J\u00e1 sabemos que p = (\\frac{u\\cdot v}{v\\cdot v})v \u00e9 a proje\u00e7\u00e3o do vetor u sobre a reta gerada por v . Quando queremos projetar um vetor v sobre um hiperplano \\pi , com vetor nornal n , temos que v = p + tn , onde t \u00e9 uma constante. Logo, podemos montar um sistema com n equa\u00e7\u00f5es. Ortogonalidade Se <u,v> = 0 , dizemos que u e v s\u00e3o ortogonais. Um conjunto \u00e9 dito ortogonal se a cada par de vetores, eles s\u00e3o ortogonais. Ele ser\u00e1 ortonormal quando seus vetores ortogonais forem normalizados. Note que se X \u00e9 um conjunto ortogonal, ent\u00e3o \u00e9 LI. Ortogonaliza\u00e7\u00e3o de Gram-Schimidt Lembre-se: Defina um vetor inicial e utilize a ideia de que cada outro vetor ser\u00e1 subtra\u00eddo das proje\u00e7\u00f5es do vetor calculado previamente. Proje\u00e7\u00e3o de um vetor sobre um subespa\u00e7o Seja W um subespa\u00e7o de E e \\alpha uma base ortogonal desse subespa\u00e7o. p = proj_W v = \\sum_{i=1}^n proj_{\\alpha_i} v Informa\u00e7\u00f5es Adicionais Extendendo a ideia dos autovalores: Dado um operador linear A: E \\to E ou existe um vetor u \\in E tal que Au = \\lambda u . Ou, ent\u00e3o, existem u, v \\in E linearmente independentes, tais que Au = \\alpha u + \\beta v e Av = \\gamma u + \\delta v . Invariante: Diz-se que um subespa\u00e7o vetorial F \\subset E \u00e9 invariante pelo operador A: E \\to F quando A(F) \\subset F . Isto \u00e9, quando a imagem dos vetorres desse subespa\u00e7o est\u00e3o nesse subespa\u00e7o. Um subespa\u00e7o de dimens\u00e3o 1 \u00e9 invariante por A se, e somente se, existe um n\u00famero \\lambda tal que Av = \\lambda v, \\forall v \\in F . Se u,v formam um subespa\u00e7o de dimens\u00e3o 2 , ele ser\u00e1 invariante se, e s\u00f3 se, Au \\in F e Av \\in F . Teorema: Todo operador linear num espa\u00e7o vetorial de dimens\u00e3o finita possui um subespa\u00e7o invariante de dimens\u00e3o 1 ou 2. Para provar esse teorema, temos que provar o lema que diz que existem um polin\u00f4mio de grau 1 ou 2 e um vetor v tal que p(A)\\cdot v = 0 . Cadeias de Markov Defini\u00e7\u00e3o: \u00c9 uma s\u00e9rie temporal discreta no qual a distribui\u00e7\u00e3o de uma popula\u00e7\u00e3o pode ser calculada por recorr\u00eancia. Ad condi\u00e7\u00f5es s\u00e3o que a popula\u00e7\u00e3o nunca torna-se negativa e que a popula\u00e7\u00e3o total \u00e9 fixa. Podemos utilizar uma matriz de trani\u00e7\u00e3o que descreva a movimenta\u00e7\u00e3o probil\u00edstica dessa popula\u00e7\u00e3o. Requere-se que a soma de cada coluna seja 1 e que n\u00e3o haja entradas negativas. O elemento ij da matriz descreve a probabilidade da popula\u00e7\u00e3o passar do estado j para o estadp i . Se T possui alguma pot\u00eancua com todas as entradas positivas, \u00e9 dito regular. Uma matrzi de transi\u00e7\u00e3o regular ter\u00e1 um estado estacion\u00e1rio. Ts = s . \u00c9 poss\u00edvel mostrar que qualquer matriz de transi\u00e7\u00e3o com as condi\u00e7\u00f5es dadas deve ter um autovalor 1 .","title":"Monitorias 12 e 13"},{"location":"alglin/files/monitoria12/#monitorias-12-e-13","text":"","title":"Monitorias 12 e 13"},{"location":"alglin/files/monitoria12/#autovalores-e-autovetores","text":"Defini\u00e7\u00e3o: Autovetor \u00e9 um vetor ( \\neq \\vec{0} ) que tem como imagem de uma transforma\u00e7\u00e3o linear um vetor proporcional. A propor\u00e7\u00e3o \u00e9 chamada de autovalor. Polin\u00f4mio Caracter\u00edstico: Polin\u00f4mio cujas ra\u00edzes s\u00e3o os autovalores de uma transforma\u00e7\u00e3o linear. Subespa\u00e7o invariante: Tamb\u00e9m conhecido como auto-espa\u00e7o, \u00e9 formado pela combina\u00e7\u00e3o dos autovetores associados ao mesmo autovalor. Teorema 1: Seja A um operador linear, \\lambda um autovalor e v um autovetor. Av = \\lambda v \\implies A^nv = \\lambda^n v . Teorema 2: A autovalores diferentes do mesmo operador correspondem autovetores linearmente independentes.","title":"Autovalores e Autovetores"},{"location":"alglin/files/monitoria12/#mudanca-de-base","text":"Considere as seguintes bases: E = \\{e_1, ..., e_n\\} U = \\{u_1, ..., u_n\\} V = \\{v_1,...,v_n\\} Considere w = (x_1,...,x_n) . Isso significa que w \u00e9 escrito como uma combina\u00e7\u00e3o linear dos vetores da base E , can\u00f4nica, e os coeficientes s\u00e3o x_1, ..., x_n . Imagine que queiramos escrever na base U . Para isso, basta encontrarmos os coeficientes de cada vetor da base U . Para isso, basta resolver o sistema linear onde cada vetor de U \u00e9 uma coluna e o vetor restultado \u00e9 o vetor na base can\u00f4nica. Assim, a matriz formada pelos vetores da base U formam uma matriz que transforma vetores da base U em vetores da base can\u00f4nica. A inversa faz o processo contr\u00e1rio. Se quis\u00e9ssemos mudar da base E para a base U sem o uso da inversa, s\u00f3 precisamos saber a transforma\u00e7\u00e3o dos vetores da base can\u00f4nica. Para fazer a transforma\u00e7\u00e3o de uma base em outra, basta transformarmos na can\u00f4nica como interm\u00e9dio.","title":"Mudan\u00e7a de Base"},{"location":"alglin/files/monitoria12/#matrizes-semelhantes-e-diagonalizacao","text":"Defini\u00e7\u00e3o: Duas matrizes s\u00e3o semelhantes se existe P invert\u00edvel tal que B = P^{-1}AP (AP = PB) , que tem o mesmo polin\u00f4mio caracter\u00edstico e o mesmo determinante. Diagonaliza\u00e7\u00e3o: Uma matriz \u00e9 diagonaliz\u00e1vel se existe uma matriz semelhante que seja diagonal. A \u00e9 diagon\u00e1liz\u00e1vel se, e s\u00f3 se, tiver n autovetores LI. Nesse caso P \u00e9 a matriz cujas colunas s\u00e3o os autovetores de A e D os autovalores correspontes. Observe que P^{-1}AP = D , logo para transformar um vetor na base V em outro na base V correspondente a imagem desse vetor na base can\u00f4nica da matriz A , basta usar a transforma\u00e7\u00e3o D .","title":"Matrizes Semelhantes e Diagonaliza\u00e7\u00e3o"},{"location":"alglin/files/monitoria12/#recorrencias","text":"Podemos utilizar matrizes para representar recorr\u00eancias. Um exemplo famoso \u00e9 a sequ\u00eancia de Fibonight.","title":"Recorr\u00eancias"},{"location":"alglin/files/monitoria12/#produto-interno","text":"Seja E um espa\u00e7o vetorial e u, v \\in E . Define-se produto interno com <u,v> com um n\u00famero real que satisfaz as seguintes condi\u00e7\u00f5es: <u,v + v'> = <u,v> + <u,v'> e <u + u',v> = <u,v> + <u',v> <u,v> = <v,u> <\\alpha u,v> = \\alpha<u,v> e <u,\\alpha v> = \\alpha<u,v> Se u \\neq 0 , <u,u> > 0","title":"Produto Interno"},{"location":"alglin/files/monitoria12/#projecoes","text":"J\u00e1 sabemos que p = (\\frac{u\\cdot v}{v\\cdot v})v \u00e9 a proje\u00e7\u00e3o do vetor u sobre a reta gerada por v . Quando queremos projetar um vetor v sobre um hiperplano \\pi , com vetor nornal n , temos que v = p + tn , onde t \u00e9 uma constante. Logo, podemos montar um sistema com n equa\u00e7\u00f5es.","title":"Proje\u00e7\u00f5es"},{"location":"alglin/files/monitoria12/#ortogonalidade","text":"Se <u,v> = 0 , dizemos que u e v s\u00e3o ortogonais. Um conjunto \u00e9 dito ortogonal se a cada par de vetores, eles s\u00e3o ortogonais. Ele ser\u00e1 ortonormal quando seus vetores ortogonais forem normalizados. Note que se X \u00e9 um conjunto ortogonal, ent\u00e3o \u00e9 LI.","title":"Ortogonalidade"},{"location":"alglin/files/monitoria12/#ortogonalizacao-de-gram-schimidt","text":"Lembre-se: Defina um vetor inicial e utilize a ideia de que cada outro vetor ser\u00e1 subtra\u00eddo das proje\u00e7\u00f5es do vetor calculado previamente.","title":"Ortogonaliza\u00e7\u00e3o de Gram-Schimidt"},{"location":"alglin/files/monitoria12/#projecao-de-um-vetor-sobre-um-subespaco","text":"Seja W um subespa\u00e7o de E e \\alpha uma base ortogonal desse subespa\u00e7o. p = proj_W v = \\sum_{i=1}^n proj_{\\alpha_i} v","title":"Proje\u00e7\u00e3o de um vetor sobre um subespa\u00e7o"},{"location":"alglin/files/monitoria12/#informacoes-adicionais","text":"Extendendo a ideia dos autovalores: Dado um operador linear A: E \\to E ou existe um vetor u \\in E tal que Au = \\lambda u . Ou, ent\u00e3o, existem u, v \\in E linearmente independentes, tais que Au = \\alpha u + \\beta v e Av = \\gamma u + \\delta v . Invariante: Diz-se que um subespa\u00e7o vetorial F \\subset E \u00e9 invariante pelo operador A: E \\to F quando A(F) \\subset F . Isto \u00e9, quando a imagem dos vetorres desse subespa\u00e7o est\u00e3o nesse subespa\u00e7o. Um subespa\u00e7o de dimens\u00e3o 1 \u00e9 invariante por A se, e somente se, existe um n\u00famero \\lambda tal que Av = \\lambda v, \\forall v \\in F . Se u,v formam um subespa\u00e7o de dimens\u00e3o 2 , ele ser\u00e1 invariante se, e s\u00f3 se, Au \\in F e Av \\in F . Teorema: Todo operador linear num espa\u00e7o vetorial de dimens\u00e3o finita possui um subespa\u00e7o invariante de dimens\u00e3o 1 ou 2. Para provar esse teorema, temos que provar o lema que diz que existem um polin\u00f4mio de grau 1 ou 2 e um vetor v tal que p(A)\\cdot v = 0 .","title":"Informa\u00e7\u00f5es Adicionais"},{"location":"alglin/files/monitoria12/#cadeias-de-markov","text":"Defini\u00e7\u00e3o: \u00c9 uma s\u00e9rie temporal discreta no qual a distribui\u00e7\u00e3o de uma popula\u00e7\u00e3o pode ser calculada por recorr\u00eancia. Ad condi\u00e7\u00f5es s\u00e3o que a popula\u00e7\u00e3o nunca torna-se negativa e que a popula\u00e7\u00e3o total \u00e9 fixa. Podemos utilizar uma matriz de trani\u00e7\u00e3o que descreva a movimenta\u00e7\u00e3o probil\u00edstica dessa popula\u00e7\u00e3o. Requere-se que a soma de cada coluna seja 1 e que n\u00e3o haja entradas negativas. O elemento ij da matriz descreve a probabilidade da popula\u00e7\u00e3o passar do estado j para o estadp i . Se T possui alguma pot\u00eancua com todas as entradas positivas, \u00e9 dito regular. Uma matrzi de transi\u00e7\u00e3o regular ter\u00e1 um estado estacion\u00e1rio. Ts = s . \u00c9 poss\u00edvel mostrar que qualquer matriz de transi\u00e7\u00e3o com as condi\u00e7\u00f5es dadas deve ter um autovalor 1 .","title":"Cadeias de Markov"},{"location":"alglin/files/monitoria6/","text":"Monitoria 6 Defini\u00e7\u00f5es e Teoremas Linearmente Independente: Um cojunto X \\subset E \u00e9 dito linearmente independente, quando nenhum vetor do conjunto \u00e9 combina\u00e7\u00e3o linear dos outros vetores. O conjunto unit\u00e1rio \u00e9 dito LI. Para isso, existe o teorema de que: \\alpha_1v_1 + ... + \\alpha_nv_n = 0 \\to \\alpha_1 = ... = \\alpha_n = 0 , se e s\u00f3 se, X \u00e9 LI. A partir disso, conclue-se que a representa\u00e7\u00e3o de um vetor como combina\u00e7\u00e3o de outros vetores \u00e9 sempre \u00fanica (se os vetores formarem um conjunto LI). Se um conjunto n\u00e3o \u00e9 LI, ele \u00e9 dito linearmente dependente. Teorema 1 Seja X = \\{x_1,x_2,...,x_m\\} . Se, \\forall k \\leq m, v_k n\u00e3o \u00e9 combina\u00e7\u00e3o linear de seus antecessores, ent\u00e3o X \u00e9 LI. Observa\u00e7\u00e3o Considere X = \\{(1,2),(3,4),(2,4)\\} \\subset \\mathbb{R}^2 , Note que X \u00e9 LD, por\u00e9m (3,4) n\u00e3o \u00e9 combina\u00e7\u00e3o linear dos outros vetores (verifique!). Por que isso n\u00e3o \u00e9 contradit\u00f3rio? Base: \u00c9 um conjunto linearmente independente que gera E. Os coeficientes s\u00e3o chamados de coordenadas do vetor nessa base. Como veremos a seguir, toda base de um espa\u00e7o vetorial apresenta o mesmo n\u00famero de elementos. Este n\u00famero \u00e9 chamado de \\textit{dimens\u00e3o}. Lema 2.1: Todo sistema homog\u00eaneo cujo n\u00famero de inc\u00f3gnitas \u00e9 maior que o n\u00famero de equa\u00e7\u00f5es admite solu\u00e7\u00e3o n\u00e3o trivial (a prova \u00e9 por indu\u00e7\u00e3o em m , o n\u00famero de equa\u00e7\u00f5es. Teorema 2.2: Se um conjunto de n vetores gera o espa\u00e7o E, ent\u00e3o qualquer conjunto com mais de n elementos \u00e9 LD. Corol\u00e1rio 2.3: Assim, se os vetores v_1,...,v_n geram o espa\u00e7o vetorial E e os vetores u_1,...,u_m s\u00e3o LI, m\\leq n . Daqui tiramos que se E admite uma base \\beta = \\{u_1,...,u_n\\} , qualquer outra base tamb\u00e9m possui n elementos. Teorema 3: Considere um espa\u00e7o vetorial de dimens\u00e3o finita: Considere o conjunto de todos os geradores de E. Ele cont\u00e9m uma base. Todo conjunto LI est\u00e1 contido numa base. Todo subespa\u00e7o vetorial tem dimens\u00e3o finita. Se a dimens\u00e3o de um subespa\u00e7o \u00e9 n , ent\u00e3o o subespa\u00e7o \u00e9 o pr\u00f3prio espa\u00e7o. Exerc\u00edcios Prove que os seguintes polin\u00f4mios s\u00e3o linearmente independentes: p(x) = x^3 - 5x^2 + 1, q(x) = 2x^4 + 5x - 6, r(x) = x^2 - 5x + 2 . Dica: Considere a base X = \\{1, x, x^2, x^3, x^4\\} Seja X um conjunto de polin\u00f4mios. Se dois polin\u00f4mios quaisquer de X t\u00eam graus diferentes, X \u00e9 LI. Dado X \\subset E , seja Y o conjunto obtido de X substituindo um dos seus elementos v por v + \\alpha u , onde u \\in X e \\alpha \\in \\mathbb{R} . Prove que X e Y geram o mesmo subespa\u00e7o vetorial de E . Conclua, ent\u00e3o que \\{v_1,...,v_k\\} \\subset E e \\{v_1, v_2 - v_1, ..., v_k - v_1\\} \\subset E geram o mesmo subespa\u00e7o vetorial de E . Mostre que os vetores u = (1,1) e v = (-1,1) formam uma base de \\mathbb{R}^2 . Considere a afirma\u00e7\u00e3o: \"A uni\u00e3o de dois conjuntos subconjuntos LI do espa\u00e7o vetorial E \u00e9 ainda um conjunto LI\". Assinale verdadeiro e falso: ( ) Nunca; ( ) Quando um deles \u00e9 disjunto do outro; ( ) Quanto um deles \u00e9 parte do outro; ( ) Quando um deles \u00e9 disjunto do subespa\u00e7o gerado pelo outro; ( ) Quando o n\u00famero de elementos de um deles mais o n\u00famero de elementos do outro \u00e9 igual \u00e0 dimens\u00e3o de E. Encontre uma base para o espa\u00e7o vetorial W = \\{\\begin{pmatrix} a \\\\ b \\\\ -b \\\\ a\\end{pmatrix}, \\forall a,b \\in \\mathbb{R}^2\\} Se f e g est\u00e3o no espa\u00e7o vetorial de todas as fun\u00e7\u00f5es com derivadas cont\u00ednuas, ent\u00e3o o determinante de \\begin{pmatrix} f(x) & g(x) \\\\ f'(x) & g'(x) \\end{pmatrix} \u00e9 conhecido como Wronskiano de f e g . Prove que f e g s\u00e3o linearmente independentes, se seu Wronskiano n\u00e3o for identicamente nulo. Esse estudo \u00e9 estremamente importante no estudo de solu\u00e7\u00f5es de sistemas de equa\u00e7\u00f5es diferenci\u00e1veis, pois identifica se duas solu\u00e7\u00f5es s\u00e3o linearmente independentes. Aplica\u00e7\u00e3o: Quadrados M\u00e1gicos Observe a imagem da Melancolia I, de Albrecht Durer de 1514: Link da obra Observa-se o quadrado m\u00e1gico: \\begin{pmatrix} 16 & 3 & 2 & 13 \\\\ 5 & 10 & 11 & 8 \\\\ 9 & 6 & 7 & 12 \\\\ 4 & 15 & 14 & 1 \\end{pmatrix} Primeira coisa interessante \u00e9 ver 15 e 14 lado a lado. A soma de cada coluna, linha e diagoral \u00e9 34 . Podemos definir uma matriz n\\times n sendo quadrado m\u00e1gico quando a soma de cada linha, coluna e diagonal \u00e9 igual. Essa soma se chama peso. Considere Mag_n o conjunto de todos os quadrados m\u00e1gicos de ordem n . Prove que Mag_3 \u00e9 um subespa\u00e7o de M_{3x3} .","title":"Monitoria 6"},{"location":"alglin/files/monitoria6/#monitoria-6","text":"","title":"Monitoria 6"},{"location":"alglin/files/monitoria6/#definicoes-e-teoremas","text":"Linearmente Independente: Um cojunto X \\subset E \u00e9 dito linearmente independente, quando nenhum vetor do conjunto \u00e9 combina\u00e7\u00e3o linear dos outros vetores. O conjunto unit\u00e1rio \u00e9 dito LI. Para isso, existe o teorema de que: \\alpha_1v_1 + ... + \\alpha_nv_n = 0 \\to \\alpha_1 = ... = \\alpha_n = 0 , se e s\u00f3 se, X \u00e9 LI. A partir disso, conclue-se que a representa\u00e7\u00e3o de um vetor como combina\u00e7\u00e3o de outros vetores \u00e9 sempre \u00fanica (se os vetores formarem um conjunto LI). Se um conjunto n\u00e3o \u00e9 LI, ele \u00e9 dito linearmente dependente. Teorema 1 Seja X = \\{x_1,x_2,...,x_m\\} . Se, \\forall k \\leq m, v_k n\u00e3o \u00e9 combina\u00e7\u00e3o linear de seus antecessores, ent\u00e3o X \u00e9 LI. Observa\u00e7\u00e3o Considere X = \\{(1,2),(3,4),(2,4)\\} \\subset \\mathbb{R}^2 , Note que X \u00e9 LD, por\u00e9m (3,4) n\u00e3o \u00e9 combina\u00e7\u00e3o linear dos outros vetores (verifique!). Por que isso n\u00e3o \u00e9 contradit\u00f3rio? Base: \u00c9 um conjunto linearmente independente que gera E. Os coeficientes s\u00e3o chamados de coordenadas do vetor nessa base. Como veremos a seguir, toda base de um espa\u00e7o vetorial apresenta o mesmo n\u00famero de elementos. Este n\u00famero \u00e9 chamado de \\textit{dimens\u00e3o}. Lema 2.1: Todo sistema homog\u00eaneo cujo n\u00famero de inc\u00f3gnitas \u00e9 maior que o n\u00famero de equa\u00e7\u00f5es admite solu\u00e7\u00e3o n\u00e3o trivial (a prova \u00e9 por indu\u00e7\u00e3o em m , o n\u00famero de equa\u00e7\u00f5es. Teorema 2.2: Se um conjunto de n vetores gera o espa\u00e7o E, ent\u00e3o qualquer conjunto com mais de n elementos \u00e9 LD. Corol\u00e1rio 2.3: Assim, se os vetores v_1,...,v_n geram o espa\u00e7o vetorial E e os vetores u_1,...,u_m s\u00e3o LI, m\\leq n . Daqui tiramos que se E admite uma base \\beta = \\{u_1,...,u_n\\} , qualquer outra base tamb\u00e9m possui n elementos. Teorema 3: Considere um espa\u00e7o vetorial de dimens\u00e3o finita: Considere o conjunto de todos os geradores de E. Ele cont\u00e9m uma base. Todo conjunto LI est\u00e1 contido numa base. Todo subespa\u00e7o vetorial tem dimens\u00e3o finita. Se a dimens\u00e3o de um subespa\u00e7o \u00e9 n , ent\u00e3o o subespa\u00e7o \u00e9 o pr\u00f3prio espa\u00e7o.","title":"Defini\u00e7\u00f5es e Teoremas"},{"location":"alglin/files/monitoria6/#exercicios","text":"Prove que os seguintes polin\u00f4mios s\u00e3o linearmente independentes: p(x) = x^3 - 5x^2 + 1, q(x) = 2x^4 + 5x - 6, r(x) = x^2 - 5x + 2 . Dica: Considere a base X = \\{1, x, x^2, x^3, x^4\\} Seja X um conjunto de polin\u00f4mios. Se dois polin\u00f4mios quaisquer de X t\u00eam graus diferentes, X \u00e9 LI. Dado X \\subset E , seja Y o conjunto obtido de X substituindo um dos seus elementos v por v + \\alpha u , onde u \\in X e \\alpha \\in \\mathbb{R} . Prove que X e Y geram o mesmo subespa\u00e7o vetorial de E . Conclua, ent\u00e3o que \\{v_1,...,v_k\\} \\subset E e \\{v_1, v_2 - v_1, ..., v_k - v_1\\} \\subset E geram o mesmo subespa\u00e7o vetorial de E . Mostre que os vetores u = (1,1) e v = (-1,1) formam uma base de \\mathbb{R}^2 . Considere a afirma\u00e7\u00e3o: \"A uni\u00e3o de dois conjuntos subconjuntos LI do espa\u00e7o vetorial E \u00e9 ainda um conjunto LI\". Assinale verdadeiro e falso: ( ) Nunca; ( ) Quando um deles \u00e9 disjunto do outro; ( ) Quanto um deles \u00e9 parte do outro; ( ) Quando um deles \u00e9 disjunto do subespa\u00e7o gerado pelo outro; ( ) Quando o n\u00famero de elementos de um deles mais o n\u00famero de elementos do outro \u00e9 igual \u00e0 dimens\u00e3o de E. Encontre uma base para o espa\u00e7o vetorial W = \\{\\begin{pmatrix} a \\\\ b \\\\ -b \\\\ a\\end{pmatrix}, \\forall a,b \\in \\mathbb{R}^2\\} Se f e g est\u00e3o no espa\u00e7o vetorial de todas as fun\u00e7\u00f5es com derivadas cont\u00ednuas, ent\u00e3o o determinante de \\begin{pmatrix} f(x) & g(x) \\\\ f'(x) & g'(x) \\end{pmatrix} \u00e9 conhecido como Wronskiano de f e g . Prove que f e g s\u00e3o linearmente independentes, se seu Wronskiano n\u00e3o for identicamente nulo. Esse estudo \u00e9 estremamente importante no estudo de solu\u00e7\u00f5es de sistemas de equa\u00e7\u00f5es diferenci\u00e1veis, pois identifica se duas solu\u00e7\u00f5es s\u00e3o linearmente independentes.","title":"Exerc\u00edcios"},{"location":"alglin/files/monitoria6/#aplicacao-quadrados-magicos","text":"Observe a imagem da Melancolia I, de Albrecht Durer de 1514: Link da obra Observa-se o quadrado m\u00e1gico: \\begin{pmatrix} 16 & 3 & 2 & 13 \\\\ 5 & 10 & 11 & 8 \\\\ 9 & 6 & 7 & 12 \\\\ 4 & 15 & 14 & 1 \\end{pmatrix} Primeira coisa interessante \u00e9 ver 15 e 14 lado a lado. A soma de cada coluna, linha e diagoral \u00e9 34 . Podemos definir uma matriz n\\times n sendo quadrado m\u00e1gico quando a soma de cada linha, coluna e diagonal \u00e9 igual. Essa soma se chama peso. Considere Mag_n o conjunto de todos os quadrados m\u00e1gicos de ordem n . Prove que Mag_3 \u00e9 um subespa\u00e7o de M_{3x3} .","title":"Aplica\u00e7\u00e3o: Quadrados M\u00e1gicos"},{"location":"alglin/files/monitoria7/","text":"Monitoria 7 Defini\u00e7\u00f5es e Teoremas Quando uma linha \u00e9 substitu\u00edda pela soma dela com um m\u00faltiplo de outra, a nova linha pertence ao mesmo subespa\u00e7o gerado pelas primeiras. Mais do que isso, o subespa\u00e7o gerado \u00e9 o mesmo. Nulidade da Matriz: Dimens\u00e3o do espa\u00e7o anulado da matriz A ( anul(A) ). Voc\u00ea sabe encontrar a nulidade de uma matriz? Teorema do Posto ! anul(A) + posto(A) = n . Considere Ax = v_E . Ent\u00e3o, A \u00e9 a matriz de passagem da base B para a base E , can\u00f4nica. Assim, v_B = A^{-1}v_E . Transforma\u00e7\u00e3o Linear \u00e9 uma fun\u00e7\u00e3o linear entre os espa\u00e7os vetorias E e F com as propriedades de soma T(u+v)=T(u)+T(v) e T(\\alpha u) = \\alpha T(u) . Lembre que T(v) = T(\\alpha_1 e_1 + ... + \\alpha_n e_n) = \\alpha_1 T(e_1) + ... + \\alpha_n T(e_n) . Logo, a transforma\u00e7\u00e3o linear est\u00e1 definida quando conhcemos as imagens dos elementos de uma base. Da\u00ed sa\u00ed a matriz de transforma\u00e7\u00e3o. O escalonamento mant\u00e9m a rela\u00e7\u00e3o entre as colunas das matrizes. Para se ter a intui\u00e7\u00e3o, basta pensar que para resolver sistemas, escalonamos as matrizes, e as inc\u00f3gnitas permanecem as mesmas para o sistema escalonado. Suponha que temos uma vetor w_\\beta = (a,b)_\\beta e queremos reescrever w_E = (a',b') , na base can\u00f4nica. Para isso, precisamos fazer uma mudan\u00e7a de bases que envolve uma matriz de tranforma\u00e7\u00e3o. Essa matriz \u00e9 simples, pois \u00e9 composta pelos vetores da base \\beta . Teorema: Seja uma transforma\u00e7\u00e3o linear A: E \\to F . A cada vetor u \\in \\beta base de E , fa\u00e7amos corresponder um vetor u' \\in F . Ent\u00e3o essa tranforma\u00e7\u00e3o, tal que Au = u' , \u00e9 \u00fanica. Importante Saber encontrar bases do espa\u00e7o-coluna, do espa\u00e7o-linha e do espa\u00e7o-anulado (logo suas dimens\u00f5es). Um funcional linear \u00e9 T: E \\to \\mathbb{R} . Um operador linear \u00e9 T: E \\to E . Lembrar de conjunto gerador. Exerc\u00edcios: Prove que os seguintes polin\u00f4mios s\u00e3o linearmente independentes: p(x) = x^3 - 5x^2 + 1, q(x) = 2x^4 + 5x - 6, r(x) = x^2 - 5x + 2 . Considere a base X = \\{1, x, x^2, x^3, x^4\\} Seja X um conjunto de polin\u00f4mios. Se dois polin\u00f4mios quaisquer de X t\u00eam graus diferentes, X \u00e9 LI. Mostre que os vetores u = (1,1) e v = (-1,1) formam uma base de \\mathbb{R}^2 . Encontre os espa\u00e7os linha, coluna e anulado da matriz: A = \\left[ \\begin{array}{ccccc} -1 & 2 & 0 & 4 & 5\\\\ 3 & -7 & 2 & 0 & 1\\\\ 2 & -5 & 2 & 4 & 6 \\end{array} \\right] Seja U = \\{u_1,u_2,u_3\\} . Como representar o vetor (a,b,c) , como combina\u00e7\u00e3o linear dos vetores de U . Exiba uma base para o espa\u00e7o vetorial formado pelos polin\u00f4mios de grau \\leq n que se anulam em x=2 e x=3 . Qual a dimens\u00e3o dessa base? Tem-se uma transforma\u00e7\u00e3o linear A(-1,1) = (1,2,3) e A(2,3) = (1,1,1) . Qual a matriz de tranforma\u00e7\u00e3o de A , em rela\u00e7\u00e3o \u00e0s bases can\u00f4nicas. Avalie as afirma\u00e7\u00f5es: ( ) Seja C = \\{(x_1,x_2,x_3,x_4,x_5); x_i = 3\\cdot x_{i-1}, i=2,...,5\\} . \u00c9 um subespa\u00e7o do \\mathbb{R}^5 . (Se verdadeiro, apresente uma base). ( ) \u00c9 poss\u00edvel encontrar dois planos do \\mathbb{R}^4 que se interesectem em apenas um ponto. ( Pense em planos com dois par\u00e2metos livres ). ( ) O conjunto de todas as matrizes cujo determinante \u00e9 maior do que zero \u00e9 um subespa\u00e7o das matrizes. ( ) A uni\u00e3o de dois conjuntos LI \u00e9 um conjunto LI, se um deles \u00e9 disjunto do subespa\u00e7o gerado pelo outro. ( ) Existe apenas uma transforma\u00e7\u00e3o linear com T(0,0,1) = (1,2) , T(0,1,0) = (2,3), T(1,0,0) = (4,7) , onde T: \\mathbb{R}^3\\to \\mathbb{R}^2 . Isto \u00e9, n\u00e3o existem x,y,z , tal que T(x,y,z) \\neq T'(x,y,z) com essas propriedades. ( ) Se u, v, w \\in E s\u00e3o colineares, ent\u00e3o Au,Av,Aw tamb\u00e9m s\u00e3o. ( ) Se Aw = Au + Av , ent\u00e3o w = u + v . Aplica\u00e7\u00e3o: Quadrados M\u00e1gicos Na monitoria 6, observamos a imagem da Melancolia I, de Albrecht Durer de 1514, em que apareceia um quadrado m\u00e1gico cl\u00e1ssico. Para lembrar: definimos uma matriz n\\times n como quadrado m\u00e1gico quando a soma de cada linha, coluna e diagonal \u00e9 igual. Essa soma se chama peso. Mag_n o conjunto de todos os quadrados m\u00e1gicos de ordem n .O quadrado ser\u00e1 cl\u00e1ssico se usarmos os todos on n\u00fameros entre 1 e n^2 . Considere Mag_2 . Voc\u00ea conseguiria encontrar uma base para esse subespa\u00e7o das matrizes 2 por 2 (provamos que \u00e9 um subespa\u00e7o na semana passada)? E Mag_3 ? Exemplo: X = \\{(1,1,1),(1,2,1)\\}, Y = \\{(1,0,0),(0,0,1)\\}","title":"Monitoria 7"},{"location":"alglin/files/monitoria7/#monitoria-7","text":"","title":"Monitoria 7"},{"location":"alglin/files/monitoria7/#definicoes-e-teoremas","text":"Quando uma linha \u00e9 substitu\u00edda pela soma dela com um m\u00faltiplo de outra, a nova linha pertence ao mesmo subespa\u00e7o gerado pelas primeiras. Mais do que isso, o subespa\u00e7o gerado \u00e9 o mesmo. Nulidade da Matriz: Dimens\u00e3o do espa\u00e7o anulado da matriz A ( anul(A) ). Voc\u00ea sabe encontrar a nulidade de uma matriz? Teorema do Posto ! anul(A) + posto(A) = n . Considere Ax = v_E . Ent\u00e3o, A \u00e9 a matriz de passagem da base B para a base E , can\u00f4nica. Assim, v_B = A^{-1}v_E . Transforma\u00e7\u00e3o Linear \u00e9 uma fun\u00e7\u00e3o linear entre os espa\u00e7os vetorias E e F com as propriedades de soma T(u+v)=T(u)+T(v) e T(\\alpha u) = \\alpha T(u) . Lembre que T(v) = T(\\alpha_1 e_1 + ... + \\alpha_n e_n) = \\alpha_1 T(e_1) + ... + \\alpha_n T(e_n) . Logo, a transforma\u00e7\u00e3o linear est\u00e1 definida quando conhcemos as imagens dos elementos de uma base. Da\u00ed sa\u00ed a matriz de transforma\u00e7\u00e3o. O escalonamento mant\u00e9m a rela\u00e7\u00e3o entre as colunas das matrizes. Para se ter a intui\u00e7\u00e3o, basta pensar que para resolver sistemas, escalonamos as matrizes, e as inc\u00f3gnitas permanecem as mesmas para o sistema escalonado. Suponha que temos uma vetor w_\\beta = (a,b)_\\beta e queremos reescrever w_E = (a',b') , na base can\u00f4nica. Para isso, precisamos fazer uma mudan\u00e7a de bases que envolve uma matriz de tranforma\u00e7\u00e3o. Essa matriz \u00e9 simples, pois \u00e9 composta pelos vetores da base \\beta . Teorema: Seja uma transforma\u00e7\u00e3o linear A: E \\to F . A cada vetor u \\in \\beta base de E , fa\u00e7amos corresponder um vetor u' \\in F . Ent\u00e3o essa tranforma\u00e7\u00e3o, tal que Au = u' , \u00e9 \u00fanica.","title":"Defini\u00e7\u00f5es e Teoremas"},{"location":"alglin/files/monitoria7/#importante","text":"Saber encontrar bases do espa\u00e7o-coluna, do espa\u00e7o-linha e do espa\u00e7o-anulado (logo suas dimens\u00f5es). Um funcional linear \u00e9 T: E \\to \\mathbb{R} . Um operador linear \u00e9 T: E \\to E . Lembrar de conjunto gerador.","title":"Importante"},{"location":"alglin/files/monitoria7/#exercicios","text":"Prove que os seguintes polin\u00f4mios s\u00e3o linearmente independentes: p(x) = x^3 - 5x^2 + 1, q(x) = 2x^4 + 5x - 6, r(x) = x^2 - 5x + 2 . Considere a base X = \\{1, x, x^2, x^3, x^4\\} Seja X um conjunto de polin\u00f4mios. Se dois polin\u00f4mios quaisquer de X t\u00eam graus diferentes, X \u00e9 LI. Mostre que os vetores u = (1,1) e v = (-1,1) formam uma base de \\mathbb{R}^2 . Encontre os espa\u00e7os linha, coluna e anulado da matriz: A = \\left[ \\begin{array}{ccccc} -1 & 2 & 0 & 4 & 5\\\\ 3 & -7 & 2 & 0 & 1\\\\ 2 & -5 & 2 & 4 & 6 \\end{array} \\right] Seja U = \\{u_1,u_2,u_3\\} . Como representar o vetor (a,b,c) , como combina\u00e7\u00e3o linear dos vetores de U . Exiba uma base para o espa\u00e7o vetorial formado pelos polin\u00f4mios de grau \\leq n que se anulam em x=2 e x=3 . Qual a dimens\u00e3o dessa base? Tem-se uma transforma\u00e7\u00e3o linear A(-1,1) = (1,2,3) e A(2,3) = (1,1,1) . Qual a matriz de tranforma\u00e7\u00e3o de A , em rela\u00e7\u00e3o \u00e0s bases can\u00f4nicas. Avalie as afirma\u00e7\u00f5es: ( ) Seja C = \\{(x_1,x_2,x_3,x_4,x_5); x_i = 3\\cdot x_{i-1}, i=2,...,5\\} . \u00c9 um subespa\u00e7o do \\mathbb{R}^5 . (Se verdadeiro, apresente uma base). ( ) \u00c9 poss\u00edvel encontrar dois planos do \\mathbb{R}^4 que se interesectem em apenas um ponto. ( Pense em planos com dois par\u00e2metos livres ). ( ) O conjunto de todas as matrizes cujo determinante \u00e9 maior do que zero \u00e9 um subespa\u00e7o das matrizes. ( ) A uni\u00e3o de dois conjuntos LI \u00e9 um conjunto LI, se um deles \u00e9 disjunto do subespa\u00e7o gerado pelo outro. ( ) Existe apenas uma transforma\u00e7\u00e3o linear com T(0,0,1) = (1,2) , T(0,1,0) = (2,3), T(1,0,0) = (4,7) , onde T: \\mathbb{R}^3\\to \\mathbb{R}^2 . Isto \u00e9, n\u00e3o existem x,y,z , tal que T(x,y,z) \\neq T'(x,y,z) com essas propriedades. ( ) Se u, v, w \\in E s\u00e3o colineares, ent\u00e3o Au,Av,Aw tamb\u00e9m s\u00e3o. ( ) Se Aw = Au + Av , ent\u00e3o w = u + v .","title":"Exerc\u00edcios:"},{"location":"alglin/files/monitoria7/#aplicacao-quadrados-magicos","text":"Na monitoria 6, observamos a imagem da Melancolia I, de Albrecht Durer de 1514, em que apareceia um quadrado m\u00e1gico cl\u00e1ssico. Para lembrar: definimos uma matriz n\\times n como quadrado m\u00e1gico quando a soma de cada linha, coluna e diagonal \u00e9 igual. Essa soma se chama peso. Mag_n o conjunto de todos os quadrados m\u00e1gicos de ordem n .O quadrado ser\u00e1 cl\u00e1ssico se usarmos os todos on n\u00fameros entre 1 e n^2 . Considere Mag_2 . Voc\u00ea conseguiria encontrar uma base para esse subespa\u00e7o das matrizes 2 por 2 (provamos que \u00e9 um subespa\u00e7o na semana passada)? E Mag_3 ? Exemplo: X = \\{(1,1,1),(1,2,1)\\}, Y = \\{(1,0,0),(0,0,1)\\}","title":"Aplica\u00e7\u00e3o: Quadrados M\u00e1gicos"},{"location":"alglin/files/monitoria4/monitoria4/","text":"Monitoria 4 T\u00f3picos j\u00e1 estudados Matrizes e suas propriedade Nota\u00e7\u00e3o Opera\u00e7\u00f5es com matrizes e suas propriedades A transposta e suas propriedades: (AB)^T = B^TA^T Sim\u00e9trica e Antissim\u00e9trica: (A + A^T) \u00e9 sim\u00e9trica e (A - A^T) \u00e9 antissim\u00e9trica, \\forall A . A inversa, quando existe, e suas propriedades M\u00e9todo para obten\u00e7\u00e3o da inversa utilizando matrizes elementares Matrizes em outros conjuntos, como Z_5 . Matriz singular: matriz n\u00e3o invert\u00edvel. Decomposi\u00e7\u00e3o LU Sistemas Lineares e sua representa\u00e7\u00e3o matricial Escalonamento: m\u00e9todo de Gauss e m\u00e9todo de Gauss-Jordan A \u00e9 equivalente a B se, e s\u00f3 se, existe uma sequ\u00eancia de opera\u00e7\u00f5es elementares que transformam A em B. Posto de uma matriz e grau de liberdade (Teorema do Posto) Posto (rank em ingl\u00eas) pode ser definido como a dimens\u00e3o do espa\u00e7o linha ou a dimens\u00e3o do espa\u00e7o coluna de uma matriz. Sistema homog\u00eaneo: sempre existe solu\u00e7\u00e3o (trivial) Posto(A) = Posto(A^T) Matrizes Elementares: representantes matriciais das tr\u00eas opera\u00e7\u00f5es elementares, trocar linha, multiplica\u00e7\u00e3o por constante e somar a linha a um m\u00faltiplo de outra linha Matriz Completa: [A|b] Espa\u00e7os Vetorias e vetores Combina\u00e7\u00e3o Linear de vetores O produto escalar -> m\u00f3dulo Proje\u00e7\u00e3o: \\vec{z} = \\frac{\\vec{v}\\cdot\\vec{u}}{\\vec{u}\\cdot\\vec{u}}\\cdot u Desigualdade de Cauchy-Schwarz e cosseno: \\|u\\|\\|v\\| \\geq \\|u\\cdot v\\| Conjuntos geradores: S = \\{v_1,v_2,...,v_k\\} \\subset\\mathbb{R}^n . O conjunto de todas as combina\u00e7\u00f5es dos elementos de S \u00e9 ger(S) . Depen\u00eancia: um conjunto \u00e9 linearmente dependente se um vetor do conjunto pode ser escrito como combina\u00e7\u00e3o linear de outros. Independente, caso contr\u00e1rio. Lembrar que se \\alpha_1\\vec{v_1} + \\alpha_2\\vec{v_2} + ... + \\alpha_k\\vec{v_k} = 0 \\implies \\alpha_1 = \\alpha_2 = ... = \\alpha_k = 0 , o conjuntos dos vetores \u00e9 linearmente independente. Espa\u00e7os vetorias dotam-se de: comutatividade e associatividade da soma, exist\u00eancia do nulo e da identidade, exist\u00eancia do sim\u00e9trico, distribuitividade com real. Subespa\u00e7os vetoriais: tem o vetor nulo e a soma e produto por real s\u00e3o fechados. Subespa\u00e7os associados a matrizes: Espa\u00e7o linha: conjunto gerado por suas linhas: lin(A) = ger(\\{L_1,L_2,...,L_m\\}) Espa\u00e7o coluna: conjunto gerado por suas colunas: col(A) = ger(\\{c_1,c_2,...,c_m\\}) Espa\u00e7o anulado por uma matriz: Tamb\u00e9m chamado de n\u00facleo, \u00e9 o conjunto solu\u00e7\u00e3o do sistema Ax = 0 Bases de um espa\u00e7o vetorial: \u00c9 um conjunto LIque gera um espa\u00e7o vetorial --- Pr\u00f3xima monitoria import numpy as np import scipy as sp import scipy.linalg as splin import sympy as sc Exerc\u00edcios Exemplo 1.4: Espa\u00e7os Vetorias Seja X um conjunto n\u00e3o vazio. O s\u00edmbolo \\mathbb{F}(X;\\mathbb{R}) representa o conjunto das fun\u00e7\u00f5es reais f,g: X \\to \\mathbb{R} . Defino a soma: f + g como (f + g)(x) = f(x) + g(x) e o produto \\alpha\\cdot f como (\\alpha f)(x) = \\alpha\\cdot f(x) Exerc\u00edcio 2.13: Combina\u00e7\u00e3o Linear Mostre que a matriz d = <script type=\"math/tex; mode=display\">\\begin{pmatrix} 4 & -4 \\\\ -6 & 16 \\end{pmatrix} pode ser escrita como combina\u00e7\u00e3o linear das matrizes: a = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}, b = \\begin{pmatrix} -1 & 2 \\\\ 3 & -4\\end{pmatrix}, c =\\begin{pmatrix} 1 & -2 \\\\ -3 & 4 \\end{pmatrix} Exemplo: Subespa\u00e7os Seja P_4 o espa\u00e7o vetorial de todos os polin\u00f4mios de grau 4 ou menos com coeficientes reais. A soma e o produto por real s\u00e3o definidos naturalmente. S_5 \u00e9 um subespa\u00e7o? S_5 = \\{f(x) \\in P_4 \\| f(1)~ is~ a ~rational~ number\\} Quest\u00e3o Teste 2: Opera\u00e7\u00f5es com matrizes A matriz A = \\frac{1}{\\sqrt{2}} <script type=\"math/tex; mode=display\">\\begin{pmatrix} 1 & -1 \\\\ 1 & 1 \\end{pmatrix} . Quest\u00e3o: \\exists n \\in \\mathbb{N} , tal que A^n = A ? Obs.: Observe que \\frac{1}{\\sqrt{2}} = \\frac{\\sqrt{2}}{2} , e se eu tratar \\theta = \\frac{\\pi}{4} , temos que A \u00e9 matriz de rota\u00e7\u00e3o. Logo, basta encontrar n tal que n\\cdot\\frac{\\pi}{4} = k\\cdot2\\cdot\\pi \\to n = 8\\cdot k \\to n = 8 \u00e9 solu\u00e7\u00e3o, logo A^8 = I \\to A^9 = A Quest\u00e3o 2.37 Dado X \\subset E , seja Y o conjunto obtido de X substituindo um dos seus elementos v por v + \\alpha u , onde u\\in X e \\alpha \\in \\mathbb{R} . Prove que X e Y geram o mesmo subespa\u00e7o vetorial de E . Conclua, ent\u00e3o que \\{v_1,...,v_k\\} \\subset E e \\{v_1, v_2 - v_1, ..., v_k - v_1\\} \\subset E geram o mesmo subespa\u00e7o vetorial de E . Algu\u00e9m percebe a implica\u00e7\u00e3o pr\u00e1tica dessa quest\u00e3o? Lembre que S(Y) \u00e9 o subespa\u00e7o gerado por Y. Uma ajuda para desenvolver essa quest\u00e3o \u00e9 reconhecer que X \\subset S(Y) \\implies S(X) \\subset S(Y) . Voc\u00ea consegue demonstrar? Conjuntos Geradores u_1 = (1,1,2,3) u_2 = (2,-1,1,2) \\pi = ger(\\{u_1, u_2\\}) w = (-2,7,5,6) \\in \\pi Queremos saber se, \\exists ~\\alpha, \\beta , tal que: \\alpha \\begin{pmatrix} 1 \\\\ 1 \\\\ 2 \\\\ 3 \\end{pmatrix} + \\beta \\begin{pmatrix} 2 \\\\ -1 \\\\ 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ 7 \\\\ 5 \\\\ 6 \\end{pmatrix} Teste 1 2018 Exerc\u00edcio 1: Escreva a seguinte matriz como a soma de uma matriz sim\u00e9trica e uma antissim\u00e9trica: A = <script type=\"math/tex; mode=display\">\\begin{pmatrix} 4 & 6 & 3\\\\ -2 & 0 & 5 \\\\ 5 & -1 & 2 \\end{pmatrix} Resolu\u00e7\u00e3o A = \\frac{1}{2}((A + A^T) + (A - A^T)) Ora, mas A + A^T e A - A^T s\u00e3o sim\u00e9trica e antissim\u00e9trica, respectivamente. Logo: A = np . array ([[ 4 , 6 , 3 ],[ - 2 , 0 , 5 ],[ 5 , - 1 , 2 ]]) A = sc . Matrix ( A ) A_T = A . T 1 / 2 * ( A + A_T ) \\left[\\begin{matrix}4.0 & 2.0 & 4.0\\\\2.0 & 0 & 2.0\\\\4.0 & 2.0 & 2.0\\end{matrix}\\right] 1 / 2 * ( A - A_T ) \\left[\\begin{matrix}0 & 4.0 & -1.0\\\\-4.0 & 0 & 3.0\\\\1.0 & -3.0 & 0\\end{matrix}\\right] Exerc\u00edcio 2: Encontre a inversa da seguinte matriz e depois resolva AX = B : A = \\begin{pmatrix} 4 & 6 & 3\\\\ -2 & 0 & 5 \\\\ 5 & -1 & 2 \\end{pmatrix} \\\\ B = \\begin{pmatrix} 7 & 1 & 4\\\\ -5 & 2 & 4 \\\\ 9 & 3 & -1 \\end{pmatrix} Resolu\u00e7\u00e3o A = np . array ([[ 4 , 6 , 3 ],[ - 2 , 0 , 5 ], [ 5 , - 1 , 2 ]]) B = np . array ([[ 7 , 1 , 4 ],[ - 5 , 2 , 5 ],[ 9 , 3 , 1 ]]) A_inv = np . linalg . inv ( A ) X = np . matmul ( A_inv , B ) sc . Matrix ( A_inv ) \\left[\\begin{matrix}0.025 & -0.075 & 0.15\\\\0.145 & -0.035 & -0.13\\\\0.01 & 0.17 & 0.06\\end{matrix}\\right] sc . Matrix ( X ) \\left[\\begin{matrix}1.9 & 0.325 & -0.125\\\\0.0199999999999999 & -0.315 & 0.275\\\\-0.24 & 0.53 & 0.95\\end{matrix}\\right] Exerc\u00edcio 3: Resolva o sistema em \\mathbb{Z}_5 : \\begin{pmatrix} 4 & 1 & 3\\\\ 3 & 0 & 2 \\\\ 0 & 4 & 2 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 4 \\end{pmatrix} Exerc\u00edcio 4: Uma rede consiste em um n\u00famero finito de n\u00f3s conectados por arestas . Cada aresta \u00e9 rotulada com um fluxo , que relaciona uma dire\u00e7\u00e3o e uma quantidade. A regra fundamental \u00e9 a conserva\u00e7\u00e3o de fluxo , isto \u00e9, em cada n\u00f3, o fluxo de entrada \u00e9 igual ao fluxo de sa\u00edda. Considere a seguinte rede: Encontre os intervalos de f_1, f_2, f_3, f_4 Exerc\u00edcio 5: Fa\u00e7a a fatora\u00e7\u00e3o LU da matriz \\begin{pmatrix} 1 & 2 & 4 \\\\ 3 & 8 & 14 \\\\ 2 & 6 & 13 \\end{pmatrix} Resolu\u00e7\u00e3o: A = sp . array ([[ 1 , 2 , 4 ], [ 3 , 8 , 14 ], [ 2 , 6 , 13 ]]) P , L , U = sp . linalg . lu ( A ) print ( \"L:\" ) sc . Matrix ( L ) L: \\left[\\begin{matrix}1.0 & 0.0 & 0.0\\\\0.666666666666667 & 1.0 & 0.0\\\\0.333333333333333 & -0.999999999999999 & 1.0\\end{matrix}\\right] print ( \"U:\" ) sc . Matrix ( U ) U: \\left[\\begin{matrix}3.0 & 8.0 & 14.0\\\\0.0 & 0.666666666666667 & 3.66666666666667\\\\0.0 & 0.0 & 3.0\\end{matrix}\\right] Curiosidades O conjunto H dos pontos x = (x_1,...,x_n) \\in \\mathbb{R^n} , tais que a_1x_1 + ... + a_nx+n = b \u00e9 uma variedade afim . Se a_i n\u00e3o s\u00e3o todos nulos, H \u00e9 um hiperplano. Por que \u00e9 necess\u00e1rio que 0 \\in F , sendo F \\subset E um subespa\u00e7o? Essa condi\u00e7\u00e3o pode ser alterada po outra? Sugest\u00e3o: Demonstrar que essa condi\u00e7\u00e3o pode ser alterada por F \\neq \\emptyset Defini\u00e7\u00e3o de segmento de reta: [u,v] = \\{(1 - t)\\cdot u + t\\cdot v ~; ~ 0 \\leq t \\leq 1\\} , sendo u, v \\in E , espa\u00e7o vetorial.","title":"Monitoria 4"},{"location":"alglin/files/monitoria4/monitoria4/#monitoria-4","text":"","title":"Monitoria 4"},{"location":"alglin/files/monitoria4/monitoria4/#topicos-ja-estudados","text":"Matrizes e suas propriedade Nota\u00e7\u00e3o Opera\u00e7\u00f5es com matrizes e suas propriedades A transposta e suas propriedades: (AB)^T = B^TA^T Sim\u00e9trica e Antissim\u00e9trica: (A + A^T) \u00e9 sim\u00e9trica e (A - A^T) \u00e9 antissim\u00e9trica, \\forall A . A inversa, quando existe, e suas propriedades M\u00e9todo para obten\u00e7\u00e3o da inversa utilizando matrizes elementares Matrizes em outros conjuntos, como Z_5 . Matriz singular: matriz n\u00e3o invert\u00edvel. Decomposi\u00e7\u00e3o LU Sistemas Lineares e sua representa\u00e7\u00e3o matricial Escalonamento: m\u00e9todo de Gauss e m\u00e9todo de Gauss-Jordan A \u00e9 equivalente a B se, e s\u00f3 se, existe uma sequ\u00eancia de opera\u00e7\u00f5es elementares que transformam A em B. Posto de uma matriz e grau de liberdade (Teorema do Posto) Posto (rank em ingl\u00eas) pode ser definido como a dimens\u00e3o do espa\u00e7o linha ou a dimens\u00e3o do espa\u00e7o coluna de uma matriz. Sistema homog\u00eaneo: sempre existe solu\u00e7\u00e3o (trivial) Posto(A) = Posto(A^T) Matrizes Elementares: representantes matriciais das tr\u00eas opera\u00e7\u00f5es elementares, trocar linha, multiplica\u00e7\u00e3o por constante e somar a linha a um m\u00faltiplo de outra linha Matriz Completa: [A|b] Espa\u00e7os Vetorias e vetores Combina\u00e7\u00e3o Linear de vetores O produto escalar -> m\u00f3dulo Proje\u00e7\u00e3o: \\vec{z} = \\frac{\\vec{v}\\cdot\\vec{u}}{\\vec{u}\\cdot\\vec{u}}\\cdot u Desigualdade de Cauchy-Schwarz e cosseno: \\|u\\|\\|v\\| \\geq \\|u\\cdot v\\| Conjuntos geradores: S = \\{v_1,v_2,...,v_k\\} \\subset\\mathbb{R}^n . O conjunto de todas as combina\u00e7\u00f5es dos elementos de S \u00e9 ger(S) . Depen\u00eancia: um conjunto \u00e9 linearmente dependente se um vetor do conjunto pode ser escrito como combina\u00e7\u00e3o linear de outros. Independente, caso contr\u00e1rio. Lembrar que se \\alpha_1\\vec{v_1} + \\alpha_2\\vec{v_2} + ... + \\alpha_k\\vec{v_k} = 0 \\implies \\alpha_1 = \\alpha_2 = ... = \\alpha_k = 0 , o conjuntos dos vetores \u00e9 linearmente independente. Espa\u00e7os vetorias dotam-se de: comutatividade e associatividade da soma, exist\u00eancia do nulo e da identidade, exist\u00eancia do sim\u00e9trico, distribuitividade com real. Subespa\u00e7os vetoriais: tem o vetor nulo e a soma e produto por real s\u00e3o fechados. Subespa\u00e7os associados a matrizes: Espa\u00e7o linha: conjunto gerado por suas linhas: lin(A) = ger(\\{L_1,L_2,...,L_m\\}) Espa\u00e7o coluna: conjunto gerado por suas colunas: col(A) = ger(\\{c_1,c_2,...,c_m\\}) Espa\u00e7o anulado por uma matriz: Tamb\u00e9m chamado de n\u00facleo, \u00e9 o conjunto solu\u00e7\u00e3o do sistema Ax = 0 Bases de um espa\u00e7o vetorial: \u00c9 um conjunto LIque gera um espa\u00e7o vetorial --- Pr\u00f3xima monitoria import numpy as np import scipy as sp import scipy.linalg as splin import sympy as sc","title":"T\u00f3picos j\u00e1 estudados"},{"location":"alglin/files/monitoria4/monitoria4/#exercicios","text":"","title":"Exerc\u00edcios"},{"location":"alglin/files/monitoria4/monitoria4/#exemplo-14-espacos-vetorias","text":"Seja X um conjunto n\u00e3o vazio. O s\u00edmbolo \\mathbb{F}(X;\\mathbb{R}) representa o conjunto das fun\u00e7\u00f5es reais f,g: X \\to \\mathbb{R} . Defino a soma: f + g como (f + g)(x) = f(x) + g(x) e o produto \\alpha\\cdot f como (\\alpha f)(x) = \\alpha\\cdot f(x)","title":"Exemplo 1.4: Espa\u00e7os Vetorias"},{"location":"alglin/files/monitoria4/monitoria4/#exercicio-213-combinacao-linear","text":"Mostre que a matriz d = <script type=\"math/tex; mode=display\">\\begin{pmatrix} 4 & -4 \\\\ -6 & 16 \\end{pmatrix} pode ser escrita como combina\u00e7\u00e3o linear das matrizes: a = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}, b = \\begin{pmatrix} -1 & 2 \\\\ 3 & -4\\end{pmatrix}, c =\\begin{pmatrix} 1 & -2 \\\\ -3 & 4 \\end{pmatrix}","title":"Exerc\u00edcio 2.13: Combina\u00e7\u00e3o Linear"},{"location":"alglin/files/monitoria4/monitoria4/#exemplo-subespacos","text":"Seja P_4 o espa\u00e7o vetorial de todos os polin\u00f4mios de grau 4 ou menos com coeficientes reais. A soma e o produto por real s\u00e3o definidos naturalmente. S_5 \u00e9 um subespa\u00e7o? S_5 = \\{f(x) \\in P_4 \\| f(1)~ is~ a ~rational~ number\\}","title":"Exemplo: Subespa\u00e7os"},{"location":"alglin/files/monitoria4/monitoria4/#questao-teste-2-operacoes-com-matrizes","text":"A matriz A = \\frac{1}{\\sqrt{2}} <script type=\"math/tex; mode=display\">\\begin{pmatrix} 1 & -1 \\\\ 1 & 1 \\end{pmatrix} . Quest\u00e3o: \\exists n \\in \\mathbb{N} , tal que A^n = A ? Obs.: Observe que \\frac{1}{\\sqrt{2}} = \\frac{\\sqrt{2}}{2} , e se eu tratar \\theta = \\frac{\\pi}{4} , temos que A \u00e9 matriz de rota\u00e7\u00e3o. Logo, basta encontrar n tal que n\\cdot\\frac{\\pi}{4} = k\\cdot2\\cdot\\pi \\to n = 8\\cdot k \\to n = 8 \u00e9 solu\u00e7\u00e3o, logo A^8 = I \\to A^9 = A","title":"Quest\u00e3o Teste 2: Opera\u00e7\u00f5es com matrizes"},{"location":"alglin/files/monitoria4/monitoria4/#questao-237","text":"Dado X \\subset E , seja Y o conjunto obtido de X substituindo um dos seus elementos v por v + \\alpha u , onde u\\in X e \\alpha \\in \\mathbb{R} . Prove que X e Y geram o mesmo subespa\u00e7o vetorial de E . Conclua, ent\u00e3o que \\{v_1,...,v_k\\} \\subset E e \\{v_1, v_2 - v_1, ..., v_k - v_1\\} \\subset E geram o mesmo subespa\u00e7o vetorial de E . Algu\u00e9m percebe a implica\u00e7\u00e3o pr\u00e1tica dessa quest\u00e3o? Lembre que S(Y) \u00e9 o subespa\u00e7o gerado por Y. Uma ajuda para desenvolver essa quest\u00e3o \u00e9 reconhecer que X \\subset S(Y) \\implies S(X) \\subset S(Y) . Voc\u00ea consegue demonstrar?","title":"Quest\u00e3o 2.37"},{"location":"alglin/files/monitoria4/monitoria4/#conjuntos-geradores","text":"u_1 = (1,1,2,3) u_2 = (2,-1,1,2) \\pi = ger(\\{u_1, u_2\\}) w = (-2,7,5,6) \\in \\pi Queremos saber se, \\exists ~\\alpha, \\beta , tal que: \\alpha \\begin{pmatrix} 1 \\\\ 1 \\\\ 2 \\\\ 3 \\end{pmatrix} + \\beta \\begin{pmatrix} 2 \\\\ -1 \\\\ 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ 7 \\\\ 5 \\\\ 6 \\end{pmatrix}","title":"Conjuntos Geradores"},{"location":"alglin/files/monitoria4/monitoria4/#teste-1-2018","text":"","title":"Teste 1 2018"},{"location":"alglin/files/monitoria4/monitoria4/#exercicio-1","text":"Escreva a seguinte matriz como a soma de uma matriz sim\u00e9trica e uma antissim\u00e9trica: A = <script type=\"math/tex; mode=display\">\\begin{pmatrix} 4 & 6 & 3\\\\ -2 & 0 & 5 \\\\ 5 & -1 & 2 \\end{pmatrix}","title":"Exerc\u00edcio 1:"},{"location":"alglin/files/monitoria4/monitoria4/#resolucao","text":"A = \\frac{1}{2}((A + A^T) + (A - A^T)) Ora, mas A + A^T e A - A^T s\u00e3o sim\u00e9trica e antissim\u00e9trica, respectivamente. Logo: A = np . array ([[ 4 , 6 , 3 ],[ - 2 , 0 , 5 ],[ 5 , - 1 , 2 ]]) A = sc . Matrix ( A ) A_T = A . T 1 / 2 * ( A + A_T ) \\left[\\begin{matrix}4.0 & 2.0 & 4.0\\\\2.0 & 0 & 2.0\\\\4.0 & 2.0 & 2.0\\end{matrix}\\right] 1 / 2 * ( A - A_T ) \\left[\\begin{matrix}0 & 4.0 & -1.0\\\\-4.0 & 0 & 3.0\\\\1.0 & -3.0 & 0\\end{matrix}\\right]","title":"Resolu\u00e7\u00e3o"},{"location":"alglin/files/monitoria4/monitoria4/#exercicio-2","text":"Encontre a inversa da seguinte matriz e depois resolva AX = B : A = \\begin{pmatrix} 4 & 6 & 3\\\\ -2 & 0 & 5 \\\\ 5 & -1 & 2 \\end{pmatrix} \\\\ B = \\begin{pmatrix} 7 & 1 & 4\\\\ -5 & 2 & 4 \\\\ 9 & 3 & -1 \\end{pmatrix}","title":"Exerc\u00edcio 2:"},{"location":"alglin/files/monitoria4/monitoria4/#resolucao_1","text":"A = np . array ([[ 4 , 6 , 3 ],[ - 2 , 0 , 5 ], [ 5 , - 1 , 2 ]]) B = np . array ([[ 7 , 1 , 4 ],[ - 5 , 2 , 5 ],[ 9 , 3 , 1 ]]) A_inv = np . linalg . inv ( A ) X = np . matmul ( A_inv , B ) sc . Matrix ( A_inv ) \\left[\\begin{matrix}0.025 & -0.075 & 0.15\\\\0.145 & -0.035 & -0.13\\\\0.01 & 0.17 & 0.06\\end{matrix}\\right] sc . Matrix ( X ) \\left[\\begin{matrix}1.9 & 0.325 & -0.125\\\\0.0199999999999999 & -0.315 & 0.275\\\\-0.24 & 0.53 & 0.95\\end{matrix}\\right]","title":"Resolu\u00e7\u00e3o"},{"location":"alglin/files/monitoria4/monitoria4/#exercicio-3","text":"Resolva o sistema em \\mathbb{Z}_5 : \\begin{pmatrix} 4 & 1 & 3\\\\ 3 & 0 & 2 \\\\ 0 & 4 & 2 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 4 \\end{pmatrix}","title":"Exerc\u00edcio 3:"},{"location":"alglin/files/monitoria4/monitoria4/#exercicio-4","text":"Uma rede consiste em um n\u00famero finito de n\u00f3s conectados por arestas . Cada aresta \u00e9 rotulada com um fluxo , que relaciona uma dire\u00e7\u00e3o e uma quantidade. A regra fundamental \u00e9 a conserva\u00e7\u00e3o de fluxo , isto \u00e9, em cada n\u00f3, o fluxo de entrada \u00e9 igual ao fluxo de sa\u00edda. Considere a seguinte rede: Encontre os intervalos de f_1, f_2, f_3, f_4","title":"Exerc\u00edcio 4:"},{"location":"alglin/files/monitoria4/monitoria4/#exercicio-5","text":"Fa\u00e7a a fatora\u00e7\u00e3o LU da matriz \\begin{pmatrix} 1 & 2 & 4 \\\\ 3 & 8 & 14 \\\\ 2 & 6 & 13 \\end{pmatrix}","title":"Exerc\u00edcio 5:"},{"location":"alglin/files/monitoria4/monitoria4/#resolucao_2","text":"A = sp . array ([[ 1 , 2 , 4 ], [ 3 , 8 , 14 ], [ 2 , 6 , 13 ]]) P , L , U = sp . linalg . lu ( A ) print ( \"L:\" ) sc . Matrix ( L ) L: \\left[\\begin{matrix}1.0 & 0.0 & 0.0\\\\0.666666666666667 & 1.0 & 0.0\\\\0.333333333333333 & -0.999999999999999 & 1.0\\end{matrix}\\right] print ( \"U:\" ) sc . Matrix ( U ) U: \\left[\\begin{matrix}3.0 & 8.0 & 14.0\\\\0.0 & 0.666666666666667 & 3.66666666666667\\\\0.0 & 0.0 & 3.0\\end{matrix}\\right]","title":"Resolu\u00e7\u00e3o:"},{"location":"alglin/files/monitoria4/monitoria4/#curiosidades","text":"O conjunto H dos pontos x = (x_1,...,x_n) \\in \\mathbb{R^n} , tais que a_1x_1 + ... + a_nx+n = b \u00e9 uma variedade afim . Se a_i n\u00e3o s\u00e3o todos nulos, H \u00e9 um hiperplano. Por que \u00e9 necess\u00e1rio que 0 \\in F , sendo F \\subset E um subespa\u00e7o? Essa condi\u00e7\u00e3o pode ser alterada po outra? Sugest\u00e3o: Demonstrar que essa condi\u00e7\u00e3o pode ser alterada por F \\neq \\emptyset Defini\u00e7\u00e3o de segmento de reta: [u,v] = \\{(1 - t)\\cdot u + t\\cdot v ~; ~ 0 \\leq t \\leq 1\\} , sendo u, v \\in E , espa\u00e7o vetorial.","title":"Curiosidades"},{"location":"analisenum/computing/","text":"Aritm\u00e9tica do computador No computador, express\u00f5es como (\\sqrt{3})^2 = 3 n\u00e3o s\u00e3o verdadeiras. Em Python, >>> (3**(1/2))**2 2.9999999999999996 Erros de c\u00e1lculo realizados pela m\u00e1quina s\u00e3o chamados de erros de arredondamento (round-off error) . Isso acontece porque n\u00fameros reais s\u00e3o representados de forma finita, e n\u00e3o cobrem todos os n\u00fameros reais de fato. N\u00fameros bin\u00e1rios Em 1985, IEEE (Institute for Electrical and Electronic Engineers) publicou um relat\u00f3rio nomeado Binary Floating Point Arithmetic Standard 754\u20131985 que estabelece padr\u00f5es para pontos flutuantes, algoritmos de arredondamento de opera\u00e7\u00f5es aritm\u00e9ticas e para lidar com exce\u00e7\u00f5es. A representa\u00e7\u00e3o usual dos n\u00fameros reais \u00e9 64-bit ( double ). O primeiro bit \u00e9 o sinal do n\u00famero. Depois vem a caracter\u00edstica que \u00e9 representa o expoente (11-bit) e por fim a fra\u00e7\u00e3o bin\u00e1ria chamada de mantissa (52-bit). Os n\u00fameros s\u00e3o armazenados na base 2. A representa\u00e7\u00e3o ponto flutuante dos n\u00fameros \u00e9, portanto, (-1)^s2^{c-1023}(1 + f), em que s \u00e9 o sinal (0 para positivo e 1 para negativo), c \u00e9 o expoente (tiramos o vi\u00e9s 1023 para assegurar que o intervalo de c fique entre -1023 e 1024) e f \u00e9 a mantissa. Exemplo: s c f 0 10000000011 1011100100010000000000000000000000000000000000000000 Nesse caso o sinal \u00e9 0, c = 1\\cdot 2^{10} + 0\\cdot 2^9 + \\dots + 1\\cdot 2 + 1 = 1027 , e f = 1\\cdot 2^{-1} + 1\\cdot 2^{-3} + \\dots 1\\cdot 2^{-12} = 0.722900390625 e, portanto, (-1)^s2^{c-1023}(1 + f) = 2^4(1 + 0.722900390625) = 27.56640625. O menor n\u00famero positivo que pode ser representado \u00e9, portanto, 2^{0-1022}(1 + 0) \\approx 10^{-307} e o maior 2^{2047 - 1023}(1 + (1 -2^{-52})) \\approx 10^{309} Quando s = c = f = 0 , representamos o 0. Note que quando um n\u00famero tem representa\u00e7\u00e3o infinita, ele deve ser, de alguma forma, colocado em forma finita. Duas maneiras comuns de se fazer isso \u00e9 chopping , quando se simplesmente corta os d\u00edgitos a mais, ou arredondamento . A representa\u00e7\u00e3o de x em ponto flutuante \u00e9 denotada por fl(x) . O infinito \u00e9 representado quando todos os valores do expoente s\u00e3o 1. OS valores de NaN (QNaN e SNaN) variam a mantissa e mant\u00e9m os expoentes todos 1. Um ponto interessante \u00e9 a densidade dos n\u00fameros ponto flutuante. Quanto maior fica o n\u00famero, menor a densidade. Isso acontece porque o tamanho da mantissa \u00e9 fixo, ent\u00e3o sempre v\u00e3o existir 2^{52} n\u00fameros representados para cada expoente. Assim o intervalo [2^{k}, 2^{k+1}) tem n\u00famero fixo de n\u00fameros que independe de k , apesar da dist\u00e2ncia crescer. Por esse motivo tamb\u00e9m, 2^{-52} \u00e9 o epsilon no sistema 64-bit. Representa\u00e7\u00e3o ponto flutuante Vamos mostrar um exemplo de como transformar um n\u00famero real 10.1 em um ponto flutuante. Separar parte inteira da parte decimal. Transformar a parte inteira e a parte fracion\u00e1ria em sua representa\u00e7\u00e3o bin\u00e1ria . Deixar o n\u00famero no formato 1.x_1x_2\\dots \\cdot 2^{\\text{expoente}} . Adicionar o vi\u00e9s 1023 ao expoente. Converter o expoente em bin\u00e1rio. Casa 53: se for 1, soma-se a casa 52. Para conferir, voc\u00ea pode verificar a representa\u00e7\u00e3o utilizando alguma linguagem de programa\u00e7\u00e3o. Por exemplo, em Python, >>> import struct def binary(num): s = struct.pack('!d', num) # packs the num to the decimal format s = struct.unpack('!Q', s)[0] # unpacks in long integer format. s = bin(s) # converts to binary form. s = s[2:].zfill(64) # fills with zeros until reaches length 64. return s[0] + ' ' + s[1:12] + ' ' + s[12:] >>> binary(1/2) '0 01111111110 0000000000000000000000000000000000000000000000000000' >>> binary(1/7) '0 01111111100 0010010010010010010010010010010010010010010010010010' Mensura\u00e7\u00e3o de erros Seja \\bar{x} uma aproxima\u00e7\u00e3o para x . Dizemos que o erro absoluto \u00e9 |x - \\bar{x}| e o erro relativo \u00e9 |x - \\bar{x}|/|x| quando x \\neq 0 . Dizemos que \\bar{x} aproxima x em t d\u00edgitos significativos se t = \\max\\left\\{s \\ge 0 \\, \\bigg| \\, \\frac{|x - \\bar{x}|}{|x|} \\le 5 \\cdot 10^{-s} \\right\\} Algumas dicas com Python Para printar todos os n\u00fameros da representa\u00e7\u00e3o num\u00e9rica (considerando arredondamento) em Python, use >>> import numpy >>> format(numpy.pi, .20) '3.141592653589793116' Para verificar o valor exatamente armazenado para o n\u00famero em quest\u00e3o: >>> from decimal import Decimal >>> Decimal.from_float(0.1) Decimal('0.1000000000000000055511151231257827021181583404541015625') Para mais detalhes, consulte o material adicional","title":"Aritm\u00e9tica do computador"},{"location":"analisenum/computing/#aritmetica-do-computador","text":"No computador, express\u00f5es como (\\sqrt{3})^2 = 3 n\u00e3o s\u00e3o verdadeiras. Em Python, >>> (3**(1/2))**2 2.9999999999999996 Erros de c\u00e1lculo realizados pela m\u00e1quina s\u00e3o chamados de erros de arredondamento (round-off error) . Isso acontece porque n\u00fameros reais s\u00e3o representados de forma finita, e n\u00e3o cobrem todos os n\u00fameros reais de fato.","title":"Aritm\u00e9tica do computador"},{"location":"analisenum/computing/#numeros-binarios","text":"Em 1985, IEEE (Institute for Electrical and Electronic Engineers) publicou um relat\u00f3rio nomeado Binary Floating Point Arithmetic Standard 754\u20131985 que estabelece padr\u00f5es para pontos flutuantes, algoritmos de arredondamento de opera\u00e7\u00f5es aritm\u00e9ticas e para lidar com exce\u00e7\u00f5es. A representa\u00e7\u00e3o usual dos n\u00fameros reais \u00e9 64-bit ( double ). O primeiro bit \u00e9 o sinal do n\u00famero. Depois vem a caracter\u00edstica que \u00e9 representa o expoente (11-bit) e por fim a fra\u00e7\u00e3o bin\u00e1ria chamada de mantissa (52-bit). Os n\u00fameros s\u00e3o armazenados na base 2. A representa\u00e7\u00e3o ponto flutuante dos n\u00fameros \u00e9, portanto, (-1)^s2^{c-1023}(1 + f), em que s \u00e9 o sinal (0 para positivo e 1 para negativo), c \u00e9 o expoente (tiramos o vi\u00e9s 1023 para assegurar que o intervalo de c fique entre -1023 e 1024) e f \u00e9 a mantissa. Exemplo: s c f 0 10000000011 1011100100010000000000000000000000000000000000000000 Nesse caso o sinal \u00e9 0, c = 1\\cdot 2^{10} + 0\\cdot 2^9 + \\dots + 1\\cdot 2 + 1 = 1027 , e f = 1\\cdot 2^{-1} + 1\\cdot 2^{-3} + \\dots 1\\cdot 2^{-12} = 0.722900390625 e, portanto, (-1)^s2^{c-1023}(1 + f) = 2^4(1 + 0.722900390625) = 27.56640625. O menor n\u00famero positivo que pode ser representado \u00e9, portanto, 2^{0-1022}(1 + 0) \\approx 10^{-307} e o maior 2^{2047 - 1023}(1 + (1 -2^{-52})) \\approx 10^{309} Quando s = c = f = 0 , representamos o 0. Note que quando um n\u00famero tem representa\u00e7\u00e3o infinita, ele deve ser, de alguma forma, colocado em forma finita. Duas maneiras comuns de se fazer isso \u00e9 chopping , quando se simplesmente corta os d\u00edgitos a mais, ou arredondamento . A representa\u00e7\u00e3o de x em ponto flutuante \u00e9 denotada por fl(x) . O infinito \u00e9 representado quando todos os valores do expoente s\u00e3o 1. OS valores de NaN (QNaN e SNaN) variam a mantissa e mant\u00e9m os expoentes todos 1. Um ponto interessante \u00e9 a densidade dos n\u00fameros ponto flutuante. Quanto maior fica o n\u00famero, menor a densidade. Isso acontece porque o tamanho da mantissa \u00e9 fixo, ent\u00e3o sempre v\u00e3o existir 2^{52} n\u00fameros representados para cada expoente. Assim o intervalo [2^{k}, 2^{k+1}) tem n\u00famero fixo de n\u00fameros que independe de k , apesar da dist\u00e2ncia crescer. Por esse motivo tamb\u00e9m, 2^{-52} \u00e9 o epsilon no sistema 64-bit.","title":"N\u00fameros bin\u00e1rios"},{"location":"analisenum/computing/#representacao-ponto-flutuante","text":"Vamos mostrar um exemplo de como transformar um n\u00famero real 10.1 em um ponto flutuante. Separar parte inteira da parte decimal. Transformar a parte inteira e a parte fracion\u00e1ria em sua representa\u00e7\u00e3o bin\u00e1ria . Deixar o n\u00famero no formato 1.x_1x_2\\dots \\cdot 2^{\\text{expoente}} . Adicionar o vi\u00e9s 1023 ao expoente. Converter o expoente em bin\u00e1rio. Casa 53: se for 1, soma-se a casa 52. Para conferir, voc\u00ea pode verificar a representa\u00e7\u00e3o utilizando alguma linguagem de programa\u00e7\u00e3o. Por exemplo, em Python, >>> import struct def binary(num): s = struct.pack('!d', num) # packs the num to the decimal format s = struct.unpack('!Q', s)[0] # unpacks in long integer format. s = bin(s) # converts to binary form. s = s[2:].zfill(64) # fills with zeros until reaches length 64. return s[0] + ' ' + s[1:12] + ' ' + s[12:] >>> binary(1/2) '0 01111111110 0000000000000000000000000000000000000000000000000000' >>> binary(1/7) '0 01111111100 0010010010010010010010010010010010010010010010010010'","title":"Representa\u00e7\u00e3o ponto flutuante"},{"location":"analisenum/computing/#mensuracao-de-erros","text":"Seja \\bar{x} uma aproxima\u00e7\u00e3o para x . Dizemos que o erro absoluto \u00e9 |x - \\bar{x}| e o erro relativo \u00e9 |x - \\bar{x}|/|x| quando x \\neq 0 . Dizemos que \\bar{x} aproxima x em t d\u00edgitos significativos se t = \\max\\left\\{s \\ge 0 \\, \\bigg| \\, \\frac{|x - \\bar{x}|}{|x|} \\le 5 \\cdot 10^{-s} \\right\\}","title":"Mensura\u00e7\u00e3o de erros"},{"location":"analisenum/computing/#algumas-dicas-com-python","text":"Para printar todos os n\u00fameros da representa\u00e7\u00e3o num\u00e9rica (considerando arredondamento) em Python, use >>> import numpy >>> format(numpy.pi, .20) '3.141592653589793116' Para verificar o valor exatamente armazenado para o n\u00famero em quest\u00e3o: >>> from decimal import Decimal >>> Decimal.from_float(0.1) Decimal('0.1000000000000000055511151231257827021181583404541015625') Para mais detalhes, consulte o material adicional","title":"Algumas dicas com Python"},{"location":"analisenum/info/","text":"Informa\u00e7\u00f5es Gerais Monitoria de Introdu\u00e7\u00e3o \u00e0 An\u00e1lise Num\u00e9rica correspondente ao per\u00edodo de 2021.2. Dia: Sexta-feira, 11:10am at\u00e9 12:50pm Link Google Meet Monitorias gravadas Ementa da disciplina T\u00f3picos Computa\u00e7\u00e3o Aritm\u00e9tica do computador Algoritmos M\u00e9todos iterativos para sistemas lineares Solu\u00e7\u00e3o num\u00e9rica de equa\u00e7\u00f5es n\u00e3o lineares Aplica\u00e7\u00e3o m\u00e9todo de Newton Listas N\u00famero Itens discutidos Solu\u00e7\u00e3o 1 Aritm\u00e9tica do computador e equa\u00e7\u00e3o recursiva 1 2 M\u00e9todos iterativos para sistemas de equa\u00e7\u00f5es lineares 2 Notas Monitoria Itens discutidos Arquivo V\u00eddeo 20/08/2021 Lista 1 e an\u00e1lise de estabilidade Visualizar N\u00e3o Provas Ano Bimestre Prova Solu\u00e7\u00f5es 2020 A1 ver arquivo ver arquivo 2021 A1 ver arquivo ver arquivo Sugest\u00f5es Adicionais Disasters attributable to bad numerical computing M\u00e9todo Gradiente Conjugado","title":"An\u00e1lise Num\u00e9rica"},{"location":"analisenum/info/#informacoes-gerais","text":"Monitoria de Introdu\u00e7\u00e3o \u00e0 An\u00e1lise Num\u00e9rica correspondente ao per\u00edodo de 2021.2. Dia: Sexta-feira, 11:10am at\u00e9 12:50pm Link Google Meet Monitorias gravadas Ementa da disciplina","title":"Informa\u00e7\u00f5es Gerais"},{"location":"analisenum/info/#topicos","text":"Computa\u00e7\u00e3o Aritm\u00e9tica do computador Algoritmos M\u00e9todos iterativos para sistemas lineares Solu\u00e7\u00e3o num\u00e9rica de equa\u00e7\u00f5es n\u00e3o lineares Aplica\u00e7\u00e3o m\u00e9todo de Newton","title":"T\u00f3picos"},{"location":"analisenum/info/#listas","text":"N\u00famero Itens discutidos Solu\u00e7\u00e3o 1 Aritm\u00e9tica do computador e equa\u00e7\u00e3o recursiva 1 2 M\u00e9todos iterativos para sistemas de equa\u00e7\u00f5es lineares 2","title":"Listas"},{"location":"analisenum/info/#notas","text":"Monitoria Itens discutidos Arquivo V\u00eddeo 20/08/2021 Lista 1 e an\u00e1lise de estabilidade Visualizar N\u00e3o","title":"Notas"},{"location":"analisenum/info/#provas","text":"Ano Bimestre Prova Solu\u00e7\u00f5es 2020 A1 ver arquivo ver arquivo 2021 A1 ver arquivo ver arquivo","title":"Provas"},{"location":"analisenum/info/#sugestoes-adicionais","text":"Disasters attributable to bad numerical computing M\u00e9todo Gradiente Conjugado","title":"Sugest\u00f5es Adicionais"},{"location":"analisenum/linear-systems/","text":"M\u00e9todos iterativos para resolver sistemas lineares Suponha que queremos resolver um problema do tipo Ax = b , em que A \u00e9 uma matriz real n \\times n e b \u00e9 um vetor em \\mathbb{R}^n . Matematicamente, se estamos interessados em encontrar x , podemos apenas calcular a inversa de A , caso exista. Se sim x = A^{-1}b . Apesar de ser f\u00e1cil de calcular, esse procedimento precisa fazer O(n^3) opera\u00e7\u00f5es, o que pode ser muito custoso quando n aumenta. Nesse caso, precisamos de alternativas mais palat\u00e1veis para resolver esse problema, principalmente para n grande. Utilizamos m\u00e9todos iterativos para ajudar! De forma geral, vamos querer reescrever o problema da forma: x = Cx + D, em que s\u00e3o matrizes. Se conseguirmos expressar Ax = b dessa forma, estaremos interessados nos pontos fixos do operador L(x) = Cx + D . Teorema: O processo iterativo x^{k+1} = Cx^k + D satisfaz o seguinte: Para todo valor inicial x^0 , a sequ\u00eancia \\{x^k\\}_{k \\in \\mathbb{N}} converge para o ponto fixo x^* = Cx^* + D se, e somente se, \\rho(C) < 1 , em que \\rho(C) \u00e9 o raio espectral da matriz C , isto \u00e9, o maior autovalor em m\u00f3dulo. Uma demonstra\u00e7\u00e3o desse resultado pode ser encontrado no livro de Richard L.Burden Numerical Analysis (p\u00e1gina 457). Corol\u00e1rio: Se ||C|| < 1 para qualquer norma matricial induzida (induzida por uma norma vetorial), ent\u00e3o a itera\u00e7\u00e3o anterior converge para o ponto fixo do operador L para qualquer chute inicial x^0 . Esse resultado \u00e9 uma consequ\u00eancia de \\rho(A) \\le ||A|| para todo norma natural ||\\cdot|| . M\u00e9todo de Jacobi M\u00e9todo de Gauss-Seidel M\u00e9todo Successive Over-Relaxation (SOR) M\u00e9todo Gradiente Conjugado Teorema: Se A \u00e9 diagonalmente estritamente dominante, ent\u00e3o para qualquer escolha de x^0 , os m\u00e9todos de Jacobi e Gauss-Seidel convergem. M\u00e9todo de Jacobi Esse m\u00e9todo \u00e9 derivado resolvendo a i th equa\u00e7\u00e3o de Ax = b para x_i (dado que a_{ii} \\neq 0 ) x_i = \\sum_{j\\neq i} \\left(-\\frac{a_{ij}x_j}{a_{ii}}\\right) + \\frac{b_i}{a_{ii}}, ~~~ \\text{ for } i = 1, 2, \\dots, n. Assim, geramos iterativamente, x_i^{(k)} = \\frac{1}{a_{ii}} \\left[ \\sum_{j\\neq i} \\left(-a_{ij} x_j^{(k-1)}\\right) + b_i\\right] Vamos escrever em formato matricial. Observe que x^{(k)} = \\begin{bmatrix} 0 & -a_{12}/a_{11} & -a_{13}/a_{11} & \\dots & -a_{1n}/a_{11} \\\\ -a_{21}/a_{22} & 0 & -a_{23}/a_{22} & \\dots & -a_{2n}/a_{22} \\\\ -a_{31}/a_{33} & -a_{32}/a_{33} & 0 & \\dots & -a_{3n}/a_{33} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ -a_{n1}/a_{nn} & -a_{n2}/a_{nn} & -a_{n3}/a_{nn} & \\dots & 0\\\\ \\end{bmatrix}x^{(k-1)} + \\begin{bmatrix} b_1/a_{11} \\\\ b_2/a_{22} \\\\ b_3/a_{33} \\\\ \\vdots \\\\ b_n/a_{nn} \\end{bmatrix} e sendo U e L as matrizes triangulares superiores e inferiores, e D a matriz diagonal de A , x^{(k)} = -D^{-1}(L + U)x^{(k-1)} + D^{-1}b Assim, o procedimento Jacobi pode ser sumarizado da seguinte forma: Sejam as entradas as matrizes A e b com uma aproxima\u00e7\u00e3o inicial x^{(0)} . Defina uma toler\u00e2ncia para o crit\u00e9rio de parada. Realize a itera\u00e7\u00e3o at\u00e9 o crit\u00e9rio de parada. Lembre de pr\u00e9-calcular as matrizes D^{-1}(L + U) e D^{-1}b . Observa\u00e7\u00e3o: Na nota\u00e7\u00e3o do livro, troca-se L e U por -L e -U para n\u00e3o aparecer o sinal na express\u00e3o acima. Acelerar a converg\u00eancia: Colocar a_{ii} o m\u00e1ximo poss\u00edvel acelera a converg\u00eancia do m\u00e9todo. M\u00e9todo de Gauss-Seidel Observe que quando iteramos o processo no m\u00e9todo de Jacobi, fixamos x^{(k-1)} e fazemos a opera\u00e7\u00e3o x^{(k)} . Por esse motivo, podemos utilizar os valores j\u00e1 calculados x_1^{(k)}, \\dots, x_{i-1}^{(k)} para atualizar o valor de x_i^{(k)} . Assim x_i^{(k)} = \\frac{1}{a_{ii}} \\left[ -\\sum_{j=1}^{i-1} \\left(a_{ij} x_j^{(k)}\\right) - \\sum_{j=i+1}^{n} \\left(a_{ij} x_j^{(k-1)}\\right) + b_i\\right] Essa simples modifica\u00e7\u00e3o gerou o m\u00e9todo de Gauss-Seidel. Para obter essa equa\u00e7\u00e3o em formato matricial, precisamos de \\sum_{j=1}^{i} \\left(a_{ij} x_j^{(k)}\\right) = - \\sum_{j=i+1}^{n} \\left(a_{ij} x_j^{(k-1)}\\right) + b_i Note que para cada linha i , estamos tomando todas as colunas j \\le i no lado esquerdo. No lado direito \u00e9 o contr\u00e1rio. Assim, (D + L)x^{(k)} = Ux^{(k-1)} + b \\implies x^{(k)} = -(D+L)^{-1}Ux^{(k-1)} + (D + L)^{-1}b O processo iterativo \u00e9 similar ao M\u00e9todo de Jordan. Teorema (Stein-Rosenberg): Se a_{ij} \\le 0 para cada i \\neq j e a_{ii} > 0 , para cada i=1,2,\\dots,n , ent\u00e3o uma, e somente uma, das afirma\u00e7\u00f5es vale: (i) 0 \\le \\rho(T_g) < \\rho(T_j) < 1 ; (ii) 1 < \\rho(T_j) < \\rho(T_g) ; (iii) \\rho(T_g) = \\rho(T_j) = 0 ; (iv) \\rho(T_g) = \\rho(T_j) = 1 , em que T_j = -D^{-1}(L+U) e T_g = -(D+L)^{-1}U s\u00e3o as matrizes dos m\u00e9todos de Jacobi e Gauss-Seidel. Converg\u00eancia Seja Ax = b o sistema com solu\u00e7\u00e3o x^* e o m\u00e9todo iterativo x^{k+1} = Cx^k +D com x^* = Cx^* + D . Assim se ||C|| < 1 , o m\u00e9todo converge para a solu\u00e7\u00e3o com os seguintes limites no erro: (i) ||x^* - x^{k}|| \\le ||C||^k||x^* - x^0|| . (ii) ||x^* - x^{k}|| \\le \\frac{||C||^k}{1 - ||C||}||x^1 - x^0|| . Assim, percebemos que a converg\u00eancia depende de ||C|| \\approx \\rho(C) (isso \u00e9 verdade porque para todo \\epsilon > 0 , existe uma norma matricial natural tal que \\rho(C) < ||C|| < \\rho(C) + \\epsilon . M\u00e9todo Successive Over-Relaxation (SOR) Baseada na an\u00e1lise de converg\u00eancia da \u00faltima se\u00e7\u00e3o, estamos interessados em minimizar \\rho(C) de maneira geral. Para isso introduz-se o SOR. O vetor res\u00edduo \u00e9 dado por r = b - A\\tilde{x} , em que \\tilde{x} \u00e9 uma aproxima\u00e7\u00e3o para a solu\u00e7\u00e3o de Ax = b . Seja r_{ii}^{(k)} = (r_{1i}^{(k)}, r_{2i}^{(k)}, \\dots, r_{ni}^{(k)}) o vetor res\u00edduo para x_{i}^{(k)} = (x_1^{(k)}, \\dots, x_{i-1}^{(k)}, x_i^{(k-1)}, \\dots, x_n^{(k)}) . Em particular, o m\u00e9todo de Gauss-Seidel pode ser reescrito de forma a x_{i}^{(k)} = x_i^{(k-1)} + \\frac{r_{ii}^{(k)}}{a_{ii}}. Para isso, a ideia ser\u00e1 escolher \\omega de forma a acelerar a converg\u00eancia e x_{i}^{(k)} = x_i^{(k-1)} + \\omega\\frac{r_{ii}^{(k)}}{a_{ii}}. Se \\omega \\in (0,1) , o m\u00e9todo \u00e9 sob-relaxamento. Se \\omega \\in (1,2) , o m\u00e9todo \u00e9 sobre-relaxamento. Em formato iterativo, x_i^{(k)} = (1 - \\omega)x_{i}^{(k-1)} + \\frac{\\omega}{a_{ii}}\\left[b_i - \\sum_{j-1}^{i-1} a_{ij}x_j^{(k)} - \\sum_{j=i+1}^n a_{ij}x_{j}^{(k-1)} \\right] que em formato matricial se reduz a x^{(k)} = (D + \\omega L)^{-1}[(1-\\omega)D - \\omega U]x^{(k-1)} + \\omega(D + \\omega L)^{-1}b. Teorema (Kahan): Se a_{ii} \\neq 0 para todo i=1,\\dots,n , ent\u00e3o o m\u00e9todo converge somente se 0 < \\omega < 2 . Esse resultado pode ser obtido calculando o raio espectral da matriz da itera\u00e7\u00e3o SOR. Teorema (Ostrowski-Reich): Se A \u00e9 matriz positiva definida e 0 < \\omega < 2 , ent\u00e3o o m\u00e9todo SOR converge para todo x^{(0)} . Por fim, \u00e9 importante destacar que para minimizarmos o raio espectral, a escolha \u00f3tima de \\omega \u00e9 dada por \\omega = \\frac{2}{1 + \\sqrt{1 - [\\rho(T_j)]^2}}, em que T_j \u00e9 a matriz de itera\u00e7\u00e3o de Jacobi. Gradiente Conjugado","title":"M\u00e9todos iterativos para resolver sistemas lineares"},{"location":"analisenum/linear-systems/#metodos-iterativos-para-resolver-sistemas-lineares","text":"Suponha que queremos resolver um problema do tipo Ax = b , em que A \u00e9 uma matriz real n \\times n e b \u00e9 um vetor em \\mathbb{R}^n . Matematicamente, se estamos interessados em encontrar x , podemos apenas calcular a inversa de A , caso exista. Se sim x = A^{-1}b . Apesar de ser f\u00e1cil de calcular, esse procedimento precisa fazer O(n^3) opera\u00e7\u00f5es, o que pode ser muito custoso quando n aumenta. Nesse caso, precisamos de alternativas mais palat\u00e1veis para resolver esse problema, principalmente para n grande. Utilizamos m\u00e9todos iterativos para ajudar! De forma geral, vamos querer reescrever o problema da forma: x = Cx + D, em que s\u00e3o matrizes. Se conseguirmos expressar Ax = b dessa forma, estaremos interessados nos pontos fixos do operador L(x) = Cx + D . Teorema: O processo iterativo x^{k+1} = Cx^k + D satisfaz o seguinte: Para todo valor inicial x^0 , a sequ\u00eancia \\{x^k\\}_{k \\in \\mathbb{N}} converge para o ponto fixo x^* = Cx^* + D se, e somente se, \\rho(C) < 1 , em que \\rho(C) \u00e9 o raio espectral da matriz C , isto \u00e9, o maior autovalor em m\u00f3dulo. Uma demonstra\u00e7\u00e3o desse resultado pode ser encontrado no livro de Richard L.Burden Numerical Analysis (p\u00e1gina 457). Corol\u00e1rio: Se ||C|| < 1 para qualquer norma matricial induzida (induzida por uma norma vetorial), ent\u00e3o a itera\u00e7\u00e3o anterior converge para o ponto fixo do operador L para qualquer chute inicial x^0 . Esse resultado \u00e9 uma consequ\u00eancia de \\rho(A) \\le ||A|| para todo norma natural ||\\cdot|| . M\u00e9todo de Jacobi M\u00e9todo de Gauss-Seidel M\u00e9todo Successive Over-Relaxation (SOR) M\u00e9todo Gradiente Conjugado Teorema: Se A \u00e9 diagonalmente estritamente dominante, ent\u00e3o para qualquer escolha de x^0 , os m\u00e9todos de Jacobi e Gauss-Seidel convergem.","title":"M\u00e9todos iterativos para resolver sistemas lineares"},{"location":"analisenum/linear-systems/#metodo-de-jacobi","text":"Esse m\u00e9todo \u00e9 derivado resolvendo a i th equa\u00e7\u00e3o de Ax = b para x_i (dado que a_{ii} \\neq 0 ) x_i = \\sum_{j\\neq i} \\left(-\\frac{a_{ij}x_j}{a_{ii}}\\right) + \\frac{b_i}{a_{ii}}, ~~~ \\text{ for } i = 1, 2, \\dots, n. Assim, geramos iterativamente, x_i^{(k)} = \\frac{1}{a_{ii}} \\left[ \\sum_{j\\neq i} \\left(-a_{ij} x_j^{(k-1)}\\right) + b_i\\right] Vamos escrever em formato matricial. Observe que x^{(k)} = \\begin{bmatrix} 0 & -a_{12}/a_{11} & -a_{13}/a_{11} & \\dots & -a_{1n}/a_{11} \\\\ -a_{21}/a_{22} & 0 & -a_{23}/a_{22} & \\dots & -a_{2n}/a_{22} \\\\ -a_{31}/a_{33} & -a_{32}/a_{33} & 0 & \\dots & -a_{3n}/a_{33} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ -a_{n1}/a_{nn} & -a_{n2}/a_{nn} & -a_{n3}/a_{nn} & \\dots & 0\\\\ \\end{bmatrix}x^{(k-1)} + \\begin{bmatrix} b_1/a_{11} \\\\ b_2/a_{22} \\\\ b_3/a_{33} \\\\ \\vdots \\\\ b_n/a_{nn} \\end{bmatrix} e sendo U e L as matrizes triangulares superiores e inferiores, e D a matriz diagonal de A , x^{(k)} = -D^{-1}(L + U)x^{(k-1)} + D^{-1}b Assim, o procedimento Jacobi pode ser sumarizado da seguinte forma: Sejam as entradas as matrizes A e b com uma aproxima\u00e7\u00e3o inicial x^{(0)} . Defina uma toler\u00e2ncia para o crit\u00e9rio de parada. Realize a itera\u00e7\u00e3o at\u00e9 o crit\u00e9rio de parada. Lembre de pr\u00e9-calcular as matrizes D^{-1}(L + U) e D^{-1}b . Observa\u00e7\u00e3o: Na nota\u00e7\u00e3o do livro, troca-se L e U por -L e -U para n\u00e3o aparecer o sinal na express\u00e3o acima. Acelerar a converg\u00eancia: Colocar a_{ii} o m\u00e1ximo poss\u00edvel acelera a converg\u00eancia do m\u00e9todo.","title":"M\u00e9todo de Jacobi"},{"location":"analisenum/linear-systems/#metodo-de-gauss-seidel","text":"Observe que quando iteramos o processo no m\u00e9todo de Jacobi, fixamos x^{(k-1)} e fazemos a opera\u00e7\u00e3o x^{(k)} . Por esse motivo, podemos utilizar os valores j\u00e1 calculados x_1^{(k)}, \\dots, x_{i-1}^{(k)} para atualizar o valor de x_i^{(k)} . Assim x_i^{(k)} = \\frac{1}{a_{ii}} \\left[ -\\sum_{j=1}^{i-1} \\left(a_{ij} x_j^{(k)}\\right) - \\sum_{j=i+1}^{n} \\left(a_{ij} x_j^{(k-1)}\\right) + b_i\\right] Essa simples modifica\u00e7\u00e3o gerou o m\u00e9todo de Gauss-Seidel. Para obter essa equa\u00e7\u00e3o em formato matricial, precisamos de \\sum_{j=1}^{i} \\left(a_{ij} x_j^{(k)}\\right) = - \\sum_{j=i+1}^{n} \\left(a_{ij} x_j^{(k-1)}\\right) + b_i Note que para cada linha i , estamos tomando todas as colunas j \\le i no lado esquerdo. No lado direito \u00e9 o contr\u00e1rio. Assim, (D + L)x^{(k)} = Ux^{(k-1)} + b \\implies x^{(k)} = -(D+L)^{-1}Ux^{(k-1)} + (D + L)^{-1}b O processo iterativo \u00e9 similar ao M\u00e9todo de Jordan. Teorema (Stein-Rosenberg): Se a_{ij} \\le 0 para cada i \\neq j e a_{ii} > 0 , para cada i=1,2,\\dots,n , ent\u00e3o uma, e somente uma, das afirma\u00e7\u00f5es vale: (i) 0 \\le \\rho(T_g) < \\rho(T_j) < 1 ; (ii) 1 < \\rho(T_j) < \\rho(T_g) ; (iii) \\rho(T_g) = \\rho(T_j) = 0 ; (iv) \\rho(T_g) = \\rho(T_j) = 1 , em que T_j = -D^{-1}(L+U) e T_g = -(D+L)^{-1}U s\u00e3o as matrizes dos m\u00e9todos de Jacobi e Gauss-Seidel.","title":"M\u00e9todo de Gauss-Seidel"},{"location":"analisenum/linear-systems/#convergencia","text":"Seja Ax = b o sistema com solu\u00e7\u00e3o x^* e o m\u00e9todo iterativo x^{k+1} = Cx^k +D com x^* = Cx^* + D . Assim se ||C|| < 1 , o m\u00e9todo converge para a solu\u00e7\u00e3o com os seguintes limites no erro: (i) ||x^* - x^{k}|| \\le ||C||^k||x^* - x^0|| . (ii) ||x^* - x^{k}|| \\le \\frac{||C||^k}{1 - ||C||}||x^1 - x^0|| . Assim, percebemos que a converg\u00eancia depende de ||C|| \\approx \\rho(C) (isso \u00e9 verdade porque para todo \\epsilon > 0 , existe uma norma matricial natural tal que \\rho(C) < ||C|| < \\rho(C) + \\epsilon .","title":"Converg\u00eancia"},{"location":"analisenum/linear-systems/#metodo-successive-over-relaxation-sor","text":"Baseada na an\u00e1lise de converg\u00eancia da \u00faltima se\u00e7\u00e3o, estamos interessados em minimizar \\rho(C) de maneira geral. Para isso introduz-se o SOR. O vetor res\u00edduo \u00e9 dado por r = b - A\\tilde{x} , em que \\tilde{x} \u00e9 uma aproxima\u00e7\u00e3o para a solu\u00e7\u00e3o de Ax = b . Seja r_{ii}^{(k)} = (r_{1i}^{(k)}, r_{2i}^{(k)}, \\dots, r_{ni}^{(k)}) o vetor res\u00edduo para x_{i}^{(k)} = (x_1^{(k)}, \\dots, x_{i-1}^{(k)}, x_i^{(k-1)}, \\dots, x_n^{(k)}) . Em particular, o m\u00e9todo de Gauss-Seidel pode ser reescrito de forma a x_{i}^{(k)} = x_i^{(k-1)} + \\frac{r_{ii}^{(k)}}{a_{ii}}. Para isso, a ideia ser\u00e1 escolher \\omega de forma a acelerar a converg\u00eancia e x_{i}^{(k)} = x_i^{(k-1)} + \\omega\\frac{r_{ii}^{(k)}}{a_{ii}}. Se \\omega \\in (0,1) , o m\u00e9todo \u00e9 sob-relaxamento. Se \\omega \\in (1,2) , o m\u00e9todo \u00e9 sobre-relaxamento. Em formato iterativo, x_i^{(k)} = (1 - \\omega)x_{i}^{(k-1)} + \\frac{\\omega}{a_{ii}}\\left[b_i - \\sum_{j-1}^{i-1} a_{ij}x_j^{(k)} - \\sum_{j=i+1}^n a_{ij}x_{j}^{(k-1)} \\right] que em formato matricial se reduz a x^{(k)} = (D + \\omega L)^{-1}[(1-\\omega)D - \\omega U]x^{(k-1)} + \\omega(D + \\omega L)^{-1}b. Teorema (Kahan): Se a_{ii} \\neq 0 para todo i=1,\\dots,n , ent\u00e3o o m\u00e9todo converge somente se 0 < \\omega < 2 . Esse resultado pode ser obtido calculando o raio espectral da matriz da itera\u00e7\u00e3o SOR. Teorema (Ostrowski-Reich): Se A \u00e9 matriz positiva definida e 0 < \\omega < 2 , ent\u00e3o o m\u00e9todo SOR converge para todo x^{(0)} . Por fim, \u00e9 importante destacar que para minimizarmos o raio espectral, a escolha \u00f3tima de \\omega \u00e9 dada por \\omega = \\frac{2}{1 + \\sqrt{1 - [\\rho(T_j)]^2}}, em que T_j \u00e9 a matriz de itera\u00e7\u00e3o de Jacobi.","title":"M\u00e9todo Successive Over-Relaxation (SOR)"},{"location":"analisenum/linear-systems/#gradiente-conjugado","text":"","title":"Gradiente Conjugado"},{"location":"analisenum/lista2/","text":"Lista 2 - An\u00e1lise Num\u00e9rica Escola de Matem\u00e1tica Aplicada, Funda\u00e7\u00e3o Getulio Vargas Professor: Hugo A. de la Cruz Cancino Monitor: Lucas Machado Moschen Data da entrega: 12/09/2021 Considere a matriz diagonal por blocos A \\in \\mathbb{R}^{m^2 \\times m^2} A = \\begin{bmatrix} T & -I & 0 & 0 & \\dots & 0 \\\\ -I & T & -I & 0 & \\dots & 0 \\\\ 0 & -I & T & -I & \\dots & 0 \\\\ &&\\dots&\\dots&& \\\\ 0 & \\dots & 0 & 0 & -I & T \\end{bmatrix} onde I \\in \\mathbb{R}^{m\\times m} \u00e9 a matriz identidade T \\in \\mathbb{R}^{m\\times m} \u00e9 dada por T = \\begin{bmatrix} 4 & -1 & 0 & 0 & \\dots & 0 \\\\ -1 & 4 & -1 & 0 & \\dots & 0 \\\\ 0 & -1 & 4 & -1 & \\dots & 0 \\\\ &&\\dots&\\dots&& \\\\ 0 & \\dots & 0 & 0 & -1 & 4 \\end{bmatrix} (a) O m\u00e9todo de Gauss-Seidel aplicado ao sistema Ax = b (b \\in R^m) \u00e9 convergente \\forall m \\in N ? Justifique bem sua resposta. Existem algumas formas de verificar esse resultado usando alguns resultados da literatura. De forma geral para m\u00e9todos iterativos, podemos fazer: (i) Mostrar que a matriz do m\u00e9todo T = -(D + L)^{-1}U tem o maior autovalor em m\u00f3dulo \u00e9 menor do que 1. Seja x autovetor de T cujo autovalor correspondente \u00e9 \\lambda . Assim -(D+L)^{-1}Ux = \\lambda x \\implies -Ux = \\lambda(D+ L)x . De fato, ter\u00edamos que ||U||||x|| \\ge |\\lambda|||(D+L)x|| para todo autovetor x , que pode ser considerado de norma 1. Assim, se consegu\u00edssemos mostrar que ||U||/||(D+L)x|| < 1 para todo autovetor x , ter\u00edamos que \\rho(T) < 1 . Se tomarmos a norma m\u00e1ximo, \u00e9 f\u00e1cil ver que ||U|| = 6 . Ser\u00e1 que \u00e9 f\u00e1cil mostrar que ||(D+L)x|| > 6 ? Eu acredito que n\u00e3o. Para isso ter\u00edamos que encontrar o formato dos autovalores, pois, de forma geral, se x \u00e9 um vetor qualquer, essa desigualdade n\u00e3o \u00e9 observada. (ii) Mostrar que ||T|| = ||(D+L)^{-1}U|| < 1 para alguma norma induzida. Infelizmente, isso tamb\u00e9m n\u00e3o parece trivial de se fazer, mesmo sabendo que a inversa de uma matriz triangular inferior \u00e9 triangular inferior. (iii) A matriz A \u00e9 estritamente diagonalmente dominante: isso n\u00e3o \u00e9 verdade para essa matriz. (iv) A matriz A \u00e9 irredutivelmente diagonalmente dominante: aqui precisamos mostrar que ela \u00e9 diagonalmente dominante (o que de fato \u00e9, pois a diagonal \u00e9 4, enquanto ou outros elementos da mesma linha s\u00e3o no m\u00e1ximo quatro valores -1) e que pelo menos uma linha vale a desigualdade estrita, o que vale para a primeira linha. Assim j\u00e1 provamos que vale a converg\u00eancia de Gauss-Seidel. Para verificar esse teorema, esse artigo pode ser um bom in\u00edcio. (v) A matriz A \u00e9 positiva definida. Provamos que se 0 < \\omega < 2 , o m\u00e9todo SOR converge nesse caso. Como Gauss-Seidel \u00e9 um casso particular \\omega = 1 , basta verificar a positividade. Um bom material nesse sentido \u00e9 esse aqui . Nesse caso, procurar os autovalores de A (que t\u00eam um formato bem conveniente) e mostrar que eles s\u00e3o positivos \u00e9 uma boa. (b) Uma boa escolha do par\u00e2metro \\omega no m\u00e9todo SOR pode levar a uma converg\u00eancia mais r\u00e1pida, comparado com Jacobi e Seidel. Em geral determinar o valor \u00f3timo de \\omega n\u00e3o \u00e9 um problema f\u00e1cil, mas em alguns casos em que a matriz do sistema tem uma estrutura espec\u00edfica \u00e9 poss\u00edvel achar esse valor \u00f3timo \\omega \u00f3timo. Por exemplo, para a matriz A acima \u00e9 conhecido que \\omega_{\\mathrm{otimo}} = \\frac{2}{1 + \\sin\\left(\\frac{\\pi}{m+1}\\right)}. Implemente o m\u00e9todo SOR para resolver o sistema Ax = b ; onde b \u00e9 um vetor tal que o sistema tem a solu\u00e7\u00e3o exata x = (2, 2, \\dots, 2) . Compare a performance do m\u00e9todo SOR usando 4 valores diferentes do par\u00e2metro \\omega : i) \\omega = \\omega_{\\mathrm{otimo}} ii) \\omega = 1 (o m\u00e9todo de Seidel) iii) \\omega = 0.5 iv) \\omega = 0 , para 4 valores diferentes de m : m = 50, 100, 1000, 5000 . Como crit\u00e9rio de compara\u00e7\u00e3o use o n\u00famero de itera\u00e7\u00f5es necess\u00e1rias para que o erro na norma \\infty seja \\le 10^{-6} . Comente sobre os resultados obtidos. A implementa\u00e7\u00e3o desse problema pode ser encontrado na pasta do reposit\u00f3rio . Vamos lembrar que a itera\u00e7\u00e3o do m\u00e9todo SOR \u00e9 dada por x_i^{(k)} = (1 - \\omega)x_{i}^{(k-1)} + \\frac{\\omega}{a_{ii}}\\left[b_i - \\sum_{j-1}^{i-1} a_{ij}x_j^{(k)} - \\sum_{j=i+1}^n a_{ij}x_{j}^{(k-1)} \\right] Para essa matriz em particular, temos uma estrutura interessante. Observe que para linha i temos: Um valor 4 na diagonal, isto \u00e9, a_{ii} = 4 . Um valor -1 \u00e0 esquerda de 4 (exceto na primeira linha de cada T ), isto \u00e9, a_{i,i-1} = -1 se i \\neq 1 \\mod m . Um valor -1 \u00e0 direita de 4 (exceto na \u00faltima linha de cada T ), isto \u00e9, a_{i,i+1} = -1 se i \\neq -1 \\mod m . Um valor -1 na identidade ao lado esquerdo de T , isto \u00e9, a_{i,i-m} = -1 se i > m . Um valor -1 na identidade ao lado direito de T , isto \u00e9, a_{i,i+m} = -1 se i + m \\le m^2 . A itera\u00e7\u00e3o se reduz a x_i^{(k)} = (1 - \\omega)x_{i}^{(k-1)} + \\frac{\\omega}{4}\\left[b_i + x_{i-1}^{(k)} + x_{i-m}^{(k)} + x_{i+1}^{(k-1)} + x_{i+m}^{(k-1)} \\right], com as restri\u00e7\u00f5es j\u00e1 citadas a cima. Al\u00e9m disso, b tamb\u00e9m tem uma estrutura bem particular que pode ser tamb\u00e9m utilizada. Multiplicando pelo vetor com s\u00f3 valores 1 faz com que somemos as linhas. O que fazemos ent\u00e3o \u00e9 subtrair de 4 o n\u00famero de condi\u00e7\u00f5es verdadeiras: i \\neq 1 \\mod m , i \\neq -1 \\mod m , i > m , e i \\le m^2 - m . Uma visualiza\u00e7\u00e3o interessante do processo que podemos ter \u00e9 sobre o vetor x . Ele \u00e9 um vetor em \\mathbb{R}^{m^2} . Por\u00e9m podemos imagina-lo como uma matriz \\mathbb{R}^{m \\times m} . Nesse caso se j = mq + r , dizemos que x_j = x_{q+1,r} a menos que j = mq . Nesse caso x_j = x_{q,m} . Com essa estrutura, veja que x_{i-m} fica na mesma coluna de x_i , mas uma linha acima. Isso nos permite escrever x_{i,j}^{(k)} = x_{i,j}^{(k-1)} + \\frac{\\omega}{4}\\left[x_{i, j-1}^{(k)} + x_{i-1,j}^{(k)} + x_{i,j+1}^{(k-1)} + x_{i+1,j}^{(k-1)} + b_{i,j} - 4x_{i,j}^{(k-1)}\\right], e as condi\u00e7\u00f5es se restringem a condi\u00e7\u00f5es de borda (isto \u00e9, 1 \\le i,j \\le m ). Do jeito que calculamos agora, n\u00e3o \u00e9 poss\u00edvel paralelizar o c\u00e1lculo. Para fazer isso, uma estrat\u00e9gia \u00e9 montar o Grid Red-Black . Esse problema \u00e9 muito custoso quando m cresce. Em particular, colocando x_0 a uma dist\u00e2ncia de Normal(0, sd = 0.01), para fazer a seguinte figura, levou 11s. Por\u00e9m, para m = 1000 , esse tempo j\u00e1 foi muito superior. Outras quest\u00f5es da lista M\u00e9todos iterativos para sistemas de equa\u00e7\u00f5es lineares","title":"Lista 2 - An\u00e1lise Num\u00e9rica"},{"location":"analisenum/lista2/#lista-2-analise-numerica","text":"Escola de Matem\u00e1tica Aplicada, Funda\u00e7\u00e3o Getulio Vargas Professor: Hugo A. de la Cruz Cancino Monitor: Lucas Machado Moschen Data da entrega: 12/09/2021 Considere a matriz diagonal por blocos A \\in \\mathbb{R}^{m^2 \\times m^2} A = \\begin{bmatrix} T & -I & 0 & 0 & \\dots & 0 \\\\ -I & T & -I & 0 & \\dots & 0 \\\\ 0 & -I & T & -I & \\dots & 0 \\\\ &&\\dots&\\dots&& \\\\ 0 & \\dots & 0 & 0 & -I & T \\end{bmatrix} onde I \\in \\mathbb{R}^{m\\times m} \u00e9 a matriz identidade T \\in \\mathbb{R}^{m\\times m} \u00e9 dada por T = \\begin{bmatrix} 4 & -1 & 0 & 0 & \\dots & 0 \\\\ -1 & 4 & -1 & 0 & \\dots & 0 \\\\ 0 & -1 & 4 & -1 & \\dots & 0 \\\\ &&\\dots&\\dots&& \\\\ 0 & \\dots & 0 & 0 & -1 & 4 \\end{bmatrix} (a) O m\u00e9todo de Gauss-Seidel aplicado ao sistema Ax = b (b \\in R^m) \u00e9 convergente \\forall m \\in N ? Justifique bem sua resposta. Existem algumas formas de verificar esse resultado usando alguns resultados da literatura. De forma geral para m\u00e9todos iterativos, podemos fazer: (i) Mostrar que a matriz do m\u00e9todo T = -(D + L)^{-1}U tem o maior autovalor em m\u00f3dulo \u00e9 menor do que 1. Seja x autovetor de T cujo autovalor correspondente \u00e9 \\lambda . Assim -(D+L)^{-1}Ux = \\lambda x \\implies -Ux = \\lambda(D+ L)x . De fato, ter\u00edamos que ||U||||x|| \\ge |\\lambda|||(D+L)x|| para todo autovetor x , que pode ser considerado de norma 1. Assim, se consegu\u00edssemos mostrar que ||U||/||(D+L)x|| < 1 para todo autovetor x , ter\u00edamos que \\rho(T) < 1 . Se tomarmos a norma m\u00e1ximo, \u00e9 f\u00e1cil ver que ||U|| = 6 . Ser\u00e1 que \u00e9 f\u00e1cil mostrar que ||(D+L)x|| > 6 ? Eu acredito que n\u00e3o. Para isso ter\u00edamos que encontrar o formato dos autovalores, pois, de forma geral, se x \u00e9 um vetor qualquer, essa desigualdade n\u00e3o \u00e9 observada. (ii) Mostrar que ||T|| = ||(D+L)^{-1}U|| < 1 para alguma norma induzida. Infelizmente, isso tamb\u00e9m n\u00e3o parece trivial de se fazer, mesmo sabendo que a inversa de uma matriz triangular inferior \u00e9 triangular inferior. (iii) A matriz A \u00e9 estritamente diagonalmente dominante: isso n\u00e3o \u00e9 verdade para essa matriz. (iv) A matriz A \u00e9 irredutivelmente diagonalmente dominante: aqui precisamos mostrar que ela \u00e9 diagonalmente dominante (o que de fato \u00e9, pois a diagonal \u00e9 4, enquanto ou outros elementos da mesma linha s\u00e3o no m\u00e1ximo quatro valores -1) e que pelo menos uma linha vale a desigualdade estrita, o que vale para a primeira linha. Assim j\u00e1 provamos que vale a converg\u00eancia de Gauss-Seidel. Para verificar esse teorema, esse artigo pode ser um bom in\u00edcio. (v) A matriz A \u00e9 positiva definida. Provamos que se 0 < \\omega < 2 , o m\u00e9todo SOR converge nesse caso. Como Gauss-Seidel \u00e9 um casso particular \\omega = 1 , basta verificar a positividade. Um bom material nesse sentido \u00e9 esse aqui . Nesse caso, procurar os autovalores de A (que t\u00eam um formato bem conveniente) e mostrar que eles s\u00e3o positivos \u00e9 uma boa. (b) Uma boa escolha do par\u00e2metro \\omega no m\u00e9todo SOR pode levar a uma converg\u00eancia mais r\u00e1pida, comparado com Jacobi e Seidel. Em geral determinar o valor \u00f3timo de \\omega n\u00e3o \u00e9 um problema f\u00e1cil, mas em alguns casos em que a matriz do sistema tem uma estrutura espec\u00edfica \u00e9 poss\u00edvel achar esse valor \u00f3timo \\omega \u00f3timo. Por exemplo, para a matriz A acima \u00e9 conhecido que \\omega_{\\mathrm{otimo}} = \\frac{2}{1 + \\sin\\left(\\frac{\\pi}{m+1}\\right)}. Implemente o m\u00e9todo SOR para resolver o sistema Ax = b ; onde b \u00e9 um vetor tal que o sistema tem a solu\u00e7\u00e3o exata x = (2, 2, \\dots, 2) . Compare a performance do m\u00e9todo SOR usando 4 valores diferentes do par\u00e2metro \\omega : i) \\omega = \\omega_{\\mathrm{otimo}} ii) \\omega = 1 (o m\u00e9todo de Seidel) iii) \\omega = 0.5 iv) \\omega = 0 , para 4 valores diferentes de m : m = 50, 100, 1000, 5000 . Como crit\u00e9rio de compara\u00e7\u00e3o use o n\u00famero de itera\u00e7\u00f5es necess\u00e1rias para que o erro na norma \\infty seja \\le 10^{-6} . Comente sobre os resultados obtidos. A implementa\u00e7\u00e3o desse problema pode ser encontrado na pasta do reposit\u00f3rio . Vamos lembrar que a itera\u00e7\u00e3o do m\u00e9todo SOR \u00e9 dada por x_i^{(k)} = (1 - \\omega)x_{i}^{(k-1)} + \\frac{\\omega}{a_{ii}}\\left[b_i - \\sum_{j-1}^{i-1} a_{ij}x_j^{(k)} - \\sum_{j=i+1}^n a_{ij}x_{j}^{(k-1)} \\right] Para essa matriz em particular, temos uma estrutura interessante. Observe que para linha i temos: Um valor 4 na diagonal, isto \u00e9, a_{ii} = 4 . Um valor -1 \u00e0 esquerda de 4 (exceto na primeira linha de cada T ), isto \u00e9, a_{i,i-1} = -1 se i \\neq 1 \\mod m . Um valor -1 \u00e0 direita de 4 (exceto na \u00faltima linha de cada T ), isto \u00e9, a_{i,i+1} = -1 se i \\neq -1 \\mod m . Um valor -1 na identidade ao lado esquerdo de T , isto \u00e9, a_{i,i-m} = -1 se i > m . Um valor -1 na identidade ao lado direito de T , isto \u00e9, a_{i,i+m} = -1 se i + m \\le m^2 . A itera\u00e7\u00e3o se reduz a x_i^{(k)} = (1 - \\omega)x_{i}^{(k-1)} + \\frac{\\omega}{4}\\left[b_i + x_{i-1}^{(k)} + x_{i-m}^{(k)} + x_{i+1}^{(k-1)} + x_{i+m}^{(k-1)} \\right], com as restri\u00e7\u00f5es j\u00e1 citadas a cima. Al\u00e9m disso, b tamb\u00e9m tem uma estrutura bem particular que pode ser tamb\u00e9m utilizada. Multiplicando pelo vetor com s\u00f3 valores 1 faz com que somemos as linhas. O que fazemos ent\u00e3o \u00e9 subtrair de 4 o n\u00famero de condi\u00e7\u00f5es verdadeiras: i \\neq 1 \\mod m , i \\neq -1 \\mod m , i > m , e i \\le m^2 - m . Uma visualiza\u00e7\u00e3o interessante do processo que podemos ter \u00e9 sobre o vetor x . Ele \u00e9 um vetor em \\mathbb{R}^{m^2} . Por\u00e9m podemos imagina-lo como uma matriz \\mathbb{R}^{m \\times m} . Nesse caso se j = mq + r , dizemos que x_j = x_{q+1,r} a menos que j = mq . Nesse caso x_j = x_{q,m} . Com essa estrutura, veja que x_{i-m} fica na mesma coluna de x_i , mas uma linha acima. Isso nos permite escrever x_{i,j}^{(k)} = x_{i,j}^{(k-1)} + \\frac{\\omega}{4}\\left[x_{i, j-1}^{(k)} + x_{i-1,j}^{(k)} + x_{i,j+1}^{(k-1)} + x_{i+1,j}^{(k-1)} + b_{i,j} - 4x_{i,j}^{(k-1)}\\right], e as condi\u00e7\u00f5es se restringem a condi\u00e7\u00f5es de borda (isto \u00e9, 1 \\le i,j \\le m ). Do jeito que calculamos agora, n\u00e3o \u00e9 poss\u00edvel paralelizar o c\u00e1lculo. Para fazer isso, uma estrat\u00e9gia \u00e9 montar o Grid Red-Black . Esse problema \u00e9 muito custoso quando m cresce. Em particular, colocando x_0 a uma dist\u00e2ncia de Normal(0, sd = 0.01), para fazer a seguinte figura, levou 11s. Por\u00e9m, para m = 1000 , esse tempo j\u00e1 foi muito superior.","title":"Lista 2 - An\u00e1lise Num\u00e9rica"},{"location":"analisenum/lista2/#outras-questoes-da-lista","text":"M\u00e9todos iterativos para sistemas de equa\u00e7\u00f5es lineares","title":"Outras quest\u00f5es da lista"},{"location":"analisenum/non_linear_equations/","text":"Solu\u00e7\u00e3o de equa\u00e7\u00f5es n\u00e3o lineares Considere uma popula\u00e7\u00e3o com crescimento proporcional ao seu tamanho a cada tempo t sujeita a migra\u00e7\u00f5es constantes, isto, se x(t) \u00e9 o tamanho da popula\u00e7\u00e3o no tempo t , \\dot{x}(t) = \\lambda x(t) + \\nu \\implies x(t) = x(0)e^{\\lambda t } + \\frac{\\nu}{\\lambda}(e^{\\lambda t} - 1) Se sabemos x(t), x(0) e \\nu , ainda n\u00e3o conseguimos resolver para \\lambda esse sistema. Obseva\u00e7\u00e3o: Muitas vezes, simplifica\u00e7\u00f5es em sistemas de equa\u00e7\u00f5es ou equa\u00e7\u00f5es podem ajudar a resolver o problema analiticamente, que \u00e9 sempre mais preciso. Nem sempre s\u00f3 jogar um sistema num solver vai resolver o problema. De forma geral, queremos resolver f(x) = 0 para f : [a,b] \\to \\mathbb{R} , em geral, precisamos que f(a) e f(b) tenham sinais diferentes e f seja pelo menos cont\u00ednua, para garantir exist\u00eancia de solu\u00e7\u00e3o atrav\u00e9s do Teorema do Valor Intermedi\u00e1rio. M\u00e9todo da Bisse\u00e7\u00e3o Esse \u00e9 um m\u00e9todo bem simples que se embasa totalmente no Teorema do Valor Intermedi\u00e1rio. Seja x^* a solu\u00e7\u00e3o do problema, isto \u00e9, f(x^*) = 0 . Tome a_1 = a, b_1 = b, x_1 = \\frac{a_1 + b_1}{2} . A partir de x_1 , verificamos o sinal de f(x_1) . Assim f(x_1) = 0 : Nesse caso a solu\u00e7\u00e3o \u00e9 x^* = x_1 . f(x_1) < 0 : Nesse caso, se f(a) < 0 e f(b) > 0 , deve haver uma solu\u00e7\u00e3o no intervalo [x_1, b] . Por isso, definimos a_2 = x_1 e b_2 = b e seguimos o procedimento. Se f(a) > 0 e f(b) < 0 , deve haver uma solu\u00e7\u00e3o em [a, x_1] e, portanto, fa_2 = a, b_2 = x_1 e seguimos o procedimento. f(x_1) > 0 : Nesse caso, se f(a) > 0 e f(b) < 0 , deve haver uma solu\u00e7\u00e3o no intervalo [x_1, b] . Por isso, definimos a_2 = x_1 e b_2 = b e seguimos o procedimento. Se f(a) < 0 e f(b) > 0 , deve haver uma solu\u00e7\u00e3o em [a, x_1] e, portanto, fa_2 = a, b_2 = x_1 e seguimos o procedimento. Assim, esse processo se resume a tomar o ponto m\u00e9dio do intervalo e ir cortando pela metade o intervalo a cada itera\u00e7\u00e3o. Precisamos de um crit\u00e9rio de parada melhor do que f(x_k) = 0 , pois estamos num \u00e2mbito cont\u00ednuo. Como cortamos o intervalo pela metade a cada itera\u00e7\u00e3o, \u00e9 f\u00e1cil ver que |x_n - x^*| \\le \\frac{b-a}{2^n}, \\forall n \\ge 1. Nesse caso, uma condi\u00e7\u00e3o de parada poss\u00edvel \u00e9 (b-a) \\le 2^n \\epsilon , em que \\epsilon \u00e9 a minha toler\u00e2ncia a erro. Alguns coment\u00e1rios importantes: calcular x_n = a_n + \\frac{b_n - a_n}{2} \u00e9 melhor numericamente do que x_n = (a_n + b_n)/2 . Al\u00e9m disso, cuidado com a condi\u00e7\u00e3o f(a_n)f(b_n) < 0 , pois pode haver problema de underflow na multiplica\u00e7\u00e3o. Regula-Falsi (M\u00e9todo da posi\u00e7\u00e3o falsa) Esse \u00e9 outro m\u00e9todo bem antigo, com as primeiras apari\u00e7\u00f5es nos registros babil\u00f4nicos. A ideia era encontrar x tal que ax + b = 0 . Essa ideia foi trazida para resolver o problema de encontrar ra\u00edzes de f . Nesse caso, dados os pontos (a, f(a)), (b, f(b)) , sabemos que existe x^* \\in (a,b) tal que f(x^*) = 0 quando os sinais s\u00e3o trocados e f \u00e9 cont\u00ednua. Tra\u00e7ando um segmento entre esses pontos, em algum momento ele vai atingir o eixo x e \u00e9 nesse ponto que teremos a itera\u00e7\u00e3o. A continua\u00e7\u00e3o do algoritmo \u00e9 o mesmo do m\u00e9todo da Bisse\u00e7\u00e3o, isto \u00e9, o intervalo vai sendo reduzido, n\u00e3o mais pelo ponto m\u00e9dio, mas pelo ponto de intersec\u00e7\u00e3o da reta que passa por (a,f(a)) e (b, f(b)) e o eixo x . A itera\u00e7\u00e3o desse m\u00e9todo \u00e9 dada por x_k = a - \\frac{f(a_k)}{f(b_k) - f(a_k)}(b_k-a_k), em que a_k e b_k s\u00e3o obtidos conforme o m\u00e9todo da Bisse\u00e7\u00e3o. Se |f'(x)| \\ge d > 0 para todo x \\in [a,b] , asseguramos que |x^* - x_k| \\le |f(x_k)|/d. Itera\u00e7\u00e3o de Ponto Fixo Se f : \\mathbb{R}^n \\to \\mathbb{R} \u00e9 uma fun\u00e7\u00e3o, dizemos que x \u00e9 ponto fixo de f quando f(x) = x . Esse nome fica claro pois f(f(\\dots(f(x)))) = x . Note que se g(x) = f(x) - x , temos que g(x) = 0 \\equiv f(x) = x , isto \u00e9, encontrar pontos fixos de f equivale a encontrar as ra\u00edzes de g . Teorema do Ponto Fixo Existem alguns teoremas de garantia de exist\u00eancia e unicidade de pontos fixos, com diferentes hip\u00f3teses. Aqui tem uma lista no Wikipedia . Teorema do Ponto Fixo de Brower Seja f cont\u00ednua em um conjunto convexo compacto C com imagem f(C) \\subseteq C . Ent\u00e3o f possui ponto fixo. Aqui uma demonstra\u00e7\u00e3o interessante com um pouquinho de topologia. No nosso caso, tomamos C = [a,b] , o intervalo limitado e fechado na reta, isto \u00e9, se f \u00e9 cont\u00ednua em [a,b] e f(x) \\in [a,b] para todo x \\in [a,b] , isto \u00e9, f([a,b]) \\subseteq [a,b] , ent\u00e3o f possui ponto fixo. Teorema do Ponto Fixo de Banach Dizemos que uma fun\u00e7\u00e3o f : X \\to X , em que X \u00e9 um espa\u00e7o normado completo \u00e9 uma contra\u00e7\u00e3o se existe L \\in [0,1) tal que ||f(x) - f(y)|| \\le L||x-y||. Como exemplo, podemos tomar X = [a,b] . Se X n\u00e3o for vazio e f for uma contra\u00e7\u00e3o, ent\u00e3o f admite um \u00fanico ponto fixo x^* . Al\u00e9m do mais, para todo x_0 \\in X , a sequ\u00eancia iniciada em x_0 que segue a itera\u00e7\u00e3o x_k = f(x_{k-1}) converge para x^* (o que permite desenvolver um m\u00e9todo). Podemos demonstrar que ||x^* - x_n|| \\le \\dfrac{L^n}{1 -L}||x_1 - x_0|| . A prova pode ser facilmente encontrada . Como j\u00e1 afirmei, se conseguirmos assegurar que f satisfaz as condi\u00e7\u00f5es do Teorema, ent\u00e3o a sequ\u00eancia x_0, x_1, x_2, \\dots, x_n, \\dots com f(x_k) = x_{k-1} converge para o ponto fixo x^* , que implica g(x^*) = 0 . M\u00e9todo de Newton-Raphson Tamb\u00e9m chamado de m\u00e9todo de Newton. Por Taylor, seja f pelo menos duas vezes deriv\u00e1vel. Assim f(x^*) = f(x) + (x^* - x)f'(x) + \\frac{(x^*-x)^2}{2}f''(x) + o((x^*-x)^2), em que o(x) \u00e9 qualquer fun\u00e7\u00e3o tal que \\lim_{x\\to 0} o(x)/x = 0 . O m\u00e9todo de Netwon assume que (x^* - x)^2 \u00e9 suficientemente pequeno, isto \u00e9, x est\u00e1 suficientemente pr\u00f3ximo de x^* . Assim, 0 \\approx f(x) + f'(x)(x^* - x) \\implies x^* \\approx x - \\frac{f(x)}{f'(x)}. Com essa ideia em mente, definimos a itera\u00e7\u00e3o x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}, em que se x_0 \u00e9 suficientemente pr\u00f3ximo de x^* , ent\u00e3o \\lim_{k \\to +\\infty} x_k = x^* . A ideia \u00e9 que as aproxima\u00e7\u00f5es s\u00e3o dadas atrav\u00e9s da tangente, isto \u00e9, x_{k+1} \u00e9 o ponto da intersec\u00e7\u00e3o do eixo x com a tangente de f no ponto x_k . Teorema de converg\u00eancia: Seja f duas vezes continuamente diferenci\u00e1vel em [a,b] . Se f'(x^*) \\neq 0 , existe \\delta > 0 tal que a sequ\u00eancia de gerada pelo m\u00e9todo de Newton converge para todo x_0 \\in [x^* - \\delta, x^* + \\delta] . A ideia dessa demonstra\u00e7\u00e3o \u00e9 introduzir a fun\u00e7\u00e3o g(x) = x -\\frac{f(x)}{f'(x)} Note que quando f(x) =0 , teremos que g(x) = x , isto \u00e9, x \u00e9 ponto fixo de g , isto \u00e9, a prova se resume a verificar as condi\u00e7\u00f5es do Teorema do Ponto Fixo de Banach para algum \\delta > 0 . A derivada de g vai ser controlada em um intervalo suficientemente pequeno. Condi\u00e7\u00e3o suficiente para converg\u00eancia: Voltando a expans\u00e3o de Taylor, f(x^*) = f(x) + (x^* - x)f'(x) + \\frac{(x^*-x)^2}{2}f''(z) para algum z entre x e x^* . Usando a itera\u00e7\u00e3o de Newton, isto \u00e9, f(x_n) = f'(x_n)(x_n - x_{n+1}) , obtemos que 0 = f'(x_n)(x^* - x_{n+1}) + \\frac{(x^*-x_n)^2}{2}f''(z_n). Definindo e_n = (x^* - x_n) , temos que e_{n+1} = \\frac{e_n^2}{2f'(x_n)}f''(z_n). Assumindo que f'(x), f''(x) \\neq 0 para x \\in [a,b] , temos que para todo x_0 tal que f(x_0)f''(x_0) > 0 , o m\u00e9todo converge, o que \u00e9 um resultado de converg\u00eancia global. Sugest\u00f5es Aplica\u00e7\u00e3o m\u00e9todo de Newton Compara\u00e7\u00e3o de m\u00e9todos num\u00e9ricos Converg\u00eancia de m\u00e9todos num\u00e9ricos . Exemplos de aplica\u00e7\u00e3o do m\u00e9todo de Newton-Raphson","title":"Solu\u00e7\u00e3o de equa\u00e7\u00f5es n\u00e3o lineares"},{"location":"analisenum/non_linear_equations/#solucao-de-equacoes-nao-lineares","text":"Considere uma popula\u00e7\u00e3o com crescimento proporcional ao seu tamanho a cada tempo t sujeita a migra\u00e7\u00f5es constantes, isto, se x(t) \u00e9 o tamanho da popula\u00e7\u00e3o no tempo t , \\dot{x}(t) = \\lambda x(t) + \\nu \\implies x(t) = x(0)e^{\\lambda t } + \\frac{\\nu}{\\lambda}(e^{\\lambda t} - 1) Se sabemos x(t), x(0) e \\nu , ainda n\u00e3o conseguimos resolver para \\lambda esse sistema. Obseva\u00e7\u00e3o: Muitas vezes, simplifica\u00e7\u00f5es em sistemas de equa\u00e7\u00f5es ou equa\u00e7\u00f5es podem ajudar a resolver o problema analiticamente, que \u00e9 sempre mais preciso. Nem sempre s\u00f3 jogar um sistema num solver vai resolver o problema. De forma geral, queremos resolver f(x) = 0 para f : [a,b] \\to \\mathbb{R} , em geral, precisamos que f(a) e f(b) tenham sinais diferentes e f seja pelo menos cont\u00ednua, para garantir exist\u00eancia de solu\u00e7\u00e3o atrav\u00e9s do Teorema do Valor Intermedi\u00e1rio.","title":"Solu\u00e7\u00e3o de equa\u00e7\u00f5es n\u00e3o lineares"},{"location":"analisenum/non_linear_equations/#metodo-da-bissecao","text":"Esse \u00e9 um m\u00e9todo bem simples que se embasa totalmente no Teorema do Valor Intermedi\u00e1rio. Seja x^* a solu\u00e7\u00e3o do problema, isto \u00e9, f(x^*) = 0 . Tome a_1 = a, b_1 = b, x_1 = \\frac{a_1 + b_1}{2} . A partir de x_1 , verificamos o sinal de f(x_1) . Assim f(x_1) = 0 : Nesse caso a solu\u00e7\u00e3o \u00e9 x^* = x_1 . f(x_1) < 0 : Nesse caso, se f(a) < 0 e f(b) > 0 , deve haver uma solu\u00e7\u00e3o no intervalo [x_1, b] . Por isso, definimos a_2 = x_1 e b_2 = b e seguimos o procedimento. Se f(a) > 0 e f(b) < 0 , deve haver uma solu\u00e7\u00e3o em [a, x_1] e, portanto, fa_2 = a, b_2 = x_1 e seguimos o procedimento. f(x_1) > 0 : Nesse caso, se f(a) > 0 e f(b) < 0 , deve haver uma solu\u00e7\u00e3o no intervalo [x_1, b] . Por isso, definimos a_2 = x_1 e b_2 = b e seguimos o procedimento. Se f(a) < 0 e f(b) > 0 , deve haver uma solu\u00e7\u00e3o em [a, x_1] e, portanto, fa_2 = a, b_2 = x_1 e seguimos o procedimento. Assim, esse processo se resume a tomar o ponto m\u00e9dio do intervalo e ir cortando pela metade o intervalo a cada itera\u00e7\u00e3o. Precisamos de um crit\u00e9rio de parada melhor do que f(x_k) = 0 , pois estamos num \u00e2mbito cont\u00ednuo. Como cortamos o intervalo pela metade a cada itera\u00e7\u00e3o, \u00e9 f\u00e1cil ver que |x_n - x^*| \\le \\frac{b-a}{2^n}, \\forall n \\ge 1. Nesse caso, uma condi\u00e7\u00e3o de parada poss\u00edvel \u00e9 (b-a) \\le 2^n \\epsilon , em que \\epsilon \u00e9 a minha toler\u00e2ncia a erro. Alguns coment\u00e1rios importantes: calcular x_n = a_n + \\frac{b_n - a_n}{2} \u00e9 melhor numericamente do que x_n = (a_n + b_n)/2 . Al\u00e9m disso, cuidado com a condi\u00e7\u00e3o f(a_n)f(b_n) < 0 , pois pode haver problema de underflow na multiplica\u00e7\u00e3o.","title":"M\u00e9todo da Bisse\u00e7\u00e3o"},{"location":"analisenum/non_linear_equations/#regula-falsi-metodo-da-posicao-falsa","text":"Esse \u00e9 outro m\u00e9todo bem antigo, com as primeiras apari\u00e7\u00f5es nos registros babil\u00f4nicos. A ideia era encontrar x tal que ax + b = 0 . Essa ideia foi trazida para resolver o problema de encontrar ra\u00edzes de f . Nesse caso, dados os pontos (a, f(a)), (b, f(b)) , sabemos que existe x^* \\in (a,b) tal que f(x^*) = 0 quando os sinais s\u00e3o trocados e f \u00e9 cont\u00ednua. Tra\u00e7ando um segmento entre esses pontos, em algum momento ele vai atingir o eixo x e \u00e9 nesse ponto que teremos a itera\u00e7\u00e3o. A continua\u00e7\u00e3o do algoritmo \u00e9 o mesmo do m\u00e9todo da Bisse\u00e7\u00e3o, isto \u00e9, o intervalo vai sendo reduzido, n\u00e3o mais pelo ponto m\u00e9dio, mas pelo ponto de intersec\u00e7\u00e3o da reta que passa por (a,f(a)) e (b, f(b)) e o eixo x . A itera\u00e7\u00e3o desse m\u00e9todo \u00e9 dada por x_k = a - \\frac{f(a_k)}{f(b_k) - f(a_k)}(b_k-a_k), em que a_k e b_k s\u00e3o obtidos conforme o m\u00e9todo da Bisse\u00e7\u00e3o. Se |f'(x)| \\ge d > 0 para todo x \\in [a,b] , asseguramos que |x^* - x_k| \\le |f(x_k)|/d.","title":"Regula-Falsi (M\u00e9todo da posi\u00e7\u00e3o falsa)"},{"location":"analisenum/non_linear_equations/#iteracao-de-ponto-fixo","text":"Se f : \\mathbb{R}^n \\to \\mathbb{R} \u00e9 uma fun\u00e7\u00e3o, dizemos que x \u00e9 ponto fixo de f quando f(x) = x . Esse nome fica claro pois f(f(\\dots(f(x)))) = x . Note que se g(x) = f(x) - x , temos que g(x) = 0 \\equiv f(x) = x , isto \u00e9, encontrar pontos fixos de f equivale a encontrar as ra\u00edzes de g .","title":"Itera\u00e7\u00e3o de Ponto Fixo"},{"location":"analisenum/non_linear_equations/#teorema-do-ponto-fixo","text":"Existem alguns teoremas de garantia de exist\u00eancia e unicidade de pontos fixos, com diferentes hip\u00f3teses. Aqui tem uma lista no Wikipedia . Teorema do Ponto Fixo de Brower Seja f cont\u00ednua em um conjunto convexo compacto C com imagem f(C) \\subseteq C . Ent\u00e3o f possui ponto fixo. Aqui uma demonstra\u00e7\u00e3o interessante com um pouquinho de topologia. No nosso caso, tomamos C = [a,b] , o intervalo limitado e fechado na reta, isto \u00e9, se f \u00e9 cont\u00ednua em [a,b] e f(x) \\in [a,b] para todo x \\in [a,b] , isto \u00e9, f([a,b]) \\subseteq [a,b] , ent\u00e3o f possui ponto fixo. Teorema do Ponto Fixo de Banach Dizemos que uma fun\u00e7\u00e3o f : X \\to X , em que X \u00e9 um espa\u00e7o normado completo \u00e9 uma contra\u00e7\u00e3o se existe L \\in [0,1) tal que ||f(x) - f(y)|| \\le L||x-y||. Como exemplo, podemos tomar X = [a,b] . Se X n\u00e3o for vazio e f for uma contra\u00e7\u00e3o, ent\u00e3o f admite um \u00fanico ponto fixo x^* . Al\u00e9m do mais, para todo x_0 \\in X , a sequ\u00eancia iniciada em x_0 que segue a itera\u00e7\u00e3o x_k = f(x_{k-1}) converge para x^* (o que permite desenvolver um m\u00e9todo). Podemos demonstrar que ||x^* - x_n|| \\le \\dfrac{L^n}{1 -L}||x_1 - x_0|| . A prova pode ser facilmente encontrada . Como j\u00e1 afirmei, se conseguirmos assegurar que f satisfaz as condi\u00e7\u00f5es do Teorema, ent\u00e3o a sequ\u00eancia x_0, x_1, x_2, \\dots, x_n, \\dots com f(x_k) = x_{k-1} converge para o ponto fixo x^* , que implica g(x^*) = 0 .","title":"Teorema do Ponto Fixo"},{"location":"analisenum/non_linear_equations/#metodo-de-newton-raphson","text":"Tamb\u00e9m chamado de m\u00e9todo de Newton. Por Taylor, seja f pelo menos duas vezes deriv\u00e1vel. Assim f(x^*) = f(x) + (x^* - x)f'(x) + \\frac{(x^*-x)^2}{2}f''(x) + o((x^*-x)^2), em que o(x) \u00e9 qualquer fun\u00e7\u00e3o tal que \\lim_{x\\to 0} o(x)/x = 0 . O m\u00e9todo de Netwon assume que (x^* - x)^2 \u00e9 suficientemente pequeno, isto \u00e9, x est\u00e1 suficientemente pr\u00f3ximo de x^* . Assim, 0 \\approx f(x) + f'(x)(x^* - x) \\implies x^* \\approx x - \\frac{f(x)}{f'(x)}. Com essa ideia em mente, definimos a itera\u00e7\u00e3o x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}, em que se x_0 \u00e9 suficientemente pr\u00f3ximo de x^* , ent\u00e3o \\lim_{k \\to +\\infty} x_k = x^* . A ideia \u00e9 que as aproxima\u00e7\u00f5es s\u00e3o dadas atrav\u00e9s da tangente, isto \u00e9, x_{k+1} \u00e9 o ponto da intersec\u00e7\u00e3o do eixo x com a tangente de f no ponto x_k . Teorema de converg\u00eancia: Seja f duas vezes continuamente diferenci\u00e1vel em [a,b] . Se f'(x^*) \\neq 0 , existe \\delta > 0 tal que a sequ\u00eancia de gerada pelo m\u00e9todo de Newton converge para todo x_0 \\in [x^* - \\delta, x^* + \\delta] . A ideia dessa demonstra\u00e7\u00e3o \u00e9 introduzir a fun\u00e7\u00e3o g(x) = x -\\frac{f(x)}{f'(x)} Note que quando f(x) =0 , teremos que g(x) = x , isto \u00e9, x \u00e9 ponto fixo de g , isto \u00e9, a prova se resume a verificar as condi\u00e7\u00f5es do Teorema do Ponto Fixo de Banach para algum \\delta > 0 . A derivada de g vai ser controlada em um intervalo suficientemente pequeno. Condi\u00e7\u00e3o suficiente para converg\u00eancia: Voltando a expans\u00e3o de Taylor, f(x^*) = f(x) + (x^* - x)f'(x) + \\frac{(x^*-x)^2}{2}f''(z) para algum z entre x e x^* . Usando a itera\u00e7\u00e3o de Newton, isto \u00e9, f(x_n) = f'(x_n)(x_n - x_{n+1}) , obtemos que 0 = f'(x_n)(x^* - x_{n+1}) + \\frac{(x^*-x_n)^2}{2}f''(z_n). Definindo e_n = (x^* - x_n) , temos que e_{n+1} = \\frac{e_n^2}{2f'(x_n)}f''(z_n). Assumindo que f'(x), f''(x) \\neq 0 para x \\in [a,b] , temos que para todo x_0 tal que f(x_0)f''(x_0) > 0 , o m\u00e9todo converge, o que \u00e9 um resultado de converg\u00eancia global.","title":"M\u00e9todo de Newton-Raphson"},{"location":"analisenum/non_linear_equations/#sugestoes","text":"Aplica\u00e7\u00e3o m\u00e9todo de Newton Compara\u00e7\u00e3o de m\u00e9todos num\u00e9ricos Converg\u00eancia de m\u00e9todos num\u00e9ricos . Exemplos de aplica\u00e7\u00e3o do m\u00e9todo de Newton-Raphson","title":"Sugest\u00f5es"},{"location":"analisenum/application_newton/non_linear_equations/","text":"M\u00e9todos num\u00e9ricos para solu\u00e7\u00e3o de equa\u00e7\u00f5es n\u00e3o lineares Escrevemos no parte anterior o resumo dos algoritmos de alguns m\u00e9todos para solu\u00e7\u00e3o de equa\u00e7\u00f5es n\u00e3o lineares, e n\u00e3o s\u00e3o os \u00fanicos. Modifica\u00e7\u00f5es desses m\u00e9todos, em especial do M\u00e9todo de Newton s\u00e3o constantemente sugeridas para melhorar a converg\u00eancia. Fica claro que muitos sistemas no mundo real s\u00e3o n\u00e3o lineares. Uma aplica\u00e7\u00e3o comum \u00e9 resolver problemas de otimiza\u00e7\u00e3o. Por exemplo, quando queremos maximizar em conjuntos abertos, se conseguirmos provar algumas condi\u00e7\u00f5es, podemos assegurar que o m\u00e1ximo se encontra quando f'(x) = 0 . Logo, o problema de otimiza\u00e7\u00e3o se resume a um problema de encontrar ra\u00edzes. import numpy as np import scipy.special as scis import scipy.optimize as scop import matplotlib.pyplot as plt % matplotlib inline Um jogador A ganha com placar (21-0) do jogador B em um jogo de raquetebol com probabilidade . P = \\frac{p+1}{2}\\left(\\frac{p}{1-p+p^2}\\right)^{21}, em que p \u00e9 a probabilidade de A ganhar um rally qualquer. Qual o valor de p que assegura que A vencer\u00e1 com esse placar em pelo menos metade dos jogos? Esse \u00e9 um problema real proposto por Ralph Levine para Joseph Keller. def P ( p ): return ( p + 1 ) / 2 * ( p / ( 1 + p ** 2 - p )) ** ( 21 ) p_values = np . linspace ( 0 , 1 , 50 ) P_values = P ( p_values ) plt . plot ( p_values , P_values , color = 'r' ) plt . axhline ( 0.5 , linestyle = '--' , color = 'darkblue' ) plt . title ( 'Probabilidade de 21-0 para cada p' ) plt . text ( 0.7 , 0.55 , '$P = 0.5$' , fontsize = 12 ) plt . xlabel ( '$p$' ) plt . ylabel ( '$P$' ) plt . show () Vamos explorar m\u00e9todos de resolver a equa\u00e7\u00e3o P(p) = 0.5 para p , isto \u00e9, P(p) - 0.5 = 0 , atrav\u00e9s do m\u00e9todo do Ponto Fixo e do M\u00e9todo de Newton. Itera\u00e7\u00e3o de Ponto Fixo Temos que f(p) = 0.5 - \\frac{p+1}{2}\\left(\\frac{p}{1-p+p^2}\\right)^{21} + p = p, \u00e9 a fun\u00e7\u00e3o que admite o ponto fixo que queremos. Um ponto importante seria provar as condi\u00e7\u00f5es de funcionamento do m\u00e9todo. Por\u00e9m, n\u00e3o \u00e9 dif\u00edcil ver que f([0,1]) \\not \\subseteq [0,1] , o que j\u00e1 quebra nosso teorema. def f ( p ): return 0.5 - P ( p ) + p def fixed_point ( x0 , tol = 1e-5 , max_ite = 1e4 ): x1 = f ( x0 ) err = [ abs ( x1 - x0 )] sols = [ x0 , x1 ] while err [ - 1 ] > tol and len ( err ) <= max_ite : sols . append ( f ( sols [ - 1 ])) err . append ( abs ( sols [ - 1 ] - sols [ - 2 ])) return { 'sol' : sols , 'errors' : err } print ( f ( 0.7 )) 1.1329638832103943 Bom, mas vamos supor que mesmo assim quis\u00e9ssemos usar esse m\u00e9todo. res = fixed_point ( 0.7 ) plt . scatter ( range ( len ( res [ 'sol' ])), res [ 'sol' ]) <matplotlib.collections.PathCollection at 0x7f6a17b5eb38> Ele n\u00e3o converge! Mas a gente j\u00e1 devia ter ficado desconfiado, pois justamente as condi\u00e7\u00f5es do Teorema n\u00e3o functionavam. Em particular, \u00e9 f\u00e1cil ver que ele n\u00e3o \u00e9 nem n\u00e3o expansivo. Usando Scipy: scop . fixed_point ( func = f , x0 = 0.81 , method = 'iteration' ) --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) <ipython-input-6-93ff8160efab> in <module> ----> 1 scop.fixed_point(func = f, x0 = 0.81, method = 'iteration') ~/anaconda3/lib/python3.7/site-packages/scipy/optimize/minpack.py in fixed_point(func, x0, args, xtol, maxiter, method) 935 use_accel = {'del2': True, 'iteration': False}[method] 936 x0 = _asarray_validated(x0, as_inexact=True) --> 937 return _fixed_point_helper(func, x0, args, xtol, maxiter, use_accel) ~/anaconda3/lib/python3.7/site-packages/scipy/optimize/minpack.py in _fixed_point_helper(func, x0, args, xtol, maxiter, use_accel) 889 p0 = p 890 msg = \"Failed to converge after %d iterations, value is %s\" % (maxiter, p) --> 891 raise RuntimeError(msg) 892 893 RuntimeError: Failed to converge after 500 iterations, value is 0.4998542288112865 Existe uma varia\u00e7\u00e3o desse dado pelo m\u00e9todo Steffensen com Aitken's \\Delta^2 , que constr\u00f3i uma sequ\u00eancia com onverg\u00eancia mais r\u00e1pida, a partir da inicial. % timeit scop . fixed_point ( func = f , x0 = 0.3 , method = 'del2' ) 1.08 ms \u00b1 120 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) scop . fixed_point ( func = f , x0 = 0.3 , method = 'del2' ) array(0.84230479) % timeit scop . fixed_point ( func = f , x0 = 0.8 , method = 'del2' ) 1.21 ms \u00b1 165 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) scop . fixed_point ( func = f , x0 = 0.8 , method = 'del2' ) array(0.84230479) Olhe o que acontece com x_0 = 0.1 . scop . fixed_point ( func = f , x0 = 0.1 , method = 'del2' ) array(-3.51843721e+13) M\u00e9todo de Newton Agora vamos usar informa\u00e7\u00e3o da derivada de P para nos ajudar com o problema de encontrar a ra\u00edz de P(p) - 0.5 = 0 . Note que P(0) < 0.5 e P(1) > 0.5 . }Para nos ajudar com as contas, vamos considerar a vers\u00e3o com log, isto \u00e9, \\log(P(p)) = - \\log(2) . Assim, \\log(p+1) - \\log(2) + 21(\\log(p) - \\log(1 - p + p^2)) = -\\log(2), o que simplica para g(p) = \\log(p+1) + 21(\\log(p) - \\log(1 - p + p^2)) = 0 . A\u00ed temos que g'(p) = \\frac{1}{p+1} + \\frac{21}{p} - \\frac{21}{1 - p + p^2}(2p - 1) def g ( p ): return np . log ( p + 1 ) + 21 * ( np . log ( p ) - np . log1p ( p ** 2 - p )) def g_prime ( p ): return 1 / ( p + 1 ) + 21 / p - 21 * ( 2 * p - 1 ) / ( 1 - p + p ** 2 ) Podemos provar analiticamente que a derivada \u00e9 estritamente positiva, mas faremos a observa\u00e7\u00e3o num\u00e9rica atrav\u00e9s do seguinte gr\u00e1fico. g_values = g_prime ( p_values [ 1 :]) plt . plot ( p_values [ 1 :], g_values , color = 'r' ) plt . title ( 'Derivada da fun\u00e7\u00e3o derivada' ) plt . xlabel ( '$p$' ) plt . ylabel ( \"$g \\' (p)$\" ) plt . show () Aplicando o m\u00e9todo de newton atrav\u00e9s do Scipy. Observe que o seu tempo deu bem menor que o anterior, mesmo com um valor de x_0 bem distante. % timeit scop . newton ( func = g , x0 = 0.1 , fprime = g_prime ) 836 \u00b5s \u00b1 117 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) scop . newton ( func = g , x0 = 0.1 , fprime = g_prime ) 0.8423047910355657 Observe o m\u00e9todo da Secante, um m\u00e9todo que tamb\u00e9m usa a ideia de tangente, mas sem consultar a derivada. % timeit scop . newton ( func = g , x0 = 0.1 ) 631 \u00b5s \u00b1 111 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) scop . newton ( func = g , x0 = 0.1 ) 0.8423047910355633 Exemplo adicional Nesse exemplo, a ideia \u00e9 verificar que Newton pode dar errado. Considere: f(x) = x\\sin(\\pi x) - \\exp(-x) f = lambda x : x * np . sin ( np . pi * x ) - np . exp ( - x ) x = np . linspace ( - 1 , 2 , 100 ) y = f ( x ) fig , ax = plt . subplots () ax . plot ( x , y , color = 'r' , zorder = 0 ) xs = [ 0.57 , 0.83 , - 0.27 ] texts = [ 'raiz$_1$' , 'raiz$_2$' , 'm\u00ednimo local' ] for i in range ( len ( xs )): ax . scatter ([ xs [ i ]], [ f ( xs [ i ])], marker = 'x' , s = 60 ) ax . text ( xs [ i ] + 0.05 , f ( xs [ i ]) - 0.1 , texts [ i ]) ax . plot ( x , np . zeros_like ( x ), color = 'gray' , ls = '-.' , alpha = 0.75 ) ax . set_xlabel ( '$x$' ) ax . set_ylabel ( '$f(x)$' ) plt . title ( '$f(x)= x sin(\\pi x) - exp(-x)$' ) plt . show () xspace = np . linspace ( - 0.7 , 0.7 , 15 ) for x in xspace : print ( 'Com x_0 = {0:5.2f} , a ra\u00edz \u00e9 {1:5.2f} ' . format ( x , scop . newton ( f , x ))) Com x_0 = -0.70, a ra\u00edz \u00e9 2.02 Com x_0 = -0.60, a ra\u00edz \u00e9 0.58 Com x_0 = -0.50, a ra\u00edz \u00e9 1.27 Com x_0 = -0.40, a ra\u00edz \u00e9 0.82 Com x_0 = -0.30, a ra\u00edz \u00e9 -0.30 Com x_0 = -0.20, a ra\u00edz \u00e9 0.82 Com x_0 = -0.10, a ra\u00edz \u00e9 2.02 Com x_0 = 0.00, a ra\u00edz \u00e9 0.82 Com x_0 = 0.10, a ra\u00edz \u00e9 0.58 Com x_0 = 0.20, a ra\u00edz \u00e9 0.58 Com x_0 = 0.30, a ra\u00edz \u00e9 0.58 Com x_0 = 0.40, a ra\u00edz \u00e9 0.58 Com x_0 = 0.50, a ra\u00edz \u00e9 0.58 Com x_0 = 0.60, a ra\u00edz \u00e9 0.58 Com x_0 = 0.70, a ra\u00edz \u00e9 0.58 Um outro exemplo Vamos comparar os m\u00e9todos agora com a fun\u00e7\u00e3o f(x) = x - \\cos(x) . Para o m\u00e9todo do ponto fixo, vamos utilizar a fun\u00e7\u00e3o g(x) = \\cos(x) , naturalmente. Note que g([0,1]) \\subseteq [0,1] e |g'([0,1])| \\subseteq [0,0.9] , isto \u00e9, temos que as hip\u00f3teses para a itera\u00e7\u00e3o do ponto fixo s\u00e3o v\u00e1lidas. def f ( x , info ): res = x - np . cos ( x ) if info [ 'print' ]: info [ 'iter_x' ] . append ( x ) info [ 'iter_res' ] . append ( res ) return res def g ( x , info ): res = np . cos ( x ) if info [ 'print' ]: info [ 'iter_x' ] . append ( x ) info [ 'iter_res' ] . append ( res ) return res x = np . linspace ( 0 , 1 , 100 ) y = f ( x , { 'iter_x' : [], 'iter_res' : [], 'print' : False }) fig , ax = plt . subplots () ax . plot ( x , y , color = 'r' , zorder = 0 ) ax . axhline ( 0 , color = 'k' , linestyle = '-.' ) ax . set_title ( '$f(x)= x - cos(x)$' ) plt . show () info_newton = { 'iter_x' : [], 'iter_res' : [], 'print' : True } info_secant = { 'iter_x' : [], 'iter_res' : [], 'print' : True } info_bisect = { 'iter_x' : [], 'iter_res' : [], 'print' : True } info_fixed = { 'iter_x' : [], 'iter_res' : [], 'print' : True } newton = scop . newton ( func = f , x0 = 0.5 , fprime = lambda x , info : 1 + np . sin ( x ), tol = 1e-10 , maxiter = 200 , args = ( info_newton ,)) secant = scop . newton ( func = f , x0 = 0.5 , tol = 1e-10 , maxiter = 200 , args = ( info_secant ,)) bisect = scop . bisect ( f = f , a = 0 , b = 1 , xtol = 1e-10 , maxiter = 200 , args = ( info_bisect ,)) fixed_point = scop . fixed_point ( func = g , x0 = 0.5 , xtol = 1e-10 , bisectmethod = 'iteration' , maxiter = 200 , args = ( info_fixed ,)) plt . plot ( info_bisect [ 'iter_x' ], label = 'bisect: {} ' . format ( len ( info_bisect [ 'iter_x' ]))) plt . plot ( info_newton [ 'iter_x' ], label = 'newton: {} ' . format ( len ( info_newton [ 'iter_x' ]))) plt . plot ( np . array ( info_fixed [ 'iter_x' ]), label = 'fixed point: {} ' . format ( len ( info_fixed [ 'iter_x' ]))) plt . plot ( info_secant [ 'iter_x' ], label = 'secant: {} ' . format ( len ( info_secant [ 'iter_x' ]))) plt . legend () plt . title ( 'Comparando alguns m\u00e9todos' ) plt . xscale ( 'log' ) plt . show ()","title":"M\u00e9todos num\u00e9ricos para solu\u00e7\u00e3o de equa\u00e7\u00f5es n\u00e3o lineares"},{"location":"analisenum/application_newton/non_linear_equations/#metodos-numericos-para-solucao-de-equacoes-nao-lineares","text":"Escrevemos no parte anterior o resumo dos algoritmos de alguns m\u00e9todos para solu\u00e7\u00e3o de equa\u00e7\u00f5es n\u00e3o lineares, e n\u00e3o s\u00e3o os \u00fanicos. Modifica\u00e7\u00f5es desses m\u00e9todos, em especial do M\u00e9todo de Newton s\u00e3o constantemente sugeridas para melhorar a converg\u00eancia. Fica claro que muitos sistemas no mundo real s\u00e3o n\u00e3o lineares. Uma aplica\u00e7\u00e3o comum \u00e9 resolver problemas de otimiza\u00e7\u00e3o. Por exemplo, quando queremos maximizar em conjuntos abertos, se conseguirmos provar algumas condi\u00e7\u00f5es, podemos assegurar que o m\u00e1ximo se encontra quando f'(x) = 0 . Logo, o problema de otimiza\u00e7\u00e3o se resume a um problema de encontrar ra\u00edzes. import numpy as np import scipy.special as scis import scipy.optimize as scop import matplotlib.pyplot as plt % matplotlib inline Um jogador A ganha com placar (21-0) do jogador B em um jogo de raquetebol com probabilidade . P = \\frac{p+1}{2}\\left(\\frac{p}{1-p+p^2}\\right)^{21}, em que p \u00e9 a probabilidade de A ganhar um rally qualquer. Qual o valor de p que assegura que A vencer\u00e1 com esse placar em pelo menos metade dos jogos? Esse \u00e9 um problema real proposto por Ralph Levine para Joseph Keller. def P ( p ): return ( p + 1 ) / 2 * ( p / ( 1 + p ** 2 - p )) ** ( 21 ) p_values = np . linspace ( 0 , 1 , 50 ) P_values = P ( p_values ) plt . plot ( p_values , P_values , color = 'r' ) plt . axhline ( 0.5 , linestyle = '--' , color = 'darkblue' ) plt . title ( 'Probabilidade de 21-0 para cada p' ) plt . text ( 0.7 , 0.55 , '$P = 0.5$' , fontsize = 12 ) plt . xlabel ( '$p$' ) plt . ylabel ( '$P$' ) plt . show () Vamos explorar m\u00e9todos de resolver a equa\u00e7\u00e3o P(p) = 0.5 para p , isto \u00e9, P(p) - 0.5 = 0 , atrav\u00e9s do m\u00e9todo do Ponto Fixo e do M\u00e9todo de Newton.","title":"M\u00e9todos num\u00e9ricos para solu\u00e7\u00e3o de equa\u00e7\u00f5es n\u00e3o lineares"},{"location":"analisenum/application_newton/non_linear_equations/#iteracao-de-ponto-fixo","text":"Temos que f(p) = 0.5 - \\frac{p+1}{2}\\left(\\frac{p}{1-p+p^2}\\right)^{21} + p = p, \u00e9 a fun\u00e7\u00e3o que admite o ponto fixo que queremos. Um ponto importante seria provar as condi\u00e7\u00f5es de funcionamento do m\u00e9todo. Por\u00e9m, n\u00e3o \u00e9 dif\u00edcil ver que f([0,1]) \\not \\subseteq [0,1] , o que j\u00e1 quebra nosso teorema. def f ( p ): return 0.5 - P ( p ) + p def fixed_point ( x0 , tol = 1e-5 , max_ite = 1e4 ): x1 = f ( x0 ) err = [ abs ( x1 - x0 )] sols = [ x0 , x1 ] while err [ - 1 ] > tol and len ( err ) <= max_ite : sols . append ( f ( sols [ - 1 ])) err . append ( abs ( sols [ - 1 ] - sols [ - 2 ])) return { 'sol' : sols , 'errors' : err } print ( f ( 0.7 )) 1.1329638832103943 Bom, mas vamos supor que mesmo assim quis\u00e9ssemos usar esse m\u00e9todo. res = fixed_point ( 0.7 ) plt . scatter ( range ( len ( res [ 'sol' ])), res [ 'sol' ]) <matplotlib.collections.PathCollection at 0x7f6a17b5eb38> Ele n\u00e3o converge! Mas a gente j\u00e1 devia ter ficado desconfiado, pois justamente as condi\u00e7\u00f5es do Teorema n\u00e3o functionavam. Em particular, \u00e9 f\u00e1cil ver que ele n\u00e3o \u00e9 nem n\u00e3o expansivo. Usando Scipy: scop . fixed_point ( func = f , x0 = 0.81 , method = 'iteration' ) --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) <ipython-input-6-93ff8160efab> in <module> ----> 1 scop.fixed_point(func = f, x0 = 0.81, method = 'iteration') ~/anaconda3/lib/python3.7/site-packages/scipy/optimize/minpack.py in fixed_point(func, x0, args, xtol, maxiter, method) 935 use_accel = {'del2': True, 'iteration': False}[method] 936 x0 = _asarray_validated(x0, as_inexact=True) --> 937 return _fixed_point_helper(func, x0, args, xtol, maxiter, use_accel) ~/anaconda3/lib/python3.7/site-packages/scipy/optimize/minpack.py in _fixed_point_helper(func, x0, args, xtol, maxiter, use_accel) 889 p0 = p 890 msg = \"Failed to converge after %d iterations, value is %s\" % (maxiter, p) --> 891 raise RuntimeError(msg) 892 893 RuntimeError: Failed to converge after 500 iterations, value is 0.4998542288112865 Existe uma varia\u00e7\u00e3o desse dado pelo m\u00e9todo Steffensen com Aitken's \\Delta^2 , que constr\u00f3i uma sequ\u00eancia com onverg\u00eancia mais r\u00e1pida, a partir da inicial. % timeit scop . fixed_point ( func = f , x0 = 0.3 , method = 'del2' ) 1.08 ms \u00b1 120 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) scop . fixed_point ( func = f , x0 = 0.3 , method = 'del2' ) array(0.84230479) % timeit scop . fixed_point ( func = f , x0 = 0.8 , method = 'del2' ) 1.21 ms \u00b1 165 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) scop . fixed_point ( func = f , x0 = 0.8 , method = 'del2' ) array(0.84230479) Olhe o que acontece com x_0 = 0.1 . scop . fixed_point ( func = f , x0 = 0.1 , method = 'del2' ) array(-3.51843721e+13)","title":"Itera\u00e7\u00e3o de Ponto Fixo"},{"location":"analisenum/application_newton/non_linear_equations/#metodo-de-newton","text":"Agora vamos usar informa\u00e7\u00e3o da derivada de P para nos ajudar com o problema de encontrar a ra\u00edz de P(p) - 0.5 = 0 . Note que P(0) < 0.5 e P(1) > 0.5 . }Para nos ajudar com as contas, vamos considerar a vers\u00e3o com log, isto \u00e9, \\log(P(p)) = - \\log(2) . Assim, \\log(p+1) - \\log(2) + 21(\\log(p) - \\log(1 - p + p^2)) = -\\log(2), o que simplica para g(p) = \\log(p+1) + 21(\\log(p) - \\log(1 - p + p^2)) = 0 . A\u00ed temos que g'(p) = \\frac{1}{p+1} + \\frac{21}{p} - \\frac{21}{1 - p + p^2}(2p - 1) def g ( p ): return np . log ( p + 1 ) + 21 * ( np . log ( p ) - np . log1p ( p ** 2 - p )) def g_prime ( p ): return 1 / ( p + 1 ) + 21 / p - 21 * ( 2 * p - 1 ) / ( 1 - p + p ** 2 ) Podemos provar analiticamente que a derivada \u00e9 estritamente positiva, mas faremos a observa\u00e7\u00e3o num\u00e9rica atrav\u00e9s do seguinte gr\u00e1fico. g_values = g_prime ( p_values [ 1 :]) plt . plot ( p_values [ 1 :], g_values , color = 'r' ) plt . title ( 'Derivada da fun\u00e7\u00e3o derivada' ) plt . xlabel ( '$p$' ) plt . ylabel ( \"$g \\' (p)$\" ) plt . show () Aplicando o m\u00e9todo de newton atrav\u00e9s do Scipy. Observe que o seu tempo deu bem menor que o anterior, mesmo com um valor de x_0 bem distante. % timeit scop . newton ( func = g , x0 = 0.1 , fprime = g_prime ) 836 \u00b5s \u00b1 117 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) scop . newton ( func = g , x0 = 0.1 , fprime = g_prime ) 0.8423047910355657 Observe o m\u00e9todo da Secante, um m\u00e9todo que tamb\u00e9m usa a ideia de tangente, mas sem consultar a derivada. % timeit scop . newton ( func = g , x0 = 0.1 ) 631 \u00b5s \u00b1 111 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) scop . newton ( func = g , x0 = 0.1 ) 0.8423047910355633","title":"M\u00e9todo de Newton"},{"location":"analisenum/application_newton/non_linear_equations/#exemplo-adicional","text":"Nesse exemplo, a ideia \u00e9 verificar que Newton pode dar errado. Considere: f(x) = x\\sin(\\pi x) - \\exp(-x) f = lambda x : x * np . sin ( np . pi * x ) - np . exp ( - x ) x = np . linspace ( - 1 , 2 , 100 ) y = f ( x ) fig , ax = plt . subplots () ax . plot ( x , y , color = 'r' , zorder = 0 ) xs = [ 0.57 , 0.83 , - 0.27 ] texts = [ 'raiz$_1$' , 'raiz$_2$' , 'm\u00ednimo local' ] for i in range ( len ( xs )): ax . scatter ([ xs [ i ]], [ f ( xs [ i ])], marker = 'x' , s = 60 ) ax . text ( xs [ i ] + 0.05 , f ( xs [ i ]) - 0.1 , texts [ i ]) ax . plot ( x , np . zeros_like ( x ), color = 'gray' , ls = '-.' , alpha = 0.75 ) ax . set_xlabel ( '$x$' ) ax . set_ylabel ( '$f(x)$' ) plt . title ( '$f(x)= x sin(\\pi x) - exp(-x)$' ) plt . show () xspace = np . linspace ( - 0.7 , 0.7 , 15 ) for x in xspace : print ( 'Com x_0 = {0:5.2f} , a ra\u00edz \u00e9 {1:5.2f} ' . format ( x , scop . newton ( f , x ))) Com x_0 = -0.70, a ra\u00edz \u00e9 2.02 Com x_0 = -0.60, a ra\u00edz \u00e9 0.58 Com x_0 = -0.50, a ra\u00edz \u00e9 1.27 Com x_0 = -0.40, a ra\u00edz \u00e9 0.82 Com x_0 = -0.30, a ra\u00edz \u00e9 -0.30 Com x_0 = -0.20, a ra\u00edz \u00e9 0.82 Com x_0 = -0.10, a ra\u00edz \u00e9 2.02 Com x_0 = 0.00, a ra\u00edz \u00e9 0.82 Com x_0 = 0.10, a ra\u00edz \u00e9 0.58 Com x_0 = 0.20, a ra\u00edz \u00e9 0.58 Com x_0 = 0.30, a ra\u00edz \u00e9 0.58 Com x_0 = 0.40, a ra\u00edz \u00e9 0.58 Com x_0 = 0.50, a ra\u00edz \u00e9 0.58 Com x_0 = 0.60, a ra\u00edz \u00e9 0.58 Com x_0 = 0.70, a ra\u00edz \u00e9 0.58","title":"Exemplo adicional"},{"location":"analisenum/application_newton/non_linear_equations/#um-outro-exemplo","text":"Vamos comparar os m\u00e9todos agora com a fun\u00e7\u00e3o f(x) = x - \\cos(x) . Para o m\u00e9todo do ponto fixo, vamos utilizar a fun\u00e7\u00e3o g(x) = \\cos(x) , naturalmente. Note que g([0,1]) \\subseteq [0,1] e |g'([0,1])| \\subseteq [0,0.9] , isto \u00e9, temos que as hip\u00f3teses para a itera\u00e7\u00e3o do ponto fixo s\u00e3o v\u00e1lidas. def f ( x , info ): res = x - np . cos ( x ) if info [ 'print' ]: info [ 'iter_x' ] . append ( x ) info [ 'iter_res' ] . append ( res ) return res def g ( x , info ): res = np . cos ( x ) if info [ 'print' ]: info [ 'iter_x' ] . append ( x ) info [ 'iter_res' ] . append ( res ) return res x = np . linspace ( 0 , 1 , 100 ) y = f ( x , { 'iter_x' : [], 'iter_res' : [], 'print' : False }) fig , ax = plt . subplots () ax . plot ( x , y , color = 'r' , zorder = 0 ) ax . axhline ( 0 , color = 'k' , linestyle = '-.' ) ax . set_title ( '$f(x)= x - cos(x)$' ) plt . show () info_newton = { 'iter_x' : [], 'iter_res' : [], 'print' : True } info_secant = { 'iter_x' : [], 'iter_res' : [], 'print' : True } info_bisect = { 'iter_x' : [], 'iter_res' : [], 'print' : True } info_fixed = { 'iter_x' : [], 'iter_res' : [], 'print' : True } newton = scop . newton ( func = f , x0 = 0.5 , fprime = lambda x , info : 1 + np . sin ( x ), tol = 1e-10 , maxiter = 200 , args = ( info_newton ,)) secant = scop . newton ( func = f , x0 = 0.5 , tol = 1e-10 , maxiter = 200 , args = ( info_secant ,)) bisect = scop . bisect ( f = f , a = 0 , b = 1 , xtol = 1e-10 , maxiter = 200 , args = ( info_bisect ,)) fixed_point = scop . fixed_point ( func = g , x0 = 0.5 , xtol = 1e-10 , bisectmethod = 'iteration' , maxiter = 200 , args = ( info_fixed ,)) plt . plot ( info_bisect [ 'iter_x' ], label = 'bisect: {} ' . format ( len ( info_bisect [ 'iter_x' ]))) plt . plot ( info_newton [ 'iter_x' ], label = 'newton: {} ' . format ( len ( info_newton [ 'iter_x' ]))) plt . plot ( np . array ( info_fixed [ 'iter_x' ]), label = 'fixed point: {} ' . format ( len ( info_fixed [ 'iter_x' ]))) plt . plot ( info_secant [ 'iter_x' ], label = 'secant: {} ' . format ( len ( info_secant [ 'iter_x' ]))) plt . legend () plt . title ( 'Comparando alguns m\u00e9todos' ) plt . xscale ( 'log' ) plt . show ()","title":"Um outro exemplo"},{"location":"curvas/assignments-a1/","text":"Temas para o trabalho de A1 Data de apresenta\u00e7\u00e3o dos trabalhos: 12/04/2021 e 16/04/2021 Temas te\u00f3ricos Teoria de Contato (Keti Tenenblat) Propriedades Globais (temas individuais) Desigualdade Isoperim\u00e9trica (Andrew Pressley, John Oprea) Teorema dos 4 v\u00e9rtices (Andrew Pressley) Curvas de Bezier (Duncan Marsh) Teorema Fundamental das Curvas Espaciais : Cristhian Grundmann e Igor Patr\u00edcio Michels Trabalhos h\u00edbridos (te\u00f3rico e computacional) Evolutas e Involutas: Aplica\u00e7\u00f5es, hist\u00f3ria da curva (Keti Tenenblat, John Oprea) Implica\u00e7\u00f5es de curvatura e tor\u00e7\u00e3o (John Oprea) C\u00f4nicas (Duncan Marsh) Trabalhos computacionais Desenho de curva a partir da curvatura (Chapman, John Oprea) Curvas de B\u00e9zier (Duncan Marsh)","title":"Temas para o trabalho de A1"},{"location":"curvas/assignments-a1/#temas-para-o-trabalho-de-a1","text":"Data de apresenta\u00e7\u00e3o dos trabalhos: 12/04/2021 e 16/04/2021","title":"Temas para o trabalho de A1"},{"location":"curvas/assignments-a1/#temas-teoricos","text":"Teoria de Contato (Keti Tenenblat) Propriedades Globais (temas individuais) Desigualdade Isoperim\u00e9trica (Andrew Pressley, John Oprea) Teorema dos 4 v\u00e9rtices (Andrew Pressley) Curvas de Bezier (Duncan Marsh) Teorema Fundamental das Curvas Espaciais : Cristhian Grundmann e Igor Patr\u00edcio Michels","title":"Temas te\u00f3ricos"},{"location":"curvas/assignments-a1/#trabalhos-hibridos-teorico-e-computacional","text":"Evolutas e Involutas: Aplica\u00e7\u00f5es, hist\u00f3ria da curva (Keti Tenenblat, John Oprea) Implica\u00e7\u00f5es de curvatura e tor\u00e7\u00e3o (John Oprea) C\u00f4nicas (Duncan Marsh)","title":"Trabalhos h\u00edbridos (te\u00f3rico e computacional)"},{"location":"curvas/assignments-a1/#trabalhos-computacionais","text":"Desenho de curva a partir da curvatura (Chapman, John Oprea) Curvas de B\u00e9zier (Duncan Marsh)","title":"Trabalhos computacionais"},{"location":"curvas/classic-curves/","text":"Curvas Cl\u00e1ssicas Como uma forma de curiosidade, nessa p\u00e1gina ser\u00e3o descritas algumas curvas que consideradas cl\u00e1ssicas na literatura, dada a hist\u00f3ria que fizeram na matem\u00e1tica, porque s\u00e3o famosas ou s\u00f3 porque s\u00e3o bonitas. Aqui ser\u00e1 fornecido apenas um pequeno resumo sobre cada uma delas e ponteiros para refer\u00eancias mais descritivas. Uma boa refer\u00eancia \u00e9 o livro de J. Dennis Lawrence . Contribui\u00e7\u00e3o volunt\u00e1ria Quem quiser contribuir com esse texto, basta seguir o seguinte processo: Fork o reposit\u00f3rio ta-sessions . Acesse o arquivo docs/curvas/classic-curves.md Fa\u00e7a as modifica\u00e7\u00f5es seguindo o padr\u00e3o das outras curvas, inserindo seu nome, quem descobriu, sua equa\u00e7\u00e3o ou parametriza\u00e7\u00e3o e uma figura, caso poss\u00edvel. Tamb\u00e9m \u00e9 necess\u00e1rio pelo menos uma refer\u00eancia exterior ao site. No caso de colocar imagem, coloque na pasta classic-curves_files e siga o padr\u00e3o daquelas que j\u00e1 est\u00e3o aqui. Procure fazer as curvas no geogebra. Para equa\u00e7\u00f5es n\u00e3o use as express\u00f5es entre $ , use \\(...\\) . Astroide Descri\u00e7\u00e3o Equa\u00e7\u00e3o Gr\u00e1fico A astroide foi discutida primeiramente pelo matem\u00e1tico Roemer em 1674 como busca para a melhor forma do dente da engrenagem. Ela \u00e9 chamada algumas vezes de tetrac\u00faspide devido \u00e0s quatro c\u00faspides (ponta). Ela ganhou esse nome apenas em 1838 em um livro de Vienna. A equa\u00e7\u00e3o propriamente foi descrita em cartas de Leibniz. Ela \u00e9 o lugar geom\u00e9trico de um ponto em uma circunfer\u00eancia que rola em uma circunfer\u00eancia maior de raio \\(a\\). Refer\u00eancia . \\(x^{2/3} + y^{2/3} = a^{2/3}\\) Cissoide de Diocles Descri\u00e7\u00e3o Equa\u00e7\u00e3o Gr\u00e1fico \u00c9 uma curva c\u00fabica planar que permite construir duas m\u00e9dias proporcionais a uma dada raz\u00e3o. Seu nome vem do grego \"forma de Hera\" e foi estuda por Diocles 2 s\u00e9culos antes da Era Comum. Ela \u00e9 o lugar geom\u00e9trico da interse\u00e7\u00e3o da reta tangente \u00e0 par\u00e1bola com a reta perpendicular a essa passando pela origem. Refer\u00eancia 1 , Refer\u00eancia 2 \\(2ay^3 - (x^2 + y^2)x = 0\\) Folium de Descartes Descri\u00e7\u00e3o Equa\u00e7\u00e3o Gr\u00e1fico Seu nome deriva do Latim que significa folha . Ela foi primeiro proposta por Ren\u00e9 Descartes em 1638. Ele desafiou o matem\u00e1tico Pierre de Fermat a encontrar a linha tangente a essa curva em um ponto qualquer. Podemos encontr\u00e1-la facilmente atrav\u00e9s da diferencia\u00e7\u00e3o impl\u00edcita. Refer\u00eancia 1 Refer\u00eancia 2 \\(x^3 + y^3 = 3axy\\) Espiral de Euler Descri\u00e7\u00e3o Equa\u00e7\u00e3o Gr\u00e1fico \u00c9 uma curva cuja curvatura varia linearmente conforme varia o comprimento de arco (veja o exemplo na p\u00e1gina sobre curvatura). Ela tem outros nomes como clotoide ou espirais de Cornu. Acredita-se que tenha sido primeiramente estudada por James Bernoulli em 1694. Sua equa\u00e7\u00e3o \u00e9 atrav\u00e9s da integral de Fresnel. Ela foi proposta como a solu\u00e7\u00e3o para o problema da elasticidade, mas hoje tem diversas aplica\u00e7\u00f5es como proje\u00e7\u00e3o do de uma esfera. \\(\\alpha(s) = (\\int_0^s \\cos(t^2)dt, \\int_0^s \\sin(t^2)dt\\)","title":"Curvas Cl\u00e1ssicas"},{"location":"curvas/classic-curves/#curvas-classicas","text":"Como uma forma de curiosidade, nessa p\u00e1gina ser\u00e3o descritas algumas curvas que consideradas cl\u00e1ssicas na literatura, dada a hist\u00f3ria que fizeram na matem\u00e1tica, porque s\u00e3o famosas ou s\u00f3 porque s\u00e3o bonitas. Aqui ser\u00e1 fornecido apenas um pequeno resumo sobre cada uma delas e ponteiros para refer\u00eancias mais descritivas. Uma boa refer\u00eancia \u00e9 o livro de J. Dennis Lawrence .","title":"Curvas Cl\u00e1ssicas"},{"location":"curvas/classic-curves/#contribuicao-voluntaria","text":"Quem quiser contribuir com esse texto, basta seguir o seguinte processo: Fork o reposit\u00f3rio ta-sessions . Acesse o arquivo docs/curvas/classic-curves.md Fa\u00e7a as modifica\u00e7\u00f5es seguindo o padr\u00e3o das outras curvas, inserindo seu nome, quem descobriu, sua equa\u00e7\u00e3o ou parametriza\u00e7\u00e3o e uma figura, caso poss\u00edvel. Tamb\u00e9m \u00e9 necess\u00e1rio pelo menos uma refer\u00eancia exterior ao site. No caso de colocar imagem, coloque na pasta classic-curves_files e siga o padr\u00e3o daquelas que j\u00e1 est\u00e3o aqui. Procure fazer as curvas no geogebra. Para equa\u00e7\u00f5es n\u00e3o use as express\u00f5es entre $ , use \\(...\\) . Astroide Descri\u00e7\u00e3o Equa\u00e7\u00e3o Gr\u00e1fico A astroide foi discutida primeiramente pelo matem\u00e1tico Roemer em 1674 como busca para a melhor forma do dente da engrenagem. Ela \u00e9 chamada algumas vezes de tetrac\u00faspide devido \u00e0s quatro c\u00faspides (ponta). Ela ganhou esse nome apenas em 1838 em um livro de Vienna. A equa\u00e7\u00e3o propriamente foi descrita em cartas de Leibniz. Ela \u00e9 o lugar geom\u00e9trico de um ponto em uma circunfer\u00eancia que rola em uma circunfer\u00eancia maior de raio \\(a\\). Refer\u00eancia . \\(x^{2/3} + y^{2/3} = a^{2/3}\\) Cissoide de Diocles Descri\u00e7\u00e3o Equa\u00e7\u00e3o Gr\u00e1fico \u00c9 uma curva c\u00fabica planar que permite construir duas m\u00e9dias proporcionais a uma dada raz\u00e3o. Seu nome vem do grego \"forma de Hera\" e foi estuda por Diocles 2 s\u00e9culos antes da Era Comum. Ela \u00e9 o lugar geom\u00e9trico da interse\u00e7\u00e3o da reta tangente \u00e0 par\u00e1bola com a reta perpendicular a essa passando pela origem. Refer\u00eancia 1 , Refer\u00eancia 2 \\(2ay^3 - (x^2 + y^2)x = 0\\) Folium de Descartes Descri\u00e7\u00e3o Equa\u00e7\u00e3o Gr\u00e1fico Seu nome deriva do Latim que significa folha . Ela foi primeiro proposta por Ren\u00e9 Descartes em 1638. Ele desafiou o matem\u00e1tico Pierre de Fermat a encontrar a linha tangente a essa curva em um ponto qualquer. Podemos encontr\u00e1-la facilmente atrav\u00e9s da diferencia\u00e7\u00e3o impl\u00edcita. Refer\u00eancia 1 Refer\u00eancia 2 \\(x^3 + y^3 = 3axy\\) Espiral de Euler Descri\u00e7\u00e3o Equa\u00e7\u00e3o Gr\u00e1fico \u00c9 uma curva cuja curvatura varia linearmente conforme varia o comprimento de arco (veja o exemplo na p\u00e1gina sobre curvatura). Ela tem outros nomes como clotoide ou espirais de Cornu. Acredita-se que tenha sido primeiramente estudada por James Bernoulli em 1694. Sua equa\u00e7\u00e3o \u00e9 atrav\u00e9s da integral de Fresnel. Ela foi proposta como a solu\u00e7\u00e3o para o problema da elasticidade, mas hoje tem diversas aplica\u00e7\u00f5es como proje\u00e7\u00e3o do de uma esfera. \\(\\alpha(s) = (\\int_0^s \\cos(t^2)dt, \\int_0^s \\sin(t^2)dt\\)","title":"Contribui\u00e7\u00e3o volunt\u00e1ria"},{"location":"curvas/first-definitions/","text":"Defini\u00e7\u00f5es preliminares Quando pensamos em curvas, em geral, expressamos como uma equa\u00e7\u00e3o, como, por exemplo, x^2 + y^2 = 1 que \u00e9 uma circunfer\u00eancia ou talvez na reta y = ax + b . Chamamos essas curvas de curvas de n\u00edvel , aquelas que s\u00e3o do tipo f(x,y) = c para uma fun\u00e7\u00e3o f:\\mathbb{R}^2 \\to \\mathbb{R} continua. Todavia uma defini\u00e7\u00e3o um tanto melhor \u00e9 pensar em uma curva como um caminho tra\u00e7ado por um ponto se movimentando. Curva parametrizada: Seja I um intervalo. Uma curva parametrizada \u00e9 uma aplica\u00e7\u00e3o cont\u00ednua \\alpha: I \\subset \\mathbb{R} \\to \\mathbb{R}^n , muitas vezes notada como \\alpha(t) = (\\alpha_1(t), ..., \\alpha_n(t)) e t \u00e9 chamado de par\u00e2metro. Algumas defini\u00e7\u00f5es pedem intervalo aberto. Dizemos que ela \u00e9 diferenci\u00e1vel quando a aplica\u00e7\u00e3o \u00e9 diferenci\u00e1vel. Por fim dizemos que a curva \u00e9 regular quando \\alpha '(t) \\neq 0, \\forall t \\in I . Observa\u00e7\u00e3o: Defini\u00e7\u00f5es de curva podem variar em cada livro. Alguns livros pedem que a aplica\u00e7\u00e3o seja de classe C^{\\infty} ou suave, enquanto outras pedem apenas classe C^2 e assim por diante. De forma geral exigir apenas a continuidade \u00e9 mais fraco e podemos pedir diferenciabilidade ou suavidade posteriormente. Tra\u00e7o da curva: Seja uma curma \\alpha:I \\to \\mathbb{R}^n . Dizemos que o tra\u00e7o de \\alpha \u00e9 a imagem da aplica\u00e7\u00e3o \\alpha , denotada \\alpha(I) . Algumas defini\u00e7\u00f5es de curva s\u00e3o precisamento o que definimos de tra\u00e7o da curva. import numpy as np import matplotlib.pyplot as plt fig = plt . figure () ax = fig . gca ( projection = '3d' ) # Prepare arrays x, y, z theta = np . linspace ( - 4 * np . pi , 4 * np . pi , 100 ) z = np . linspace ( - 2 , 2 , 100 ) r = z ** 2 + 1 x = r * np . sin ( theta ) y = r * np . cos ( theta ) ax . plot ( x , y , z , label = 'arbitrary parametric curve' ) ax . legend () plt . show () Encontrando parametriza\u00e7\u00f5es Exemplo 1 Vamos encontrar uma parametriza\u00e7\u00e3o para a par\u00e1bola y = x^2 na reta. Seja \\gamma(t) = (\\gamma_1(t), \\gamma_2(t)) . Pela rela\u00e7\u00e3o, temos que \\gamma_2(t) = \\gamma_1(t)^2, \\forall t \\in \\mathbb{R} . Uma solu\u00e7\u00e3o trivial sera colocar \\gamma_1(t) = t . Nesse caso, \\gamma(t) = (t,t^2) \u00e9 uma curva cujo tra\u00e7o \u00e9 uma par\u00e1bola. Observe que essa n\u00e3o \u00e9 a \u00fanica parametriza\u00e7\u00e3o . Por exemplo (\\frac{t}{2}, \\frac{t^2}{4}) tamb\u00e9m \u00e9 uma parametriza\u00e7\u00e3o na reta. Isso levanta uma quest\u00e3o: temos duas parametriza\u00e7\u00f5es diferentes para a mesma curva. Como dizer que elas s\u00e3o iguais, em um certo sentido, j\u00e1 que suas imagens s\u00e3o iguais? Exemplo 2 Considere a curva astroide dada pela pela equa\u00e7\u00e3o x^{2/3} + y^{2/3} = 1 . Uma maneira \u00e9 propor a parametriza\u00e7\u00e3o dada por x(t) = t e y(t) = (1 - t^{2/3})^{3/2}. Primeiro temos que observar que t \\in [-1,1] devido a raiz quadrada que tomamos na express\u00e3o - o valor dentro do par\u00eanteses n\u00e3o pode ser negativo. Em particular y n\u00e3o pode ser negativo nessa parametriza\u00e7\u00e3o. Isso n\u00e3o corresponde a imagem total da curva, pois y^{2/3} + x^{2/3} = 1 \u00e9 sim\u00e9trico em rela\u00e7\u00e3o ao dois eixos. Poder\u00edamos tentar adaptar essa parametriza\u00e7\u00e3o, mas o mais conveniente \u00e9 lembrar da identidade trigonom\u00e9trica cos^2(t) + sen^2(t) = 1 . Assim podemos escrever que (cos(t)^3)^{2/3} + (sen(t)^3)^{2/3} = 1 . Como consequ\u00eancia (cos^3(t), sen^3(t)) \u00e9 uma parametriza\u00e7\u00e3o da astroide. Note que essa curva \u00e9 cont\u00ednua e definida em toda reta. # Astroid fig = plt . figure ( figsize = ( 5 , 5 )) ax = plt . subplot () ax . grid ( alpha =. 5 ) t = np . linspace ( - np . pi , np . pi , 100 ) x = np . cos ( t ) ** 3 y = np . sin ( t ) ** 3 ax . plot ( x , y , label = 'Astroid' ) ax . axvline ( x = 0 , color = 'grey' , alpha = . 7 ) ax . axhline ( y = 0 , color = 'grey' , alpha = . 7 ) ax . legend () plt . show () Vetor tangente Em geral, quando estudamos curvas e superf\u00edcies, \u00e9 comum encontrar o tempo suave associado. A defini\u00e7\u00e3o de fun\u00e7\u00e3o suavde varia em cada contexto e pode ir desde uma fun\u00e7\u00e3o diferenci\u00e1vel com fun\u00e7\u00e3o cont\u00ednua at\u00e9 fun\u00e7\u00e3o que tem derivada de qualquer ordem (sempre considerando o intervalo I de defini\u00e7\u00e3o. Lembre que se \\gamma(t) = (\\gamma_1(t), ..., \\gamma_n(t) , a derivada de \\gamma \u00e9 \\dot{\\gamma(t)} = (\\dot{\\gamma_1}(t), ..., \\dot{\\gamma_n}(t)). Vetor tangente: Seja \\alpha uma curva parametrizada. Sua primeira derivada \\dot{\\alpha}(t) \u00e9 chamada de vetor tangente a cada tempo t . Proposi\u00e7\u00e3o Se o vetor tangente de uma curva parametrizada \u00e9 constante, ent\u00e3o o tra\u00e7o da curva \u00e9 parte de uma reta. De fato se \\dot{\\alpha}(t) = c , onde c \u00e9 um vetor constante, pelo teorema fundamental do c\u00e1lculo, \\alpha(t) = \\int_{t_0}^t \\dot{\\alpha}(s)ds = (t - t_0)c = ct + d, d = - t_0 c, t_0 \\in I Se c \\neq 0 , esta \u00e9 a equa\u00e7\u00e3o param\u00e9trica de um segmento de reta (potencialmente infinito). Se c = 0 , a imagem da curva \u00e9 um \u00fanico ponto. Comprimento de arco Definimos o comprimento de arco de uma curva \\gamma come\u00e7ando no ponto \\gamma(t_0) como a fun\u00e7\u00e3o s(t) = \\int_{t_0}^t ||\\dot{\\gamma}(s)||ds Se escolhermos um ponto \\tilde{t}_0 diferente, o resultado ser\u00e1 diferente. Dizemos que a curva tem velocidade unit\u00e1ria se ||\\dot{\\gamma}(t)|| = 1 Reparametriza\u00e7\u00e3o Sejam I e J intervalos. Uma mudan\u00e7a de par\u00e2metro \u00e9 uma fun\u00e7\u00e3o h: J \\to I bijetiva cont\u00ednua com inversa cont\u00ednua. Em particular, uma fun\u00e7\u00e3o com essa propriedade \u00e9 chamada de homeomorfismo . Sejam \\tilde{\\gamma}:J \\to \\mathbb{R}^n e \\gamma: I \\to \\mathbb{R}^n dua curvas. Dizemos que \\tilde{\\gamma} \u00e9 reparametriza\u00e7\u00e3o da curva \\gamma se existe uma mudan\u00e7a de par\u00e2metro h tal que \\tilde{\\gamma} = \\gamma \\circ h Essa nota\u00e7\u00e3o significa que \\forall t \\in J, \\tilde{\\gamma}(t) = \\gamma(h(t)) . Observe que se \\tilde{\\gamma} \u00e9 reparametriza\u00e7\u00e3o de \\gamma , essa \u00e9 reparametriza\u00e7\u00e3o da primeira. Observa\u00e7\u00e3o: Dependendo em como definimos curva, existem varia\u00e7\u00f5es nessa defini\u00e7\u00e3o. De forma geral, podemos dizer que duas curvas de classe C^k s\u00e3o equivalentes, isto \u00e9, uma \u00e9 reparametriza\u00e7\u00e3o da outra, quando existe uma mapa bijetivo de classe C^k com inversa tamb\u00e9m de classe C^k tal que a igualdade acima \u00e9 v\u00e1lida em todo ponto. Para mais detalhes, consulte o Wikipedia . Lembre que uma curva pode ter muitas parametriza\u00e7\u00f5es, mas nem todas s\u00e3o reparametriza\u00e7\u00f5es uma da outra, como no exemplo a baixo: Exemplo: Considere as seguintes parametriza\u00e7\u00f5es da circunfer\u00eancia: \\alpha(t) = (cos(t), sen(t)), t \\in [0,2\\pi] \\beta(t) = (cos(2t), sen(2t)), t \\in [0,2\\pi] A segunda parametriza\u00e7\u00e3o \"d\u00e1 uma volta a mais na circunfer\u00eancia\". Devemos nos perguntar se existe uma mudan\u00e7a de par\u00e2metro h entre esses intervalos que garanta cos(2t) = cos(h(t)) sen(2t) = sen(h(t)) N\u00e3o conseguimos fazer isso e manter a bijetividade de h entre os intervalos. Uma solu\u00e7\u00e3o para esse problema seria considerar o dom\u00ednio de \\beta o intervalor [0,\\pi] . Nesse caso h(t) = 2t \u00e9 uma mudan\u00e7a de par\u00e2metro entre as parametriza\u00e7\u00f5es. Proposi\u00e7\u00f5es importantes Tente demonstrar essas proposi\u00e7\u00f5es: Toda reparametriza\u00e7\u00e3o de uma curva regular \u00e9 regular. O comprimento de uma curva diferenci\u00e1vel regular n\u00e3o muda depois de uma reparametriza\u00e7\u00e3o. Teorema da reparametriza\u00e7\u00e3o Uma curva parametrizada tem uma reparametriza\u00e7\u00e3o com velocidade unit\u00e1ria se, e somente se, \u00e9 regular. Demonstra\u00e7\u00e3o Um rascunho da demonstra\u00e7\u00e3o supondo a regularidade da curva. Seja \\alpha uma curve (diferenci\u00e1vel). Queremos encontrar \\beta : J \\to \\mathbb{R}^n tal que \\beta = \\alpha \\circ h para algum h difeomorfismo (bijeito diferenci\u00e1vel com inversa diferenci\u00e1vel). Se existesse, ele deveria ter o seguinte comportamento, ||\\beta '(t)|| = ||\\alpha'(h(t))h'(t)|| = 1, por hip\u00f3tese. Dado t_0 \\in I, t \\in I, s(t) := \\int_{t_0}^t ||\\alpha '(\\tau)||d\\tau \u00e9 uma fun\u00e7\u00e3o crescente e deriv\u00e1vel, pois \\alpha \u00e9 regular. Ent\u00e3o ela possui uma fun\u00e7\u00e3o inversa t: s(I) \\to I tamb\u00e9m crescente e deriv\u00e1vel, de forma que t'(s) = \\frac{1}{\\frac{ds}{dt}(t(s))} = \\frac{1}{s'(t(s))} = \\frac{1}{||\\alpha'(t(s))||} Ent\u00e3o defina \\beta: s(I) \\to \\mathbb{R}^n de forma que \\beta(s) = \\alpha(t(s)) . Ent\u00e3o, ||\\beta'(s)|| = ||\\alpha'(t(s))t'(s)|| = 1 Ent\u00e3o a mudan\u00e7a de par\u00e2metro que est\u00e1vamos procurando era a inversa da fun\u00e7\u00e3o de comprimento de curva. Curvas fechadas Curva T-peri\u00f3dica Seja T \\in \\mathbb{R} . Dizemos que uma curva suave \\alpha : \\mathbb{R} \\to \\mathbb{R}^n \u00e9 T-peri\u00f3dica se \\alpha(t + T) = \\alpha(t), t \\in \\mathbb{R} Se \\alpha \u00e9 n\u00e3o constante, mas T-peri\u00f3dica, com T \\neq 0 , ent\u00e3o ela \u00e9 dita fechada . Dizemos que o per\u00edodo da curva fechada \u00e9 o menor n\u00famero positivo T tal que \\alpha seja T-peri\u00f3dica. Exemplo: A elipse \u00e9 um exemplo onde o per\u00eddo \u00e9 2\\pi . Auto-intersec\u00e7\u00e3o Uma curva \\alpha tem uma auto-intersec\u00e7\u00e3o no ponto p se existem a \\neq b tal que \\alpha(a) = \\alpha(b) = p e se \\alpha \u00e9 fechada com per\u00edodo T , ent\u00e3o a - b n\u00e3o \u00e9 um inteiro m\u00faltiplo de T .","title":"Defini\u00e7\u00f5es preliminares"},{"location":"curvas/first-definitions/#definicoes-preliminares","text":"Quando pensamos em curvas, em geral, expressamos como uma equa\u00e7\u00e3o, como, por exemplo, x^2 + y^2 = 1 que \u00e9 uma circunfer\u00eancia ou talvez na reta y = ax + b . Chamamos essas curvas de curvas de n\u00edvel , aquelas que s\u00e3o do tipo f(x,y) = c para uma fun\u00e7\u00e3o f:\\mathbb{R}^2 \\to \\mathbb{R} continua. Todavia uma defini\u00e7\u00e3o um tanto melhor \u00e9 pensar em uma curva como um caminho tra\u00e7ado por um ponto se movimentando. Curva parametrizada: Seja I um intervalo. Uma curva parametrizada \u00e9 uma aplica\u00e7\u00e3o cont\u00ednua \\alpha: I \\subset \\mathbb{R} \\to \\mathbb{R}^n , muitas vezes notada como \\alpha(t) = (\\alpha_1(t), ..., \\alpha_n(t)) e t \u00e9 chamado de par\u00e2metro. Algumas defini\u00e7\u00f5es pedem intervalo aberto. Dizemos que ela \u00e9 diferenci\u00e1vel quando a aplica\u00e7\u00e3o \u00e9 diferenci\u00e1vel. Por fim dizemos que a curva \u00e9 regular quando \\alpha '(t) \\neq 0, \\forall t \\in I . Observa\u00e7\u00e3o: Defini\u00e7\u00f5es de curva podem variar em cada livro. Alguns livros pedem que a aplica\u00e7\u00e3o seja de classe C^{\\infty} ou suave, enquanto outras pedem apenas classe C^2 e assim por diante. De forma geral exigir apenas a continuidade \u00e9 mais fraco e podemos pedir diferenciabilidade ou suavidade posteriormente. Tra\u00e7o da curva: Seja uma curma \\alpha:I \\to \\mathbb{R}^n . Dizemos que o tra\u00e7o de \\alpha \u00e9 a imagem da aplica\u00e7\u00e3o \\alpha , denotada \\alpha(I) . Algumas defini\u00e7\u00f5es de curva s\u00e3o precisamento o que definimos de tra\u00e7o da curva. import numpy as np import matplotlib.pyplot as plt fig = plt . figure () ax = fig . gca ( projection = '3d' ) # Prepare arrays x, y, z theta = np . linspace ( - 4 * np . pi , 4 * np . pi , 100 ) z = np . linspace ( - 2 , 2 , 100 ) r = z ** 2 + 1 x = r * np . sin ( theta ) y = r * np . cos ( theta ) ax . plot ( x , y , z , label = 'arbitrary parametric curve' ) ax . legend () plt . show ()","title":"Defini\u00e7\u00f5es preliminares"},{"location":"curvas/first-definitions/#encontrando-parametrizacoes","text":"","title":"Encontrando parametriza\u00e7\u00f5es"},{"location":"curvas/first-definitions/#exemplo-1","text":"Vamos encontrar uma parametriza\u00e7\u00e3o para a par\u00e1bola y = x^2 na reta. Seja \\gamma(t) = (\\gamma_1(t), \\gamma_2(t)) . Pela rela\u00e7\u00e3o, temos que \\gamma_2(t) = \\gamma_1(t)^2, \\forall t \\in \\mathbb{R} . Uma solu\u00e7\u00e3o trivial sera colocar \\gamma_1(t) = t . Nesse caso, \\gamma(t) = (t,t^2) \u00e9 uma curva cujo tra\u00e7o \u00e9 uma par\u00e1bola. Observe que essa n\u00e3o \u00e9 a \u00fanica parametriza\u00e7\u00e3o . Por exemplo (\\frac{t}{2}, \\frac{t^2}{4}) tamb\u00e9m \u00e9 uma parametriza\u00e7\u00e3o na reta. Isso levanta uma quest\u00e3o: temos duas parametriza\u00e7\u00f5es diferentes para a mesma curva. Como dizer que elas s\u00e3o iguais, em um certo sentido, j\u00e1 que suas imagens s\u00e3o iguais?","title":"Exemplo 1"},{"location":"curvas/first-definitions/#exemplo-2","text":"Considere a curva astroide dada pela pela equa\u00e7\u00e3o x^{2/3} + y^{2/3} = 1 . Uma maneira \u00e9 propor a parametriza\u00e7\u00e3o dada por x(t) = t e y(t) = (1 - t^{2/3})^{3/2}. Primeiro temos que observar que t \\in [-1,1] devido a raiz quadrada que tomamos na express\u00e3o - o valor dentro do par\u00eanteses n\u00e3o pode ser negativo. Em particular y n\u00e3o pode ser negativo nessa parametriza\u00e7\u00e3o. Isso n\u00e3o corresponde a imagem total da curva, pois y^{2/3} + x^{2/3} = 1 \u00e9 sim\u00e9trico em rela\u00e7\u00e3o ao dois eixos. Poder\u00edamos tentar adaptar essa parametriza\u00e7\u00e3o, mas o mais conveniente \u00e9 lembrar da identidade trigonom\u00e9trica cos^2(t) + sen^2(t) = 1 . Assim podemos escrever que (cos(t)^3)^{2/3} + (sen(t)^3)^{2/3} = 1 . Como consequ\u00eancia (cos^3(t), sen^3(t)) \u00e9 uma parametriza\u00e7\u00e3o da astroide. Note que essa curva \u00e9 cont\u00ednua e definida em toda reta. # Astroid fig = plt . figure ( figsize = ( 5 , 5 )) ax = plt . subplot () ax . grid ( alpha =. 5 ) t = np . linspace ( - np . pi , np . pi , 100 ) x = np . cos ( t ) ** 3 y = np . sin ( t ) ** 3 ax . plot ( x , y , label = 'Astroid' ) ax . axvline ( x = 0 , color = 'grey' , alpha = . 7 ) ax . axhline ( y = 0 , color = 'grey' , alpha = . 7 ) ax . legend () plt . show ()","title":"Exemplo 2"},{"location":"curvas/first-definitions/#vetor-tangente","text":"Em geral, quando estudamos curvas e superf\u00edcies, \u00e9 comum encontrar o tempo suave associado. A defini\u00e7\u00e3o de fun\u00e7\u00e3o suavde varia em cada contexto e pode ir desde uma fun\u00e7\u00e3o diferenci\u00e1vel com fun\u00e7\u00e3o cont\u00ednua at\u00e9 fun\u00e7\u00e3o que tem derivada de qualquer ordem (sempre considerando o intervalo I de defini\u00e7\u00e3o. Lembre que se \\gamma(t) = (\\gamma_1(t), ..., \\gamma_n(t) , a derivada de \\gamma \u00e9 \\dot{\\gamma(t)} = (\\dot{\\gamma_1}(t), ..., \\dot{\\gamma_n}(t)). Vetor tangente: Seja \\alpha uma curva parametrizada. Sua primeira derivada \\dot{\\alpha}(t) \u00e9 chamada de vetor tangente a cada tempo t .","title":"Vetor tangente"},{"location":"curvas/first-definitions/#proposicao","text":"Se o vetor tangente de uma curva parametrizada \u00e9 constante, ent\u00e3o o tra\u00e7o da curva \u00e9 parte de uma reta. De fato se \\dot{\\alpha}(t) = c , onde c \u00e9 um vetor constante, pelo teorema fundamental do c\u00e1lculo, \\alpha(t) = \\int_{t_0}^t \\dot{\\alpha}(s)ds = (t - t_0)c = ct + d, d = - t_0 c, t_0 \\in I Se c \\neq 0 , esta \u00e9 a equa\u00e7\u00e3o param\u00e9trica de um segmento de reta (potencialmente infinito). Se c = 0 , a imagem da curva \u00e9 um \u00fanico ponto.","title":"Proposi\u00e7\u00e3o"},{"location":"curvas/first-definitions/#comprimento-de-arco","text":"Definimos o comprimento de arco de uma curva \\gamma come\u00e7ando no ponto \\gamma(t_0) como a fun\u00e7\u00e3o s(t) = \\int_{t_0}^t ||\\dot{\\gamma}(s)||ds Se escolhermos um ponto \\tilde{t}_0 diferente, o resultado ser\u00e1 diferente. Dizemos que a curva tem velocidade unit\u00e1ria se ||\\dot{\\gamma}(t)|| = 1","title":"Comprimento de arco"},{"location":"curvas/first-definitions/#reparametrizacao","text":"Sejam I e J intervalos. Uma mudan\u00e7a de par\u00e2metro \u00e9 uma fun\u00e7\u00e3o h: J \\to I bijetiva cont\u00ednua com inversa cont\u00ednua. Em particular, uma fun\u00e7\u00e3o com essa propriedade \u00e9 chamada de homeomorfismo . Sejam \\tilde{\\gamma}:J \\to \\mathbb{R}^n e \\gamma: I \\to \\mathbb{R}^n dua curvas. Dizemos que \\tilde{\\gamma} \u00e9 reparametriza\u00e7\u00e3o da curva \\gamma se existe uma mudan\u00e7a de par\u00e2metro h tal que \\tilde{\\gamma} = \\gamma \\circ h Essa nota\u00e7\u00e3o significa que \\forall t \\in J, \\tilde{\\gamma}(t) = \\gamma(h(t)) . Observe que se \\tilde{\\gamma} \u00e9 reparametriza\u00e7\u00e3o de \\gamma , essa \u00e9 reparametriza\u00e7\u00e3o da primeira. Observa\u00e7\u00e3o: Dependendo em como definimos curva, existem varia\u00e7\u00f5es nessa defini\u00e7\u00e3o. De forma geral, podemos dizer que duas curvas de classe C^k s\u00e3o equivalentes, isto \u00e9, uma \u00e9 reparametriza\u00e7\u00e3o da outra, quando existe uma mapa bijetivo de classe C^k com inversa tamb\u00e9m de classe C^k tal que a igualdade acima \u00e9 v\u00e1lida em todo ponto. Para mais detalhes, consulte o Wikipedia . Lembre que uma curva pode ter muitas parametriza\u00e7\u00f5es, mas nem todas s\u00e3o reparametriza\u00e7\u00f5es uma da outra, como no exemplo a baixo: Exemplo: Considere as seguintes parametriza\u00e7\u00f5es da circunfer\u00eancia: \\alpha(t) = (cos(t), sen(t)), t \\in [0,2\\pi] \\beta(t) = (cos(2t), sen(2t)), t \\in [0,2\\pi] A segunda parametriza\u00e7\u00e3o \"d\u00e1 uma volta a mais na circunfer\u00eancia\". Devemos nos perguntar se existe uma mudan\u00e7a de par\u00e2metro h entre esses intervalos que garanta cos(2t) = cos(h(t)) sen(2t) = sen(h(t)) N\u00e3o conseguimos fazer isso e manter a bijetividade de h entre os intervalos. Uma solu\u00e7\u00e3o para esse problema seria considerar o dom\u00ednio de \\beta o intervalor [0,\\pi] . Nesse caso h(t) = 2t \u00e9 uma mudan\u00e7a de par\u00e2metro entre as parametriza\u00e7\u00f5es.","title":"Reparametriza\u00e7\u00e3o"},{"location":"curvas/first-definitions/#proposicoes-importantes","text":"Tente demonstrar essas proposi\u00e7\u00f5es: Toda reparametriza\u00e7\u00e3o de uma curva regular \u00e9 regular. O comprimento de uma curva diferenci\u00e1vel regular n\u00e3o muda depois de uma reparametriza\u00e7\u00e3o.","title":"Proposi\u00e7\u00f5es importantes"},{"location":"curvas/first-definitions/#teorema-da-reparametrizacao","text":"Uma curva parametrizada tem uma reparametriza\u00e7\u00e3o com velocidade unit\u00e1ria se, e somente se, \u00e9 regular.","title":"Teorema da reparametriza\u00e7\u00e3o"},{"location":"curvas/first-definitions/#demonstracao","text":"Um rascunho da demonstra\u00e7\u00e3o supondo a regularidade da curva. Seja \\alpha uma curve (diferenci\u00e1vel). Queremos encontrar \\beta : J \\to \\mathbb{R}^n tal que \\beta = \\alpha \\circ h para algum h difeomorfismo (bijeito diferenci\u00e1vel com inversa diferenci\u00e1vel). Se existesse, ele deveria ter o seguinte comportamento, ||\\beta '(t)|| = ||\\alpha'(h(t))h'(t)|| = 1, por hip\u00f3tese. Dado t_0 \\in I, t \\in I, s(t) := \\int_{t_0}^t ||\\alpha '(\\tau)||d\\tau \u00e9 uma fun\u00e7\u00e3o crescente e deriv\u00e1vel, pois \\alpha \u00e9 regular. Ent\u00e3o ela possui uma fun\u00e7\u00e3o inversa t: s(I) \\to I tamb\u00e9m crescente e deriv\u00e1vel, de forma que t'(s) = \\frac{1}{\\frac{ds}{dt}(t(s))} = \\frac{1}{s'(t(s))} = \\frac{1}{||\\alpha'(t(s))||} Ent\u00e3o defina \\beta: s(I) \\to \\mathbb{R}^n de forma que \\beta(s) = \\alpha(t(s)) . Ent\u00e3o, ||\\beta'(s)|| = ||\\alpha'(t(s))t'(s)|| = 1 Ent\u00e3o a mudan\u00e7a de par\u00e2metro que est\u00e1vamos procurando era a inversa da fun\u00e7\u00e3o de comprimento de curva.","title":"Demonstra\u00e7\u00e3o"},{"location":"curvas/first-definitions/#curvas-fechadas","text":"","title":"Curvas fechadas"},{"location":"curvas/first-definitions/#curva-t-periodica","text":"Seja T \\in \\mathbb{R} . Dizemos que uma curva suave \\alpha : \\mathbb{R} \\to \\mathbb{R}^n \u00e9 T-peri\u00f3dica se \\alpha(t + T) = \\alpha(t), t \\in \\mathbb{R} Se \\alpha \u00e9 n\u00e3o constante, mas T-peri\u00f3dica, com T \\neq 0 , ent\u00e3o ela \u00e9 dita fechada . Dizemos que o per\u00edodo da curva fechada \u00e9 o menor n\u00famero positivo T tal que \\alpha seja T-peri\u00f3dica. Exemplo: A elipse \u00e9 um exemplo onde o per\u00eddo \u00e9 2\\pi .","title":"Curva T-peri\u00f3dica"},{"location":"curvas/first-definitions/#auto-interseccao","text":"Uma curva \\alpha tem uma auto-intersec\u00e7\u00e3o no ponto p se existem a \\neq b tal que \\alpha(a) = \\alpha(b) = p e se \\alpha \u00e9 fechada com per\u00edodo T , ent\u00e3o a - b n\u00e3o \u00e9 um inteiro m\u00faltiplo de T .","title":"Auto-intersec\u00e7\u00e3o"},{"location":"curvas/fundamental-forms/","text":"Formas fundamentais Primeira forma fundamental A primeira preocupa\u00e7\u00e3o que podemos ter em uma superf\u00edcie \u00e9 medir a dist\u00e2ncia entre dois pontos (o ser humano sempre fez isso de diversas formas no planeta terra). Para isso, introduzimos o conceito da primeira forma fundamental . Defini\u00e7\u00e3o: Seja p um ponto na superf\u00edcie de S . A primeira forma fundamental de S em p associa v, w \\in T_pS ao escalar \\langle v, w \\rangle_{p,S} = v \\cdot w Seja v \\in T_pS . Ent\u00e3o, podemos representar w como uma combina\u00e7\u00e3o linear dos vetores \\sigma_u e \\sigma_v : w = \\lambda \\sigma_u + \\mu \\sigma_v . Assim \\langle w, w \\rangle = \\lambda^2\\langle \\sigma_u, \\sigma_u \\rangle + 2\\lambda\\mu\\langle \\sigma_u, \\sigma_v \\rangle + \\mu^2\\langle \\sigma_v, \\sigma_v \\rangle Definimos E = ||\\sigma_u||^2, F = \\sigma_u \\cdot \\sigma_v, G = ||\\sigma_v||^2 e assim, exprimimos a forma fundamental como Edu^2 + 2Fdudv + Gdv^2 em que du = \\lambda e dv = \\mu . Se \\gamma(t) = \\sigma(u(t), v(t)) , podemos calcular o comprimento de arco utilizando que \\dot{\\gamma} = \\dot{u}\\sigma_u + \\dot{v}\\sigma_v e, ent\u00e3o, ||\\dot{\\gamma}||^2 = E\\dot{u}^2 + 2F\\dot{u}\\dot{v} + G\\dot{v}^2 Portanto L(\\gamma) = \\int (E\\dot{u}^2 + 2F\\dot{u}\\dot{v} + G\\dot{v}^2)^{1/2} dt Isometrias em superf\u00edcies Se \\mathcal{S}_1 e \\mathcal{S}_2 s\u00e3o superf\u00edcies, dizemos que eles s\u00e3o localmente isom\u00e9tricos se qualquer curva de \\mathcal{S}_1 pode ser levada por um mapa suave para uma curva em \\mathcal{S}_2 de mesmo comprimento, isto \u00e9, toda curva pode ser levada de uma superf\u00edcie para outra, mantendo comprimento. O mapa que realiza essa fun\u00e7\u00e3o \u00e9 uma isometria local . Seja o mapa D_pf : T_p\\mathcal{S}_1 \\to T_{f(p)}\\mathcal{S}_2 a derivada da fun\u00e7\u00e3o suave f entre as superf\u00edcies. Podemos provar que f ser\u00e1 uma isometria local se, e somente se, D_p\\mathcal{S}_1 \u00e9 uma isometria (isto \u00e9, preserva dist\u00e2ncias) para todo ponto p \\in \\mathcal{S}_1 . Lembrando que por isometria, queremos dizer que \\langle v, v \\rangle_p = \\langle D_p f(v), D_p f(v) \\rangle_{f(p)}. Se f for uma isometria local, ele ser\u00e1 um difeomorfismo local dada a invertibilidade de sua derivada D_pf . Um corol\u00e1rio interessante \u00e9 que para todo mapa \\sigma_1 de \\mathcal{S}_1 , os patches \\sigma_1 de \\mathcal{S}_1 , e f \\circ \\sigma_1 de \\mathcal{S}_2 tem a mesma forma fundamental. Segunda forma fundamental Seja p=\\sigma(u,v) um ponto em uma superf\u00edcie. A medida que nos afastamos de (u,v) , a superf\u00edcie se distancia do plano tangente segundo a dist\u00e2ncia (aproximada) (\\sigma(u + \\Delta u, v + \\Delta v) - \\sigma(u,v))\\cdot N. Pelo teorema de Taylor, essa dist\u00e2ncia vale (\\sigma_u\\Delta u + \\sigma_v\\Delta v + \\frac{1}{2}(\\sigma_{uu}(\\Delta u)^2 + 2\\sigma_{uv}\\Delta u\\Delta v + \\sigma_{vv}(\\Delta v)^2) + R(\\Delta u, \\Delta v))\\cdot N onde esse esse resto sobre (\\Delta u)^2 + (\\Delta v)^2 tende a 0. Como N \u00e9 perpendicular a \\sigma_u, \\sigma_v , o resto da express\u00e3o \u00e9 escrita como \\frac{1}{2}(e(\\Delta u)^2 + 2f\\Delta u\\Delta v + g(\\Delta v)^2) + R(\\Delta u, \\Delta v), em que e = \\sigma_{uu}\\cdot N, f = \\sigma_{uv}\\cdot N, f = \\sigma_{vv}\\cdot N. A express\u00e3o acima \u00e9 a segunda forma fundamental e est\u00e1 associada ao curvamento da superf\u00edcie em rela\u00e7\u00e3o ao plano tangente. Mapa de Gauss e de Weingarten Mapa de Gauss: O mapa \\mathcal{G} : \\mathcal{S} \\to \\mathcal{S}^2 que associa cada ponto da superf\u00edcie p \\in \\mathcal{S} , o ponto N_p \\in \\mathcal{S}^2 que \u00e9 o vetor normal unit\u00e1rio de \\mathcal{S} em p . Medimos a varia\u00e7\u00e3o de N ao longo de \\mathcal{S} pela derivada D_p \\mathcal{G} : T_p\\mathcal{S} \\to T_{\\mathcal{G}(p)}\\mathcal{S}^2. Seja q = \\mathcal{G}(p) . O plano tangente a esse ponto \u00e9 perpendicular a q e passa pela origem. Observe, no entanto, que esse plano, \u00e9 perpendicular a N_p , que \u00e9 exatamente T_p\\mathcal{S} . Portanto esse mapa pode ser escrito como D_p \\mathcal{G} : T_p\\mathcal{S} \\to T_p\\mathcal{S}. Mapa de Weingarten: O mapa de Weingarten da superf\u00edcie \\mathcal{S} no ponto p \u00e9 definida como \\mathcal{W}_{p,S} = - D_p\\mathcal{S} . A segunda forma fundamental pode ser equivalentemente escrita como \\langle \\langle v, w \\rangle \\rangle_{p,S} := \\langle \\mathcal{W}_{p,S}(v), w \\rangle_{p,S}, v, w \\in T_p\\mathcal{S}. Podemos provar essa rela\u00e7\u00e3o, al\u00e9m de provar que o mapa de Weingarten \u00e9 autoadjunto . Curvatura normal e geod\u00e9sica: Seja uma curva \\gamma na superf\u00edcie \\mathcal{S} . A segunda derivada de \\gamma (relacionada com sua curvatura) pode ser escrita como combina\u00e7\u00e3o linear \\ddot{\\gamma} = \\kappa_n N + \\kappa_g(N\\times\\dot{\\gamma}). Chamamos \\kappa_n de curvatura normal e \\kappa_g de geod\u00e9sica. Em geral s\u00f3 a magnitude desses valores \u00e9 bem definida. Al\u00e9m disso \\kappa_n = L\\dot{u}^2 + 2M\\dot{u}\\dot{v} + N\\dot{v}^2","title":"Formas fundamentais"},{"location":"curvas/fundamental-forms/#formas-fundamentais","text":"","title":"Formas fundamentais"},{"location":"curvas/fundamental-forms/#primeira-forma-fundamental","text":"A primeira preocupa\u00e7\u00e3o que podemos ter em uma superf\u00edcie \u00e9 medir a dist\u00e2ncia entre dois pontos (o ser humano sempre fez isso de diversas formas no planeta terra). Para isso, introduzimos o conceito da primeira forma fundamental . Defini\u00e7\u00e3o: Seja p um ponto na superf\u00edcie de S . A primeira forma fundamental de S em p associa v, w \\in T_pS ao escalar \\langle v, w \\rangle_{p,S} = v \\cdot w Seja v \\in T_pS . Ent\u00e3o, podemos representar w como uma combina\u00e7\u00e3o linear dos vetores \\sigma_u e \\sigma_v : w = \\lambda \\sigma_u + \\mu \\sigma_v . Assim \\langle w, w \\rangle = \\lambda^2\\langle \\sigma_u, \\sigma_u \\rangle + 2\\lambda\\mu\\langle \\sigma_u, \\sigma_v \\rangle + \\mu^2\\langle \\sigma_v, \\sigma_v \\rangle Definimos E = ||\\sigma_u||^2, F = \\sigma_u \\cdot \\sigma_v, G = ||\\sigma_v||^2 e assim, exprimimos a forma fundamental como Edu^2 + 2Fdudv + Gdv^2 em que du = \\lambda e dv = \\mu . Se \\gamma(t) = \\sigma(u(t), v(t)) , podemos calcular o comprimento de arco utilizando que \\dot{\\gamma} = \\dot{u}\\sigma_u + \\dot{v}\\sigma_v e, ent\u00e3o, ||\\dot{\\gamma}||^2 = E\\dot{u}^2 + 2F\\dot{u}\\dot{v} + G\\dot{v}^2 Portanto L(\\gamma) = \\int (E\\dot{u}^2 + 2F\\dot{u}\\dot{v} + G\\dot{v}^2)^{1/2} dt","title":"Primeira forma fundamental"},{"location":"curvas/fundamental-forms/#isometrias-em-superficies","text":"Se \\mathcal{S}_1 e \\mathcal{S}_2 s\u00e3o superf\u00edcies, dizemos que eles s\u00e3o localmente isom\u00e9tricos se qualquer curva de \\mathcal{S}_1 pode ser levada por um mapa suave para uma curva em \\mathcal{S}_2 de mesmo comprimento, isto \u00e9, toda curva pode ser levada de uma superf\u00edcie para outra, mantendo comprimento. O mapa que realiza essa fun\u00e7\u00e3o \u00e9 uma isometria local . Seja o mapa D_pf : T_p\\mathcal{S}_1 \\to T_{f(p)}\\mathcal{S}_2 a derivada da fun\u00e7\u00e3o suave f entre as superf\u00edcies. Podemos provar que f ser\u00e1 uma isometria local se, e somente se, D_p\\mathcal{S}_1 \u00e9 uma isometria (isto \u00e9, preserva dist\u00e2ncias) para todo ponto p \\in \\mathcal{S}_1 . Lembrando que por isometria, queremos dizer que \\langle v, v \\rangle_p = \\langle D_p f(v), D_p f(v) \\rangle_{f(p)}. Se f for uma isometria local, ele ser\u00e1 um difeomorfismo local dada a invertibilidade de sua derivada D_pf . Um corol\u00e1rio interessante \u00e9 que para todo mapa \\sigma_1 de \\mathcal{S}_1 , os patches \\sigma_1 de \\mathcal{S}_1 , e f \\circ \\sigma_1 de \\mathcal{S}_2 tem a mesma forma fundamental.","title":"Isometrias em superf\u00edcies"},{"location":"curvas/fundamental-forms/#segunda-forma-fundamental","text":"Seja p=\\sigma(u,v) um ponto em uma superf\u00edcie. A medida que nos afastamos de (u,v) , a superf\u00edcie se distancia do plano tangente segundo a dist\u00e2ncia (aproximada) (\\sigma(u + \\Delta u, v + \\Delta v) - \\sigma(u,v))\\cdot N. Pelo teorema de Taylor, essa dist\u00e2ncia vale (\\sigma_u\\Delta u + \\sigma_v\\Delta v + \\frac{1}{2}(\\sigma_{uu}(\\Delta u)^2 + 2\\sigma_{uv}\\Delta u\\Delta v + \\sigma_{vv}(\\Delta v)^2) + R(\\Delta u, \\Delta v))\\cdot N onde esse esse resto sobre (\\Delta u)^2 + (\\Delta v)^2 tende a 0. Como N \u00e9 perpendicular a \\sigma_u, \\sigma_v , o resto da express\u00e3o \u00e9 escrita como \\frac{1}{2}(e(\\Delta u)^2 + 2f\\Delta u\\Delta v + g(\\Delta v)^2) + R(\\Delta u, \\Delta v), em que e = \\sigma_{uu}\\cdot N, f = \\sigma_{uv}\\cdot N, f = \\sigma_{vv}\\cdot N. A express\u00e3o acima \u00e9 a segunda forma fundamental e est\u00e1 associada ao curvamento da superf\u00edcie em rela\u00e7\u00e3o ao plano tangente.","title":"Segunda forma fundamental"},{"location":"curvas/fundamental-forms/#mapa-de-gauss-e-de-weingarten","text":"Mapa de Gauss: O mapa \\mathcal{G} : \\mathcal{S} \\to \\mathcal{S}^2 que associa cada ponto da superf\u00edcie p \\in \\mathcal{S} , o ponto N_p \\in \\mathcal{S}^2 que \u00e9 o vetor normal unit\u00e1rio de \\mathcal{S} em p . Medimos a varia\u00e7\u00e3o de N ao longo de \\mathcal{S} pela derivada D_p \\mathcal{G} : T_p\\mathcal{S} \\to T_{\\mathcal{G}(p)}\\mathcal{S}^2. Seja q = \\mathcal{G}(p) . O plano tangente a esse ponto \u00e9 perpendicular a q e passa pela origem. Observe, no entanto, que esse plano, \u00e9 perpendicular a N_p , que \u00e9 exatamente T_p\\mathcal{S} . Portanto esse mapa pode ser escrito como D_p \\mathcal{G} : T_p\\mathcal{S} \\to T_p\\mathcal{S}. Mapa de Weingarten: O mapa de Weingarten da superf\u00edcie \\mathcal{S} no ponto p \u00e9 definida como \\mathcal{W}_{p,S} = - D_p\\mathcal{S} . A segunda forma fundamental pode ser equivalentemente escrita como \\langle \\langle v, w \\rangle \\rangle_{p,S} := \\langle \\mathcal{W}_{p,S}(v), w \\rangle_{p,S}, v, w \\in T_p\\mathcal{S}. Podemos provar essa rela\u00e7\u00e3o, al\u00e9m de provar que o mapa de Weingarten \u00e9 autoadjunto . Curvatura normal e geod\u00e9sica: Seja uma curva \\gamma na superf\u00edcie \\mathcal{S} . A segunda derivada de \\gamma (relacionada com sua curvatura) pode ser escrita como combina\u00e7\u00e3o linear \\ddot{\\gamma} = \\kappa_n N + \\kappa_g(N\\times\\dot{\\gamma}). Chamamos \\kappa_n de curvatura normal e \\kappa_g de geod\u00e9sica. Em geral s\u00f3 a magnitude desses valores \u00e9 bem definida. Al\u00e9m disso \\kappa_n = L\\dot{u}^2 + 2M\\dot{u}\\dot{v} + N\\dot{v}^2","title":"Mapa de Gauss e de Weingarten"},{"location":"curvas/info/","text":"Informa\u00e7\u00f5es Gerais Monitoria de Curvas e Superf\u00edcies correspondente ao per\u00edodo de 2021.1. Dia: Quinta-feira 17h30min Link Google Meet Monitorias gravadas Ementa da disciplina Notebooks documentados T\u00f3picos Nesse item podemos encontrar diversas curvas cl\u00e1ssicas . Voc\u00ea pode contribuir para aumentar o acervo! Conceitos introdut\u00f3rios de curvas Defini\u00e7\u00f5es preliminares Curvatura Curvas no espa\u00e7o Notas sobre equa\u00e7\u00f5es de Frenet Conceitos introdut\u00f3rios de superf\u00edcies Resumo de topologia Superf\u00edcies Formas fundamentais Curvatura Gaussiana, m\u00e9dia, e principais Trabalhos Trabalhos sobre curvas Listas N\u00famero Itens discutidos Solu\u00e7\u00e3o 1 Conceito de curva param\u00e9trica, vetor tangente e exemplos 1 2 Reparametriza\u00e7\u00e3o e comprimento de arco 2 3 Comprimento de arco, curvatura e diferenciabilidade 3 4 Curvatura e tor\u00e7\u00e3o, triedro de Frenet 4 4.1 F\u00f3rmulas de Frenet para n\u00e3o unit-speed curvas - 5 Superf\u00edcies regulares, mudan\u00e7a de par\u00e2metro 5 6 Introdu\u00e7\u00e3o \u00e0 Topologia 6 7 Primeira forma fundamental 7 8 Formas fundamentais 8 Notas Monitoria Itens discutidos Arquivo V\u00eddeo 26/02/2021 Lista 1 e GeoGebra - Sim 05/03/2021 Lista 2 e conceito de reparametriza\u00e7\u00e3o Visualizar N\u00e3o 12/03/2021 Aplica\u00e7\u00f5es diferenci\u00e1veis Visualizar Sim 19/03/2021 Lista 3 e aplica\u00e7\u00f5es diferenci\u00e1veis Visualizar Sim 25/03/2021 Lista 4 e 4.1 e p\u00e1gina \"Curvas no Espa\u00e7o\" Frenet N\u00e3o 30/04/2021 Introdu\u00e7\u00e3o a superf\u00edcies, esfera e duplo cone Visualizar Sim 13/05/2021 Defini\u00e7\u00f5es de superf\u00edcies Visualizar N\u00e3o 20/05/2021 Lista 6 e conceitos de topologia Visualizar Sim Sugest\u00f5es Adicionais Teoria Curso de Geometria Diferencial IMPA Professor NJ WildBerger : Curvas cl\u00e1ssicas e Hist\u00f3ria da Geometria Diferencial Livro Introdu\u00e7\u00e3o \u00e0 Geometria Diferencial do professor Ronaldo F. de Lima Recursos Computacionais Recursos Geogebra de curvas Lista de sistemas alg\u00e9bricos : Eu tenho bastante familiaridade com o SymPy (Python) e Matlab. Comandos CAS Geogebra Artigo sobre open source CAS","title":"Curvas"},{"location":"curvas/info/#informacoes-gerais","text":"Monitoria de Curvas e Superf\u00edcies correspondente ao per\u00edodo de 2021.1. Dia: Quinta-feira 17h30min Link Google Meet Monitorias gravadas Ementa da disciplina Notebooks documentados","title":"Informa\u00e7\u00f5es Gerais"},{"location":"curvas/info/#topicos","text":"Nesse item podemos encontrar diversas curvas cl\u00e1ssicas . Voc\u00ea pode contribuir para aumentar o acervo! Conceitos introdut\u00f3rios de curvas Defini\u00e7\u00f5es preliminares Curvatura Curvas no espa\u00e7o Notas sobre equa\u00e7\u00f5es de Frenet Conceitos introdut\u00f3rios de superf\u00edcies Resumo de topologia Superf\u00edcies Formas fundamentais Curvatura Gaussiana, m\u00e9dia, e principais Trabalhos Trabalhos sobre curvas","title":"T\u00f3picos"},{"location":"curvas/info/#listas","text":"N\u00famero Itens discutidos Solu\u00e7\u00e3o 1 Conceito de curva param\u00e9trica, vetor tangente e exemplos 1 2 Reparametriza\u00e7\u00e3o e comprimento de arco 2 3 Comprimento de arco, curvatura e diferenciabilidade 3 4 Curvatura e tor\u00e7\u00e3o, triedro de Frenet 4 4.1 F\u00f3rmulas de Frenet para n\u00e3o unit-speed curvas - 5 Superf\u00edcies regulares, mudan\u00e7a de par\u00e2metro 5 6 Introdu\u00e7\u00e3o \u00e0 Topologia 6 7 Primeira forma fundamental 7 8 Formas fundamentais 8","title":"Listas"},{"location":"curvas/info/#notas","text":"Monitoria Itens discutidos Arquivo V\u00eddeo 26/02/2021 Lista 1 e GeoGebra - Sim 05/03/2021 Lista 2 e conceito de reparametriza\u00e7\u00e3o Visualizar N\u00e3o 12/03/2021 Aplica\u00e7\u00f5es diferenci\u00e1veis Visualizar Sim 19/03/2021 Lista 3 e aplica\u00e7\u00f5es diferenci\u00e1veis Visualizar Sim 25/03/2021 Lista 4 e 4.1 e p\u00e1gina \"Curvas no Espa\u00e7o\" Frenet N\u00e3o 30/04/2021 Introdu\u00e7\u00e3o a superf\u00edcies, esfera e duplo cone Visualizar Sim 13/05/2021 Defini\u00e7\u00f5es de superf\u00edcies Visualizar N\u00e3o 20/05/2021 Lista 6 e conceitos de topologia Visualizar Sim","title":"Notas"},{"location":"curvas/info/#sugestoes-adicionais","text":"","title":"Sugest\u00f5es Adicionais"},{"location":"curvas/info/#teoria","text":"Curso de Geometria Diferencial IMPA Professor NJ WildBerger : Curvas cl\u00e1ssicas e Hist\u00f3ria da Geometria Diferencial Livro Introdu\u00e7\u00e3o \u00e0 Geometria Diferencial do professor Ronaldo F. de Lima","title":"Teoria"},{"location":"curvas/info/#recursos-computacionais","text":"Recursos Geogebra de curvas Lista de sistemas alg\u00e9bricos : Eu tenho bastante familiaridade com o SymPy (Python) e Matlab. Comandos CAS Geogebra Artigo sobre open source CAS","title":"Recursos Computacionais"},{"location":"curvas/intro-topology/","text":"Introdu\u00e7\u00e3o \u00e0 topologia Topologia \u00e9 uma \u00e1rea da matem\u00e1tica que estuda objetos geom\u00e9tricos com no\u00e7\u00f5es de continuidade e converg\u00eancia. Material na internet sobre esse t\u00f3pico n\u00e3o falta, mas eu gostaria de destacar o curso de Introdu\u00e7\u00e3o \u00e0 Topologia Geral da Universidade de Bras\u00edlia pelo professor Andr\u00e9 Caldas. Estudar topologia pode contribuir para a compreens\u00e3o de superf\u00edcies em um n\u00edvel mais profundo, pois uma superf\u00edcie em \\mathbb{R}^3 \u00e9 um objeto que se parece com um plano na vizinhan\u00e7a de todo ponto. Mas esses conceitos s\u00f3 ficam precisos com o estudo de topologia. Aqui faremos apenas um resumo de alguns conceitos sob a \u00f3tica do que chamamos de espa\u00e7os m\u00e9tricos . Defini\u00e7\u00f5es b\u00e1sicas Bola aberta: A bola aberta de centro x \\in \\mathbb{R}^n e raio r > 0 \u00e9 o conjunto \\mathcal{B}_r(x) = \\{y \\in \\mathbb{R}^n : ||x-y|| < r\\} . Se n=1 , esses conjuntos s\u00e3o chamados de intervalos abertos e se n=2 de discos abertos. Bola fechada: \\bar{\\mathcal{B}}_r(x) = \\{y \\in \\mathbb{R}^n : ||x-y|| \\le r\\} . Conjunto aberto: Dizemos que o conjunto U \\subseteq \\mathbb{R}^n \u00e9 aberto se \\forall x \\in U, \\exists \\epsilon > 0 tal que \\mathcal{B}_{\\epsilon}(x) \\subseteq U . Conjunto fechado: Dizemos que F \\subseteq \\mathbb{R}^n \u00e9 fechado se F^c \u00e9 aberto. Lema: Toda bola aberta \u00e9 um conjunto aberto. Proposi\u00e7\u00e3o: A partir da defini\u00e7\u00e3o de conjunto aberto, podemos demonstrar que: (i) O conjunto vazio \\emptyset \u00e9 aberto. (ii) A uni\u00e3o de conjuntos abertos \u00e9 um conjunto aberto. (iii) A intersec\u00e7\u00e3o finita de de conjuntos abertos \u00e9 aberta. Podemos definir uma topologia usando (i)-(iii). Ponto interior: se x \\in A \\subseteq \\mathbb{R}^n \u00e9 ponto interior de A se existe \\epsilon > 0 tal que \\mathcal{B}_{\\epsilon}(x) \\subseteq A . Vizinhan\u00e7a: Dizemos que U \\subseteq \\mathbb{R}^n \u00e9 uma vizinhan\u00e7a do ponto x \\in \\mathbb{R}^n se existe \\epsilon > 0 tal que \\mathcal{B}_{\\epsilon}(x) \\subseteq U . Ponto de fronteira: Se x \\in \\mathbb{R} \u00e9 ponto de fronteira de A se \\forall \\epsilon > 0 , a bola \\mathcal{B}_{\\epsilon}(x) cont\u00e9m pontos de A e pontos de A^c . O conjunto desses pontos \u00e9 notado como \\partial A . Caracteriza\u00e7\u00e3o de conjuntos fechados: Seja F \\subseteq \\mathbb{R}^n . Ele \u00e9 fechado se, e somente se, toda sequ\u00eancia de elementos de F converge a um elemento de F . Converg\u00eancia e continuidade Converg\u00eancia: Seja \\{x_n\\}_{n \\in \\mathbb{N}} \\subseteq \\mathbb{R}^n uma sequ\u00eancia de pontos. Dizemos que x_n converge para um ponto x^* \\in \\mathbb{R}^n , quando \\forall \\epsilon > 0 , existir N \\in \\mathbb{N} tal que n \\ge N, d(x_n, x^*) < \\epsilon e denotamos x_n \\to x^* . Ponto de ader\u00eancia: Seja A \\subseteq \\mathbb{R}^n . Dizemos que a \\in \\mathbb{R}^n \u00e9 ponto aderente de A se existe \\{x_n\\} \\subseteq A tal que x_n converge para a . Continuidade: Uma fun\u00e7\u00e3o f : \\mathbb{R}^n \\to \\mathbb{R}^m \u00e9 cont\u00ednua se para todo conjunto aberto V \\subseteq \\mathbb{R}^m , a imagem inversa f^{-1}(V) = \\{x \\in \\mathbb{R}^n | f(x) \\in V\\} \u00e9 conjunto aberto. De forma equivalente, dizemos que f \u00e9 cont\u00ednua em a \\in \\mathbb{R}^n quando para todo sequ\u00eancia \\{x_n\\} \\subseteq \\mathbb{R}^n convergente para a , ent\u00e3o f(x_n) converge para f(a) e f \u00e9 cont\u00ednua quando \u00e9 cont\u00ednua para todo ponto a \\in \\mathbb{R}^n . Agora, se f: X \\subseteq \\mathbb{R}^n \\to Y \\subseteq \\mathbb{R}^m , dizemos que ela \u00e9 cont\u00ednua quando para todo V \\subseteq \\mathbb{R}^m , existe um conjunto aberto U \\subseteq \\mathbb{R}^n tal que U \\cap X = \\{x \\in U: f(x) \\in V\\} . Homeomorfismo: Se f : X \\to Y \u00e9 cont\u00ednua e bijetiva e se o mapa inverso f^{-1} : Y \\to X tamb\u00e9m \u00e9 cont\u00ednuo, dizemos que f \u00e9 um homeomorfismo e X e Y s\u00e3o homeomorfos . Difeomorfismo: Sejam X \\subseteq \\mathbb{R}^n e Y \\subseteq \\mathbb{R}^m . Dizemos que f : X \\to Y \u00e9 diferenci\u00e1vel quando para cada a \\in X , existe uma extens\u00e3o um aberto U \\subseteq \\mathbb{R}^n contendo a tal que F: U \\to \\mathbb{R}^m \u00e9 diferenci\u00e1vel e F|_{U \\cap X} = f|_{U \\cap X} . Se f \u00e9 um homeomorfismo diferenci\u00e1vel e f^{-1} \u00e9 diferenci\u00e1vel, ent\u00e3o f \u00e9 um difeomorfismo. Ent\u00e3o X e Y s\u00e3o difeomorfos.","title":"Introdu\u00e7\u00e3o \u00e0 topologia"},{"location":"curvas/intro-topology/#introducao-a-topologia","text":"Topologia \u00e9 uma \u00e1rea da matem\u00e1tica que estuda objetos geom\u00e9tricos com no\u00e7\u00f5es de continuidade e converg\u00eancia. Material na internet sobre esse t\u00f3pico n\u00e3o falta, mas eu gostaria de destacar o curso de Introdu\u00e7\u00e3o \u00e0 Topologia Geral da Universidade de Bras\u00edlia pelo professor Andr\u00e9 Caldas. Estudar topologia pode contribuir para a compreens\u00e3o de superf\u00edcies em um n\u00edvel mais profundo, pois uma superf\u00edcie em \\mathbb{R}^3 \u00e9 um objeto que se parece com um plano na vizinhan\u00e7a de todo ponto. Mas esses conceitos s\u00f3 ficam precisos com o estudo de topologia. Aqui faremos apenas um resumo de alguns conceitos sob a \u00f3tica do que chamamos de espa\u00e7os m\u00e9tricos .","title":"Introdu\u00e7\u00e3o \u00e0 topologia"},{"location":"curvas/intro-topology/#definicoes-basicas","text":"Bola aberta: A bola aberta de centro x \\in \\mathbb{R}^n e raio r > 0 \u00e9 o conjunto \\mathcal{B}_r(x) = \\{y \\in \\mathbb{R}^n : ||x-y|| < r\\} . Se n=1 , esses conjuntos s\u00e3o chamados de intervalos abertos e se n=2 de discos abertos. Bola fechada: \\bar{\\mathcal{B}}_r(x) = \\{y \\in \\mathbb{R}^n : ||x-y|| \\le r\\} . Conjunto aberto: Dizemos que o conjunto U \\subseteq \\mathbb{R}^n \u00e9 aberto se \\forall x \\in U, \\exists \\epsilon > 0 tal que \\mathcal{B}_{\\epsilon}(x) \\subseteq U . Conjunto fechado: Dizemos que F \\subseteq \\mathbb{R}^n \u00e9 fechado se F^c \u00e9 aberto. Lema: Toda bola aberta \u00e9 um conjunto aberto. Proposi\u00e7\u00e3o: A partir da defini\u00e7\u00e3o de conjunto aberto, podemos demonstrar que: (i) O conjunto vazio \\emptyset \u00e9 aberto. (ii) A uni\u00e3o de conjuntos abertos \u00e9 um conjunto aberto. (iii) A intersec\u00e7\u00e3o finita de de conjuntos abertos \u00e9 aberta. Podemos definir uma topologia usando (i)-(iii). Ponto interior: se x \\in A \\subseteq \\mathbb{R}^n \u00e9 ponto interior de A se existe \\epsilon > 0 tal que \\mathcal{B}_{\\epsilon}(x) \\subseteq A . Vizinhan\u00e7a: Dizemos que U \\subseteq \\mathbb{R}^n \u00e9 uma vizinhan\u00e7a do ponto x \\in \\mathbb{R}^n se existe \\epsilon > 0 tal que \\mathcal{B}_{\\epsilon}(x) \\subseteq U . Ponto de fronteira: Se x \\in \\mathbb{R} \u00e9 ponto de fronteira de A se \\forall \\epsilon > 0 , a bola \\mathcal{B}_{\\epsilon}(x) cont\u00e9m pontos de A e pontos de A^c . O conjunto desses pontos \u00e9 notado como \\partial A . Caracteriza\u00e7\u00e3o de conjuntos fechados: Seja F \\subseteq \\mathbb{R}^n . Ele \u00e9 fechado se, e somente se, toda sequ\u00eancia de elementos de F converge a um elemento de F .","title":"Defini\u00e7\u00f5es b\u00e1sicas"},{"location":"curvas/intro-topology/#convergencia-e-continuidade","text":"Converg\u00eancia: Seja \\{x_n\\}_{n \\in \\mathbb{N}} \\subseteq \\mathbb{R}^n uma sequ\u00eancia de pontos. Dizemos que x_n converge para um ponto x^* \\in \\mathbb{R}^n , quando \\forall \\epsilon > 0 , existir N \\in \\mathbb{N} tal que n \\ge N, d(x_n, x^*) < \\epsilon e denotamos x_n \\to x^* . Ponto de ader\u00eancia: Seja A \\subseteq \\mathbb{R}^n . Dizemos que a \\in \\mathbb{R}^n \u00e9 ponto aderente de A se existe \\{x_n\\} \\subseteq A tal que x_n converge para a . Continuidade: Uma fun\u00e7\u00e3o f : \\mathbb{R}^n \\to \\mathbb{R}^m \u00e9 cont\u00ednua se para todo conjunto aberto V \\subseteq \\mathbb{R}^m , a imagem inversa f^{-1}(V) = \\{x \\in \\mathbb{R}^n | f(x) \\in V\\} \u00e9 conjunto aberto. De forma equivalente, dizemos que f \u00e9 cont\u00ednua em a \\in \\mathbb{R}^n quando para todo sequ\u00eancia \\{x_n\\} \\subseteq \\mathbb{R}^n convergente para a , ent\u00e3o f(x_n) converge para f(a) e f \u00e9 cont\u00ednua quando \u00e9 cont\u00ednua para todo ponto a \\in \\mathbb{R}^n . Agora, se f: X \\subseteq \\mathbb{R}^n \\to Y \\subseteq \\mathbb{R}^m , dizemos que ela \u00e9 cont\u00ednua quando para todo V \\subseteq \\mathbb{R}^m , existe um conjunto aberto U \\subseteq \\mathbb{R}^n tal que U \\cap X = \\{x \\in U: f(x) \\in V\\} . Homeomorfismo: Se f : X \\to Y \u00e9 cont\u00ednua e bijetiva e se o mapa inverso f^{-1} : Y \\to X tamb\u00e9m \u00e9 cont\u00ednuo, dizemos que f \u00e9 um homeomorfismo e X e Y s\u00e3o homeomorfos . Difeomorfismo: Sejam X \\subseteq \\mathbb{R}^n e Y \\subseteq \\mathbb{R}^m . Dizemos que f : X \\to Y \u00e9 diferenci\u00e1vel quando para cada a \\in X , existe uma extens\u00e3o um aberto U \\subseteq \\mathbb{R}^n contendo a tal que F: U \\to \\mathbb{R}^m \u00e9 diferenci\u00e1vel e F|_{U \\cap X} = f|_{U \\cap X} . Se f \u00e9 um homeomorfismo diferenci\u00e1vel e f^{-1} \u00e9 diferenci\u00e1vel, ent\u00e3o f \u00e9 um difeomorfismo. Ent\u00e3o X e Y s\u00e3o difeomorfos.","title":"Converg\u00eancia e continuidade"},{"location":"curvas/notes-frenet/","text":"F\u00f3rmulas de Frenet de Curvas Regulares Seja \\alpha : I \\to \\mathbb{R}^3 uma curva regular, isto \u00e9, tal que ||\\alpha'(t)|| nunca se anula. Por esse motivo, existe uma reparametriza\u00e7\u00e3o de \\alpha pelo comprimento de arco. Defina h : I \\to J \\\\ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ t \\mapsto h(t) = \\int_{t_0}^t ||\\alpha'(r)||dr e \\phi(s) := h^{-1}(s) . Assim, seja \\beta a reparametriza\u00e7\u00e3o pelo comprimento de arco, \\beta = \\alpha \\circ \\phi : J \\to \\mathbb{R}^3 Observe que \\dfrac{d}{dt}h(c) = ||\\alpha'(c)|| e que \\dfrac{d}{ds}\\phi(d) = \\dfrac{1}{\\frac{dh}{dt}(\\phi(d))} pelo Teorema da Fun\u00e7\u00e3o Inversa na reta. Diremos que \\phi(s) = t e que h(t) = s . Sejam T_{\\beta}, N_{\\beta}, B_{\\beta} o triedro de Frenet de \\beta (que \u00e9 parametrizada pelo comprimento de arco) e \\kappa_{\\beta} e \\tau_{\\beta} a curvatura e a tor\u00e7\u00e3o, respectivamente. Ent\u00e3o, definimos o triedro de Frenet para \\alpha da seguinte forma: T_{\\alpha} : I \\to \\mathbb{R}^3 \\\\ ~~~~~~~~~~~~~~~~~~ t \\mapsto T_{\\beta}(h(t)) N_{\\alpha} : I \\to \\mathbb{R}^3 \\\\ ~~~~~~~~~~~~~~~~~~ t \\mapsto N_{\\beta}(h(t)) B_{\\alpha} : I \\to \\mathbb{R}^3 \\\\ ~~~~~~~~~~~~~~~~~~ t \\mapsto B_{\\beta}(h(t)) \\kappa_{\\alpha} : I \\to \\mathbb{R}^3 \\\\ ~~~~~~~~~~~~~~~~~~ t \\mapsto \\kappa_{\\beta}(h(t)) \\tau_{\\alpha} : I \\to \\mathbb{R}^3 \\\\ ~~~~~~~~~~~~~~~~~~ t \\mapsto \\tau_{\\beta}(h(t)) Como h(t) \\in J , tudo est\u00e1 bem definido acima. Nesse caso, vamos provar que T_{\\alpha}(t) = \\dfrac{\\dot\\alpha(t)}{||\\dot\\alpha(t)||} . Veja que T_{\\alpha}(t) = T_{\\beta}(h(t)) = \\dot\\beta(h(t)) por defini\u00e7\u00e3o. Al\u00e9m disso, \\dot\\beta(h(t)) = \\dfrac{d}{ds}\\alpha(\\phi(h(t))) (lembrando que s = h(t) ).Pela regra da cadeia, \\dot\\beta(h(t)) = \\dot{\\alpha}(\\phi(h(t)))\\dot\\phi(h(t)) = \\dot\\alpha(t)\\dot{\\phi}(h(t)), pois \\phi(h(t)) = \\phi(s) = t . Por fim, usando o Teorema da Fun\u00e7\u00e3o Inversa, \\dot\\alpha(t)\\dot{\\phi}(h(t)) = \\frac{\\dot\\alpha(t)}{\\dot{h}(\\phi(h(t)))} = \\frac{\\dot\\alpha(t)}{||\\dot{\\alpha}(t)||} o que prova nosso resultado inicial. Esse texto foi feito para deixar mais claro como fazer o exerc\u00edcio 6 da lista 4 e como calcular os vetores normal e binormal para uma curva regular qualquer.","title":"F\u00f3rmulas de Frenet de Curvas Regulares"},{"location":"curvas/notes-frenet/#formulas-de-frenet-de-curvas-regulares","text":"Seja \\alpha : I \\to \\mathbb{R}^3 uma curva regular, isto \u00e9, tal que ||\\alpha'(t)|| nunca se anula. Por esse motivo, existe uma reparametriza\u00e7\u00e3o de \\alpha pelo comprimento de arco. Defina h : I \\to J \\\\ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ t \\mapsto h(t) = \\int_{t_0}^t ||\\alpha'(r)||dr e \\phi(s) := h^{-1}(s) . Assim, seja \\beta a reparametriza\u00e7\u00e3o pelo comprimento de arco, \\beta = \\alpha \\circ \\phi : J \\to \\mathbb{R}^3 Observe que \\dfrac{d}{dt}h(c) = ||\\alpha'(c)|| e que \\dfrac{d}{ds}\\phi(d) = \\dfrac{1}{\\frac{dh}{dt}(\\phi(d))} pelo Teorema da Fun\u00e7\u00e3o Inversa na reta. Diremos que \\phi(s) = t e que h(t) = s . Sejam T_{\\beta}, N_{\\beta}, B_{\\beta} o triedro de Frenet de \\beta (que \u00e9 parametrizada pelo comprimento de arco) e \\kappa_{\\beta} e \\tau_{\\beta} a curvatura e a tor\u00e7\u00e3o, respectivamente. Ent\u00e3o, definimos o triedro de Frenet para \\alpha da seguinte forma: T_{\\alpha} : I \\to \\mathbb{R}^3 \\\\ ~~~~~~~~~~~~~~~~~~ t \\mapsto T_{\\beta}(h(t)) N_{\\alpha} : I \\to \\mathbb{R}^3 \\\\ ~~~~~~~~~~~~~~~~~~ t \\mapsto N_{\\beta}(h(t)) B_{\\alpha} : I \\to \\mathbb{R}^3 \\\\ ~~~~~~~~~~~~~~~~~~ t \\mapsto B_{\\beta}(h(t)) \\kappa_{\\alpha} : I \\to \\mathbb{R}^3 \\\\ ~~~~~~~~~~~~~~~~~~ t \\mapsto \\kappa_{\\beta}(h(t)) \\tau_{\\alpha} : I \\to \\mathbb{R}^3 \\\\ ~~~~~~~~~~~~~~~~~~ t \\mapsto \\tau_{\\beta}(h(t)) Como h(t) \\in J , tudo est\u00e1 bem definido acima. Nesse caso, vamos provar que T_{\\alpha}(t) = \\dfrac{\\dot\\alpha(t)}{||\\dot\\alpha(t)||} . Veja que T_{\\alpha}(t) = T_{\\beta}(h(t)) = \\dot\\beta(h(t)) por defini\u00e7\u00e3o. Al\u00e9m disso, \\dot\\beta(h(t)) = \\dfrac{d}{ds}\\alpha(\\phi(h(t))) (lembrando que s = h(t) ).Pela regra da cadeia, \\dot\\beta(h(t)) = \\dot{\\alpha}(\\phi(h(t)))\\dot\\phi(h(t)) = \\dot\\alpha(t)\\dot{\\phi}(h(t)), pois \\phi(h(t)) = \\phi(s) = t . Por fim, usando o Teorema da Fun\u00e7\u00e3o Inversa, \\dot\\alpha(t)\\dot{\\phi}(h(t)) = \\frac{\\dot\\alpha(t)}{\\dot{h}(\\phi(h(t)))} = \\frac{\\dot\\alpha(t)}{||\\dot{\\alpha}(t)||} o que prova nosso resultado inicial. Esse texto foi feito para deixar mais claro como fazer o exerc\u00edcio 6 da lista 4 e como calcular os vetores normal e binormal para uma curva regular qualquer.","title":"F\u00f3rmulas de Frenet de Curvas Regulares"},{"location":"curvas/regular-surfaces/","text":"Superf\u00edcies Defini\u00e7\u00e3o: Dizemos que S \\subseteq \\mathbb{R}^n \u00e9 uma superf\u00edcie se para todo ponto p \\in S , existe um conjunto aberto U \\subset \\mathbb{R}^2 e um conjunto aberto W \\subseteq \\mathbb{R}^3 contendo p tal que S \\cap W \u00e9 homeomorfo a U . O homeomorfismo \\sigma : U \\to S \\cap W \u00e9 um patch ou parametriza\u00e7\u00e3o . A cole\u00e7\u00e3o desses homeomorfismos que cobrem S \u00e9 chamada de atlas . Dizemos que S \\cap W \u00e9 aberto em S sempre que W for aberto. Observe que para cada ponto da superf\u00edcie, olhamos para uma vizinhan\u00e7a dele (conjunto aberto que o cont\u00e9m) e essa vizinhan\u00e7a restrita \u00e0 superf\u00edcie deve ser \"parecida\" com um subconjunto do plano. Quando caminhamos no planeta, temos a sensa\u00e7\u00e3o de caminharmos num plano justamente por esse conceito. Tamb\u00e9m observe que nosso atlas pode ser formado por uma quantidade infinita de patches, mas, em geral, existe um n\u00famero m\u00ednimo. Superf\u00edcies suaves Defini\u00e7\u00e3o: Um patch \\sigma : U \\to \\mathbb{R}^3 \u00e9 regular se \u00e9 uma fun\u00e7\u00e3o suave (diferenci\u00e1vel nas tr\u00eas componentes com derivadas parciais cont\u00ednuas de todas as ordens) e os vetores \\sigma_u = \\frac{\\partial}{\\partial u} \\sigma e \\sigma_v = \\frac{\\partial}{\\partial v} \\sigma s\u00e3o linearmente independentes para todo (u,v) \\in U . Isto \u00e9, basta exigir que \\sigma_u \\times \\sigma_v \\neq 0 . Uma superf\u00edcie regular \u00e9 uma superf\u00edcie tal que p \\in S , existe um patch regular cuja imagem contenha p . Proposi\u00e7\u00e3o: Sejam U e V abertos de \\mathbb{R}^2 e \\sigma : U \\to \\mathbb{R}^3 um patch regular. Seja \\phi : V \\to U um difeomorfismo. Ent\u00e3o \\tilde{\\sigma} = \\sigma \\circ \\phi : V \\to \\mathbb{R}^3 \u00e9 um patch regular. Dizemos que \\sigma e \\tilde{\\sigma} s\u00e3o reparametriza\u00e7\u00f5es um do outro e \\phi um mapa reparametriza\u00e7\u00e3o. Esse princ\u00edpio permite que definhamos uma propriedade para superf\u00edcies regulares desde que definhamos para qualquer patch regular de forma que ser\u00e1 inalterada para outra parametriza\u00e7\u00e3o. Mapas suaves Queremos entender a no\u00e7\u00e3o de mapa suave entre duas superf\u00edcies suaves. At\u00e9 agora, definimos a suavidade entre conjuntos de \\mathbb{R}^n . Suponha que as superf\u00edcies S_1 e S_2 s\u00e3o cobertas pelos patches \\sigma_1 : U_1 \\to \\mathbb{R}^3 e \\sigma_2 : U_2 \\to \\mathbb{R}^3 , respectivamente. Dizemos que f \u00e9 suave se \\sigma_2^{-1} \\circ f \\circ \\sigma_1 : U_1 \\to U_2 \u00e9 suave. Al\u00e9m disso, se f: S_1 \\to S_2 \u00e9 um difeomorfismo e \\sigma_1 \u00e9 um patch regular S_1 , ent\u00e3o f \\circ \\sigma_1 \u00e9 um patch regular em S_2 . Tangentes Defini\u00e7\u00e3o: Um vetor tangente a uma superf\u00edcie S em um ponto p \\in S \u00e9 o vetor tangente em p de uma curva em S que passa por p . O espa\u00e7o tangente \u00e9 o conjunto desses vetores e \u00e9 denotado por T_pS . Seja uma curva \\alpha em S . que passa por p . Em uma vizinhan\u00e7a de p , podemos dizer que \\alpha(t) = \\sigma(u(t), v(t)) tal que \\alpha(t_0) = p . Podemos provar que u e v s\u00e3o suaves. Essa curva tem uma tangente em p denotada por \\alpha '(t_0) . Mas note que, essa curva n\u00e3o \u00e9 necessariamente a \u00fanica que passa por p . Essa defini\u00e7\u00e3o \u00e9 pouco trat\u00e1vel para enxergar o espa\u00e7o tangente. Para isso, o teorema a seguir prop\u00f5e uma caracteriza\u00e7\u00e3o mais palat\u00e1vel: Proposi\u00e7\u00e3o: Seja \\sigma : U \\to \\mathbb{R}^3 um patch da superf\u00edcie S que passa pelo ponto p em (u_0, v_0) . O espa\u00e7o tangente a S em p \u00e9 o plano gerado pelos vetores \\sigma_u(u_0, v_0) e \\sigma_v(u_0, v_0) que passa pela origem. Observe que o plano tangente \u00e9 independente da escolha do patch. Derivada de mapa suave: Com a defini\u00e7\u00e3o de mapa tangente, podemos definir a derivada de um para f: S_1 \\to S_2 . A derivada de f em p \\in S_1 mede a varia\u00e7\u00e3o de f(p) \\in S_2 quando p se move em sua vizinhan\u00e7a. Assim, dizemos que a derivada D_pf de f em p \\in S \u00e9 um mapa D_pf : T_pS_1 \\to T_{f(p)}S_2 tal que, se w \\in T_pS_1 , existe uma curva \\alpha em S_1 tal que w = \\alpha '(t_0) . Ent\u00e3o \\gamma = f \\circ \\alpha \u00e9 uma curva em S_2 passando por f(p) em t_0 e, ent\u00e3o D_{f(p)}S_2 = \\gamma '(t_0) . Normais e orientabilidade Defini\u00e7\u00e3o: O vetor normal unit\u00e1rio a S no ponto p \u00e9 dado por N_{\\sigma} = \\frac{\\sigma_u \\times \\sigma_v}{||\\sigma_u \\times \\sigma_v||} que \u00e9 exatamente o vetor normal ao plano tangente. Note que esse vetor n\u00e3o \u00e9 independente da escolha do patch \\sigma : U \\to \\mathbb{R}^3 . Se \\tilde{\\sigma} : \\tilde{U} \\to \\mathbb{R}^3 \u00e9 outro patch regular para S em p , podemos demostrar que: Proposi\u00e7\u00e3o: Seja \\phi : \\tilde{U} \\to U um difeomorfismo. Ent\u00e3o \\tilde{\\sigma} = \\sigma \\circ \\phi : \\tilde{U} \\to \\mathbb{R}^3 \u00e9 um mapa regular. Al\u00e9m disso, \\tilde{\\sigma}_{\\tilde{u}} \\times \\tilde{\\sigma}_{\\tilde{v}} = \\operatorname{det}(J(\\phi)) \\, \\tilde{\\sigma}_u \\times \\tilde{\\sigma}_v em que \\operatorname{det}(J(\\phi)) \u00e9 o determinante do Jacobiano da transforma\u00e7\u00e3o \\phi . Portanto N_{\\tilde{\\sigma}} = \\operatorname{sign}(\\operatorname{det}(J(\\phi))) N_{\\sigma} . Orientabilidade: Dizemos que S \u00e9 superf\u00edcie orient\u00e1vel se existe um atlas A para S , de forma que, para quaisquer dois patches \\sigma_1 e \\sigma_2 em A , se \\phi \u00e9 o mapa de transi\u00e7\u00e3o entre eles, ent\u00e3o \\operatorname{det}(J(\\phi)) > 0 . Abordagem alternativa A abordagem utilizada por Ronaldo Freire \u00e9 um pouco diferente e ser\u00e1 apresentada nessa subse\u00e7\u00e3o. Defini\u00e7\u00e3o (campos): Seja uma superf\u00edcie \\mathcal{S} . Uma aplica\u00e7\u00e3o f: \\mathcal{S} \\to \\mathbb{R}^3 \u00e9 chamada de campo. Ela ser\u00e1 unit\u00e1ria quando ||f(p)|| = 1 para todo ponto da superf\u00edcie; tangente se sua imagem est\u00e1 contida no espa\u00e7o (plano) tangente; e normal se pertence ao complemento ortogonal do espa\u00e7o tangente, isto \u00e9, sua imagem \u00e9 ortogonal ao plano tangente. Quando o campo \u00e9 normal, unit\u00e1rio, e diferencial, denotamos por N : \\mathcal{S} \\to \\mathbb{R}^3 . Note que sempre podemos definir essa fun\u00e7\u00e3o associando cada ponto da superf\u00edcie ao vetor normal \u00e0 superf\u00edcie que passe no ponto. Superf\u00edcie orient\u00e1vel: Uma superf\u00edcie regular \u00e9 orient\u00e1vel quando podemos definir um campo N nessa superf\u00edcie. Esse campo define a orienta\u00e7\u00e3o da superf\u00edcie, quando existir. Uma superf\u00edcie regular n\u00e3o orient\u00e1vel \u00e9 a fita de Moebius , uma faixa de papel com uma das extremidades torcidas. Atlas coerente: Um atlas (fam\u00edlia de parametriza\u00e7\u00f5es de uma superf\u00edcie) \u00e9 coerente quando dadas duas parametriza\u00e7\u00f5es \\sigma_1 e \\sigma_2 do atlas, o mapa de transi\u00e7\u00e3o entre as parametriza\u00e7\u00f5es tem determinante positivo. Note que essa \u00e9 a defini\u00e7\u00e3o que Pressley utiliza. Podemos provar (inclusive Freire prova) que ambas s\u00e3o defini\u00e7\u00f5es equivalentes.","title":"Superf\u00edcies"},{"location":"curvas/regular-surfaces/#superficies","text":"Defini\u00e7\u00e3o: Dizemos que S \\subseteq \\mathbb{R}^n \u00e9 uma superf\u00edcie se para todo ponto p \\in S , existe um conjunto aberto U \\subset \\mathbb{R}^2 e um conjunto aberto W \\subseteq \\mathbb{R}^3 contendo p tal que S \\cap W \u00e9 homeomorfo a U . O homeomorfismo \\sigma : U \\to S \\cap W \u00e9 um patch ou parametriza\u00e7\u00e3o . A cole\u00e7\u00e3o desses homeomorfismos que cobrem S \u00e9 chamada de atlas . Dizemos que S \\cap W \u00e9 aberto em S sempre que W for aberto. Observe que para cada ponto da superf\u00edcie, olhamos para uma vizinhan\u00e7a dele (conjunto aberto que o cont\u00e9m) e essa vizinhan\u00e7a restrita \u00e0 superf\u00edcie deve ser \"parecida\" com um subconjunto do plano. Quando caminhamos no planeta, temos a sensa\u00e7\u00e3o de caminharmos num plano justamente por esse conceito. Tamb\u00e9m observe que nosso atlas pode ser formado por uma quantidade infinita de patches, mas, em geral, existe um n\u00famero m\u00ednimo.","title":"Superf\u00edcies"},{"location":"curvas/regular-surfaces/#superficies-suaves","text":"Defini\u00e7\u00e3o: Um patch \\sigma : U \\to \\mathbb{R}^3 \u00e9 regular se \u00e9 uma fun\u00e7\u00e3o suave (diferenci\u00e1vel nas tr\u00eas componentes com derivadas parciais cont\u00ednuas de todas as ordens) e os vetores \\sigma_u = \\frac{\\partial}{\\partial u} \\sigma e \\sigma_v = \\frac{\\partial}{\\partial v} \\sigma s\u00e3o linearmente independentes para todo (u,v) \\in U . Isto \u00e9, basta exigir que \\sigma_u \\times \\sigma_v \\neq 0 . Uma superf\u00edcie regular \u00e9 uma superf\u00edcie tal que p \\in S , existe um patch regular cuja imagem contenha p . Proposi\u00e7\u00e3o: Sejam U e V abertos de \\mathbb{R}^2 e \\sigma : U \\to \\mathbb{R}^3 um patch regular. Seja \\phi : V \\to U um difeomorfismo. Ent\u00e3o \\tilde{\\sigma} = \\sigma \\circ \\phi : V \\to \\mathbb{R}^3 \u00e9 um patch regular. Dizemos que \\sigma e \\tilde{\\sigma} s\u00e3o reparametriza\u00e7\u00f5es um do outro e \\phi um mapa reparametriza\u00e7\u00e3o. Esse princ\u00edpio permite que definhamos uma propriedade para superf\u00edcies regulares desde que definhamos para qualquer patch regular de forma que ser\u00e1 inalterada para outra parametriza\u00e7\u00e3o.","title":"Superf\u00edcies suaves"},{"location":"curvas/regular-surfaces/#mapas-suaves","text":"Queremos entender a no\u00e7\u00e3o de mapa suave entre duas superf\u00edcies suaves. At\u00e9 agora, definimos a suavidade entre conjuntos de \\mathbb{R}^n . Suponha que as superf\u00edcies S_1 e S_2 s\u00e3o cobertas pelos patches \\sigma_1 : U_1 \\to \\mathbb{R}^3 e \\sigma_2 : U_2 \\to \\mathbb{R}^3 , respectivamente. Dizemos que f \u00e9 suave se \\sigma_2^{-1} \\circ f \\circ \\sigma_1 : U_1 \\to U_2 \u00e9 suave. Al\u00e9m disso, se f: S_1 \\to S_2 \u00e9 um difeomorfismo e \\sigma_1 \u00e9 um patch regular S_1 , ent\u00e3o f \\circ \\sigma_1 \u00e9 um patch regular em S_2 .","title":"Mapas suaves"},{"location":"curvas/regular-surfaces/#tangentes","text":"Defini\u00e7\u00e3o: Um vetor tangente a uma superf\u00edcie S em um ponto p \\in S \u00e9 o vetor tangente em p de uma curva em S que passa por p . O espa\u00e7o tangente \u00e9 o conjunto desses vetores e \u00e9 denotado por T_pS . Seja uma curva \\alpha em S . que passa por p . Em uma vizinhan\u00e7a de p , podemos dizer que \\alpha(t) = \\sigma(u(t), v(t)) tal que \\alpha(t_0) = p . Podemos provar que u e v s\u00e3o suaves. Essa curva tem uma tangente em p denotada por \\alpha '(t_0) . Mas note que, essa curva n\u00e3o \u00e9 necessariamente a \u00fanica que passa por p . Essa defini\u00e7\u00e3o \u00e9 pouco trat\u00e1vel para enxergar o espa\u00e7o tangente. Para isso, o teorema a seguir prop\u00f5e uma caracteriza\u00e7\u00e3o mais palat\u00e1vel: Proposi\u00e7\u00e3o: Seja \\sigma : U \\to \\mathbb{R}^3 um patch da superf\u00edcie S que passa pelo ponto p em (u_0, v_0) . O espa\u00e7o tangente a S em p \u00e9 o plano gerado pelos vetores \\sigma_u(u_0, v_0) e \\sigma_v(u_0, v_0) que passa pela origem. Observe que o plano tangente \u00e9 independente da escolha do patch. Derivada de mapa suave: Com a defini\u00e7\u00e3o de mapa tangente, podemos definir a derivada de um para f: S_1 \\to S_2 . A derivada de f em p \\in S_1 mede a varia\u00e7\u00e3o de f(p) \\in S_2 quando p se move em sua vizinhan\u00e7a. Assim, dizemos que a derivada D_pf de f em p \\in S \u00e9 um mapa D_pf : T_pS_1 \\to T_{f(p)}S_2 tal que, se w \\in T_pS_1 , existe uma curva \\alpha em S_1 tal que w = \\alpha '(t_0) . Ent\u00e3o \\gamma = f \\circ \\alpha \u00e9 uma curva em S_2 passando por f(p) em t_0 e, ent\u00e3o D_{f(p)}S_2 = \\gamma '(t_0) .","title":"Tangentes"},{"location":"curvas/regular-surfaces/#normais-e-orientabilidade","text":"Defini\u00e7\u00e3o: O vetor normal unit\u00e1rio a S no ponto p \u00e9 dado por N_{\\sigma} = \\frac{\\sigma_u \\times \\sigma_v}{||\\sigma_u \\times \\sigma_v||} que \u00e9 exatamente o vetor normal ao plano tangente. Note que esse vetor n\u00e3o \u00e9 independente da escolha do patch \\sigma : U \\to \\mathbb{R}^3 . Se \\tilde{\\sigma} : \\tilde{U} \\to \\mathbb{R}^3 \u00e9 outro patch regular para S em p , podemos demostrar que: Proposi\u00e7\u00e3o: Seja \\phi : \\tilde{U} \\to U um difeomorfismo. Ent\u00e3o \\tilde{\\sigma} = \\sigma \\circ \\phi : \\tilde{U} \\to \\mathbb{R}^3 \u00e9 um mapa regular. Al\u00e9m disso, \\tilde{\\sigma}_{\\tilde{u}} \\times \\tilde{\\sigma}_{\\tilde{v}} = \\operatorname{det}(J(\\phi)) \\, \\tilde{\\sigma}_u \\times \\tilde{\\sigma}_v em que \\operatorname{det}(J(\\phi)) \u00e9 o determinante do Jacobiano da transforma\u00e7\u00e3o \\phi . Portanto N_{\\tilde{\\sigma}} = \\operatorname{sign}(\\operatorname{det}(J(\\phi))) N_{\\sigma} . Orientabilidade: Dizemos que S \u00e9 superf\u00edcie orient\u00e1vel se existe um atlas A para S , de forma que, para quaisquer dois patches \\sigma_1 e \\sigma_2 em A , se \\phi \u00e9 o mapa de transi\u00e7\u00e3o entre eles, ent\u00e3o \\operatorname{det}(J(\\phi)) > 0 .","title":"Normais e orientabilidade"},{"location":"curvas/regular-surfaces/#abordagem-alternativa","text":"A abordagem utilizada por Ronaldo Freire \u00e9 um pouco diferente e ser\u00e1 apresentada nessa subse\u00e7\u00e3o. Defini\u00e7\u00e3o (campos): Seja uma superf\u00edcie \\mathcal{S} . Uma aplica\u00e7\u00e3o f: \\mathcal{S} \\to \\mathbb{R}^3 \u00e9 chamada de campo. Ela ser\u00e1 unit\u00e1ria quando ||f(p)|| = 1 para todo ponto da superf\u00edcie; tangente se sua imagem est\u00e1 contida no espa\u00e7o (plano) tangente; e normal se pertence ao complemento ortogonal do espa\u00e7o tangente, isto \u00e9, sua imagem \u00e9 ortogonal ao plano tangente. Quando o campo \u00e9 normal, unit\u00e1rio, e diferencial, denotamos por N : \\mathcal{S} \\to \\mathbb{R}^3 . Note que sempre podemos definir essa fun\u00e7\u00e3o associando cada ponto da superf\u00edcie ao vetor normal \u00e0 superf\u00edcie que passe no ponto. Superf\u00edcie orient\u00e1vel: Uma superf\u00edcie regular \u00e9 orient\u00e1vel quando podemos definir um campo N nessa superf\u00edcie. Esse campo define a orienta\u00e7\u00e3o da superf\u00edcie, quando existir. Uma superf\u00edcie regular n\u00e3o orient\u00e1vel \u00e9 a fita de Moebius , uma faixa de papel com uma das extremidades torcidas. Atlas coerente: Um atlas (fam\u00edlia de parametriza\u00e7\u00f5es de uma superf\u00edcie) \u00e9 coerente quando dadas duas parametriza\u00e7\u00f5es \\sigma_1 e \\sigma_2 do atlas, o mapa de transi\u00e7\u00e3o entre as parametriza\u00e7\u00f5es tem determinante positivo. Note que essa \u00e9 a defini\u00e7\u00e3o que Pressley utiliza. Podemos provar (inclusive Freire prova) que ambas s\u00e3o defini\u00e7\u00f5es equivalentes.","title":"Abordagem alternativa"},{"location":"curvas/space-curves/","text":"Curvas no espa\u00e7o Vimos que as curvas no plano s\u00e3o essencialmente definidas por sua curvatura. Por\u00e9m, isso n\u00e3o \u00e9 verdade no espa\u00e7o. Considere, por exemplo, a h\u00e9lice circular \\alpha(t) = \\left(\\frac{1}{2}\\cos(t), \\frac{1}{2}\\sin(t), \\frac{1}{2}t\\right) Ela est\u00e1 parametrizada pelo comprimento de arco. Ent\u00e3o, se formos calcular a curvatura (usando a mesma no\u00e7\u00e3o de curvas planas), teremos que \\kappa_{\\alpha}(t) = ||\\alpha''(t)|| = 1 Entretanto, a h\u00e9lice n\u00e3o \u00e9 uma circunfer\u00eancia de raio 1! Para isso, vamos precisar introduzir o conceito de tor\u00e7\u00e3o . Veja tamb\u00e9m que n\u00e3o \u00e9 poss\u00edvel definir uma fun\u00e7\u00e3o \u00e2ngulo! Ent\u00e3o, curvatura com sinal \u00e9 um conceito vago no espa\u00e7o. Assim, para lembrar, definimos curvatura para uma curva parametrizada pelo comprimento de arco no espa\u00e7o como \\kappa_{\\alpha}(t) = ||\\alpha''(t)|| Se a curva \\alpha \u00e9 regular qualquer, seja \\beta = \\alpha \\circ h uma reparametriza\u00e7\u00e3o pelo comprimento de arco. Definimos a curvatura de \\alpha como \\kappa_{\\alpha}(t) = \\kappa_{\\beta}(h^{-1}(t)) Em particular, teremos que \\kappa_{\\alpha}(t) = \\frac{||\\alpha'(t) \\times \\alpha''(t)||}{||\\alpha'(t)||^3} Dizemos que uma curva \\alpha \u00e9 2-regular (ou regular de sgunda ordem) quando os vetores \\alpha'(t) e \\alpha''(t) s\u00e3o linearmente independentes para todo t . Se a curva for parametrizada pelo comprimento de arco, isso equivale a dizer que \\alpha'' \\neq 0 (verifique!). Se a curva for regular, isso \u00e9 equivalente a provar que \\kappa_{\\alpha}(t) > 0 (verifique!). Triedo de Frenet J\u00e1 vimos que n\u00e3o faz sentido rotacionar o vetor tangente. Por isso, definimos o vetor normal principal de uma curva \\alpha parametrizada pelo comprimento de arco como N(s) = \\frac{1}{\\kappa_{\\alpha}(s)}(\\dot{T}(s)) onde T(s) = \\alpha'(s) . Veja que ambos os vetores s\u00e3o unit\u00e1rios e ortogonais (quando a curva \u00e9 parametrizada pelo comprimento de arco, a primeira e segunda derivadas s\u00e3o ortogonais). Definimos, ent\u00e3o o vetor binormal como B(s) = T(s) \\times N(s) De fato B \u00e9 ortogonal a T e a N e, al\u00e9m disso, ||B(s)|| = ||T(s)||||N(s)|| - 2\\langle T(s), N(s) \\rangle = 1 Definimos, portanto, uma base ortonormal \\{T(s), N(s), B(s)\\} para \\mathbb{R}^3 . Tor\u00e7\u00e3o Observe que \\dot{B}(s) = \\dot{T}(s) \\times N(s) + T(s) \\times \\dot{N}(s) Como \\dot{T}(s) \\parallel N(s) m, ent\u00e3o \\dot{B}(s) = T(s) \\times \\dot{N}(s) Como B(s) \u00e9 um vetor unit\u00e1rio, B(s) \\perp \\dot{B}(s) (veja que isso acontece com qualquer fun\u00e7\u00e3o vetorial unit\u00e1ria). Al\u00e9m disso \\dot{B}(s) \\perp T(s) pela equa\u00e7\u00e3o acima. Pelo Triedro de Frenet, \\dot{B}(s) \\perp N(s) \\implies \\dot{B}(s) = \\tau(s)N(s) chamamos \\tau de tor\u00e7\u00e3o. Alguns livros, \\tau tem o sinal oposto (mas isso \u00e9 s\u00f3 quest\u00e3o de conven\u00e7\u00e3o e n\u00e3o tr\u00e1s problemas te\u00f3ricos. Mais uma vez, se a curva \\alpha \u00e9 uma curva regular, tomamos uma reparametriza\u00e7\u00e3o pelo comprimento de arco \\beta = \\alpha \\circ h e \\tau_{\\alpha}(s) = \\tau_{\\beta}(h^{-1}(s)) Em especial \\tau = \\frac{\\langle \\dot{\\alpha} \\times \\ddot{\\alpha}, \\dddot{\\alpha} \\rangle}{||\\dot{\\alpha}\\times\\ddot{\\alpha}||^2} Equa\u00e7\u00f5es de Frenet J\u00e1 sabemos que \\dot{T}(s) = \\kappa_{\\alpha}(s)N(s) e \\dot{B}(s) = \\tau_{\\alpha}(s)N(s) . Agora, vamos calcular \\dot{N}(s) . Sabemos que B = T \\times N \\implies N = B \\times T \\implies \\dot{N}(s) = \\dot{B}(s) \\times T(s) + B(s) \\times \\dot{T}(s) Portanto, \\dot{N}(s) = \\tau_{\\alpha} N(s) \\times T(s) + \\kappa_{\\alpha} B(s) \\times N(s) = -\\tau_{\\alpha} B(s) - \\kappa_{\\alpha} T(s) Chegamos ent\u00e3o que, se \\alpha \u00e9 uma curva parametrizada pelo comprimento de arco regular de ordem 2, ent\u00e3o \\dot{T} = \\kappa N \\dot{N} = - \\kappa T - \\tau B \\dot{B} = \\tau N Observe que se x(s) = (T(s), N(s), B(s)) \\in \\mathbb{R}^9, \\dot{x}(s) = Ax(s) , onde A = \\begin{bmatrix} 0_{3\\times 3} & \\kappa I_{3\\times 3} & 0_{3\\times 3} \\\\ -\\kappa I_{3\\times 3} & 0_{3\\times 3} & -\\tau I_{3\\times 3} \\\\ 0_{3\\times 3} & \\tau I_{3\\times 3} & 0_{3\\times 3} \\end{bmatrix} Para curvas regulares quaisquer, a defini\u00e7\u00e3o se d\u00e1 usando a inversa do comprimento de arco, como fizemos com a curvatura e a tor\u00e7\u00e3o. Planos Plano Osculador: Determinado pelos vetores tangente e normal. O vetor binormal \u00e9 normal ao plano osculador e, portanto, podemos escrever sua equa\u00e7\u00e3o como (x - \\alpha(s))\\cdot B(s) = 0 . Plano Normal: Plano determinado pelos vetores normal e binormal. Plano Retificante: Plano determinando pelos vetores tangente e normal. Alguns simples desenhos podem ser vistos nesse site . Consequ\u00eancias Curva plana e tor\u00e7\u00e3o nula Seja \\alpha : I \\to \\mathbb{R}^3 uma curva 2-regular, parametrizada por comprimento de arco. Ent\u00e3o, \\alpha \u00e9 plana se, e somente se, sua tor\u00e7\u00e3o \\tau_{\\alpha} \u00e9 identicamente nula. A demonstra\u00e7\u00e3o desse fato se divide na ida e volta. Supondo que a curva seja plana, devemos inserir a curva em um plano. Assim, para cada s \\in I, \\langle \\alpha(s) - p, v \\rangle = 0 para algum p nesse plano. Em particular, obtermos que v = \\pm B_{\\alpha}(s) , pois obteremos, derivando, que a tangente e a normal s\u00e3o ortogonais a v . Nesse caso B'_{\\alpha}(s) = 0 , pois esse vetor ser\u00e1 constante. A rec\u00edproca usa o fato que B_{\\alpha}(s) ser\u00e1 constante e quer se provar que f(s) = \\langle \\alpha(s) - \\alpha(s_0), B_{\\alpha}(s) \\rangle \\equiv 0 . Circunfer\u00eancia e curvatura constante Seja \\alpha uma curva parametrizada pelo comprimento de arco em \\mathbb{R}^3 com consntate curvatura e tor\u00e7\u00e3o nula. Ent\u00e3o \\alpha \u00e9 parametriza\u00e7\u00e3o de (parte de) um c\u00edrculo. Sabemos pelo item anterior que estaremos em um plano. A ideia \u00e9 provar que \\alpha - \\frac{1}{\\kappa_{\\alpha}}n \u00e9 um vetor constante para provarmos que a curva est\u00e1 contida em uma esfera. Assim, basta provar que a curva est\u00e1 contida na intersec\u00e7\u00e3o de uma esfera e um plano. Teorema Fundamental da Teoria Local das Curvas Espaciais Sejam \\alpha(s) e \\gamma(s) duas curvas parametrizadas pelo comprimento de arco em \\mathbb{R}^3 com a mesma curvatura \\kappa(s) > 0 e a mesma tor\u00e7\u00e3o \\tau(s), \\forall s . Ent\u00e3o, existe um movimento r\u00edgido direto M tal que \\alpha(s) = M(\\gamma(s)), \\forall s . Al\u00e9m disso, se \\kappa e \\tau s\u00e3o fun\u00e7\u00f5es suaves, tal que \\kappa > 0 em toda parte, existe uma curva parametrizada pelo comprimento de arco em \\mathbb{R}^3 cuja curvatura \u00e9 \\kappa e cuja tor\u00e7\u00e3o \u00e9 \\tau . A demonstra\u00e7\u00e3o desse teorema super importante pode ser encontrada na p\u00e1gina 52 do livro do Pressley de Introdu\u00e7\u00e3o \u00e0 Geometria Diferencial. \u00c9 uma aplica\u00e7\u00e3o do Teorema da Exist\u00eancia e Unicidade de Equa\u00e7\u00f5es Diferenciais nas Equa\u00e7\u00f5es de Frenet-Sarret.","title":"Curvas no espa\u00e7o"},{"location":"curvas/space-curves/#curvas-no-espaco","text":"Vimos que as curvas no plano s\u00e3o essencialmente definidas por sua curvatura. Por\u00e9m, isso n\u00e3o \u00e9 verdade no espa\u00e7o. Considere, por exemplo, a h\u00e9lice circular \\alpha(t) = \\left(\\frac{1}{2}\\cos(t), \\frac{1}{2}\\sin(t), \\frac{1}{2}t\\right) Ela est\u00e1 parametrizada pelo comprimento de arco. Ent\u00e3o, se formos calcular a curvatura (usando a mesma no\u00e7\u00e3o de curvas planas), teremos que \\kappa_{\\alpha}(t) = ||\\alpha''(t)|| = 1 Entretanto, a h\u00e9lice n\u00e3o \u00e9 uma circunfer\u00eancia de raio 1! Para isso, vamos precisar introduzir o conceito de tor\u00e7\u00e3o . Veja tamb\u00e9m que n\u00e3o \u00e9 poss\u00edvel definir uma fun\u00e7\u00e3o \u00e2ngulo! Ent\u00e3o, curvatura com sinal \u00e9 um conceito vago no espa\u00e7o. Assim, para lembrar, definimos curvatura para uma curva parametrizada pelo comprimento de arco no espa\u00e7o como \\kappa_{\\alpha}(t) = ||\\alpha''(t)|| Se a curva \\alpha \u00e9 regular qualquer, seja \\beta = \\alpha \\circ h uma reparametriza\u00e7\u00e3o pelo comprimento de arco. Definimos a curvatura de \\alpha como \\kappa_{\\alpha}(t) = \\kappa_{\\beta}(h^{-1}(t)) Em particular, teremos que \\kappa_{\\alpha}(t) = \\frac{||\\alpha'(t) \\times \\alpha''(t)||}{||\\alpha'(t)||^3} Dizemos que uma curva \\alpha \u00e9 2-regular (ou regular de sgunda ordem) quando os vetores \\alpha'(t) e \\alpha''(t) s\u00e3o linearmente independentes para todo t . Se a curva for parametrizada pelo comprimento de arco, isso equivale a dizer que \\alpha'' \\neq 0 (verifique!). Se a curva for regular, isso \u00e9 equivalente a provar que \\kappa_{\\alpha}(t) > 0 (verifique!).","title":"Curvas no espa\u00e7o"},{"location":"curvas/space-curves/#triedo-de-frenet","text":"J\u00e1 vimos que n\u00e3o faz sentido rotacionar o vetor tangente. Por isso, definimos o vetor normal principal de uma curva \\alpha parametrizada pelo comprimento de arco como N(s) = \\frac{1}{\\kappa_{\\alpha}(s)}(\\dot{T}(s)) onde T(s) = \\alpha'(s) . Veja que ambos os vetores s\u00e3o unit\u00e1rios e ortogonais (quando a curva \u00e9 parametrizada pelo comprimento de arco, a primeira e segunda derivadas s\u00e3o ortogonais). Definimos, ent\u00e3o o vetor binormal como B(s) = T(s) \\times N(s) De fato B \u00e9 ortogonal a T e a N e, al\u00e9m disso, ||B(s)|| = ||T(s)||||N(s)|| - 2\\langle T(s), N(s) \\rangle = 1 Definimos, portanto, uma base ortonormal \\{T(s), N(s), B(s)\\} para \\mathbb{R}^3 .","title":"Triedo de Frenet"},{"location":"curvas/space-curves/#torcao","text":"Observe que \\dot{B}(s) = \\dot{T}(s) \\times N(s) + T(s) \\times \\dot{N}(s) Como \\dot{T}(s) \\parallel N(s) m, ent\u00e3o \\dot{B}(s) = T(s) \\times \\dot{N}(s) Como B(s) \u00e9 um vetor unit\u00e1rio, B(s) \\perp \\dot{B}(s) (veja que isso acontece com qualquer fun\u00e7\u00e3o vetorial unit\u00e1ria). Al\u00e9m disso \\dot{B}(s) \\perp T(s) pela equa\u00e7\u00e3o acima. Pelo Triedro de Frenet, \\dot{B}(s) \\perp N(s) \\implies \\dot{B}(s) = \\tau(s)N(s) chamamos \\tau de tor\u00e7\u00e3o. Alguns livros, \\tau tem o sinal oposto (mas isso \u00e9 s\u00f3 quest\u00e3o de conven\u00e7\u00e3o e n\u00e3o tr\u00e1s problemas te\u00f3ricos. Mais uma vez, se a curva \\alpha \u00e9 uma curva regular, tomamos uma reparametriza\u00e7\u00e3o pelo comprimento de arco \\beta = \\alpha \\circ h e \\tau_{\\alpha}(s) = \\tau_{\\beta}(h^{-1}(s)) Em especial \\tau = \\frac{\\langle \\dot{\\alpha} \\times \\ddot{\\alpha}, \\dddot{\\alpha} \\rangle}{||\\dot{\\alpha}\\times\\ddot{\\alpha}||^2}","title":"Tor\u00e7\u00e3o"},{"location":"curvas/space-curves/#equacoes-de-frenet","text":"J\u00e1 sabemos que \\dot{T}(s) = \\kappa_{\\alpha}(s)N(s) e \\dot{B}(s) = \\tau_{\\alpha}(s)N(s) . Agora, vamos calcular \\dot{N}(s) . Sabemos que B = T \\times N \\implies N = B \\times T \\implies \\dot{N}(s) = \\dot{B}(s) \\times T(s) + B(s) \\times \\dot{T}(s) Portanto, \\dot{N}(s) = \\tau_{\\alpha} N(s) \\times T(s) + \\kappa_{\\alpha} B(s) \\times N(s) = -\\tau_{\\alpha} B(s) - \\kappa_{\\alpha} T(s) Chegamos ent\u00e3o que, se \\alpha \u00e9 uma curva parametrizada pelo comprimento de arco regular de ordem 2, ent\u00e3o \\dot{T} = \\kappa N \\dot{N} = - \\kappa T - \\tau B \\dot{B} = \\tau N Observe que se x(s) = (T(s), N(s), B(s)) \\in \\mathbb{R}^9, \\dot{x}(s) = Ax(s) , onde A = \\begin{bmatrix} 0_{3\\times 3} & \\kappa I_{3\\times 3} & 0_{3\\times 3} \\\\ -\\kappa I_{3\\times 3} & 0_{3\\times 3} & -\\tau I_{3\\times 3} \\\\ 0_{3\\times 3} & \\tau I_{3\\times 3} & 0_{3\\times 3} \\end{bmatrix} Para curvas regulares quaisquer, a defini\u00e7\u00e3o se d\u00e1 usando a inversa do comprimento de arco, como fizemos com a curvatura e a tor\u00e7\u00e3o.","title":"Equa\u00e7\u00f5es de Frenet"},{"location":"curvas/space-curves/#planos","text":"Plano Osculador: Determinado pelos vetores tangente e normal. O vetor binormal \u00e9 normal ao plano osculador e, portanto, podemos escrever sua equa\u00e7\u00e3o como (x - \\alpha(s))\\cdot B(s) = 0 . Plano Normal: Plano determinado pelos vetores normal e binormal. Plano Retificante: Plano determinando pelos vetores tangente e normal. Alguns simples desenhos podem ser vistos nesse site .","title":"Planos"},{"location":"curvas/space-curves/#consequencias","text":"","title":"Consequ\u00eancias"},{"location":"curvas/space-curves/#curva-plana-e-torcao-nula","text":"Seja \\alpha : I \\to \\mathbb{R}^3 uma curva 2-regular, parametrizada por comprimento de arco. Ent\u00e3o, \\alpha \u00e9 plana se, e somente se, sua tor\u00e7\u00e3o \\tau_{\\alpha} \u00e9 identicamente nula. A demonstra\u00e7\u00e3o desse fato se divide na ida e volta. Supondo que a curva seja plana, devemos inserir a curva em um plano. Assim, para cada s \\in I, \\langle \\alpha(s) - p, v \\rangle = 0 para algum p nesse plano. Em particular, obtermos que v = \\pm B_{\\alpha}(s) , pois obteremos, derivando, que a tangente e a normal s\u00e3o ortogonais a v . Nesse caso B'_{\\alpha}(s) = 0 , pois esse vetor ser\u00e1 constante. A rec\u00edproca usa o fato que B_{\\alpha}(s) ser\u00e1 constante e quer se provar que f(s) = \\langle \\alpha(s) - \\alpha(s_0), B_{\\alpha}(s) \\rangle \\equiv 0 .","title":"Curva plana e tor\u00e7\u00e3o nula"},{"location":"curvas/space-curves/#circunferencia-e-curvatura-constante","text":"Seja \\alpha uma curva parametrizada pelo comprimento de arco em \\mathbb{R}^3 com consntate curvatura e tor\u00e7\u00e3o nula. Ent\u00e3o \\alpha \u00e9 parametriza\u00e7\u00e3o de (parte de) um c\u00edrculo. Sabemos pelo item anterior que estaremos em um plano. A ideia \u00e9 provar que \\alpha - \\frac{1}{\\kappa_{\\alpha}}n \u00e9 um vetor constante para provarmos que a curva est\u00e1 contida em uma esfera. Assim, basta provar que a curva est\u00e1 contida na intersec\u00e7\u00e3o de uma esfera e um plano.","title":"Circunfer\u00eancia e curvatura constante"},{"location":"curvas/space-curves/#teorema-fundamental-da-teoria-local-das-curvas-espaciais","text":"Sejam \\alpha(s) e \\gamma(s) duas curvas parametrizadas pelo comprimento de arco em \\mathbb{R}^3 com a mesma curvatura \\kappa(s) > 0 e a mesma tor\u00e7\u00e3o \\tau(s), \\forall s . Ent\u00e3o, existe um movimento r\u00edgido direto M tal que \\alpha(s) = M(\\gamma(s)), \\forall s . Al\u00e9m disso, se \\kappa e \\tau s\u00e3o fun\u00e7\u00f5es suaves, tal que \\kappa > 0 em toda parte, existe uma curva parametrizada pelo comprimento de arco em \\mathbb{R}^3 cuja curvatura \u00e9 \\kappa e cuja tor\u00e7\u00e3o \u00e9 \\tau . A demonstra\u00e7\u00e3o desse teorema super importante pode ser encontrada na p\u00e1gina 52 do livro do Pressley de Introdu\u00e7\u00e3o \u00e0 Geometria Diferencial. \u00c9 uma aplica\u00e7\u00e3o do Teorema da Exist\u00eancia e Unicidade de Equa\u00e7\u00f5es Diferenciais nas Equa\u00e7\u00f5es de Frenet-Sarret.","title":"Teorema Fundamental da Teoria Local das Curvas Espaciais"},{"location":"curvas/surface-curvature/","text":"Curvaturas de Superf\u00edcies Curvaturas Gaussiana e m\u00e9dia Defini\u00e7\u00e3o: Seja \\mathcal{W} o mapa de Weingarten de uma superf\u00edcie orientada \\mathcal{S} e p \\in \\mathcal{S} . A curvatura Gaussiana K e a curvatura m\u00e9dia H de \\mathcal{S} em p s\u00e3o K = \\det(\\mathcal{W}), \\; \\; H = \\frac{1}{2}\\operatorname{tra\u00e7o}(\\mathcal{W}). Defina, considerando a primeira e segunda formas fundamentais, \\mathcal{F}_I = \\begin{pmatrix} E & F \\\\ F & G \\end{pmatrix}, \\; \\; \\mathcal{F}_{II} = \\begin{pmatrix} e & f \\\\ f & g \\end{pmatrix}. Proposi\u00e7\u00e3o: Seja \\sigma uma parametriza\u00e7\u00e3o da superf\u00edcie orientada \\mathcal{S} . Ent\u00e3o a matriz do mapa de Weingarten com respeito a base \\{\\sigma_u, \\sigma_v\\} de T_pS \u00e9 \\mathcal{F}_I^{-1}\\mathcal{F}_{II} . Corol\u00e1rio: H = \\frac{eG - 2fF + gE}{2(EF - F^2)}, \\; \\; K = \\frac{eg - f^2}{EG - F^2} Curvaturas principais Seja p \\in \\mathcal{S} . Existem \\kappa_1, \\kappa_2 e uma base \\{u, v\\} do plano tangente T_p\\mathcal{S} tal que \\mathcal{W}(u) = \\kappa_1u, \\; \\; \\mathcal{W}(v) = \\kappa_2v, em outras palavras, o mapa de Weingarten possui autovalores e autovetores. As curvaturas principais de \\mathcal{S} s\u00e3o os autovalores do mapa, e u, v s\u00e3o os vetores principais correspondentes. Pontos umb\u00edlicos (ou umbilicais): Pontos em que \\kappa_1 = \\kappa_2 . Em particular, p \u00e9 umb\u00edlico se, e somente se, o mapa de Weinngarten \u00e9 um mapa identidade multiplicado por um escalar. N Proposi\u00e7\u00e3o: As curvaturas principais em um ponto da superf\u00edcie s\u00e3o os valores m\u00e1ximo e m\u00ednimo da curvatura normal de todas as curvas da superf\u00edcie que passam pelo ponto. A demonstra\u00e7\u00e3o dessa proposi\u00e7\u00e3o utiliza o Teorema de Euler que afirma que \\kappa_n = \\kappa_1 \\cos^2(\\theta) + \\kappa_2 \\sin^2(\\theta), onde \\theta \u00e9 o \u00e2ngulo orientado \\hat{u\\dot{\\gamma}} . Proposi\u00e7\u00e3o: Seja \\mathcal{S} uma superf\u00edcie conectada em que todo ponto \u00e9 umb\u00edlico. Ent\u00e3o \\mathcal{S} \u00e9 um conjunto aberto da esfera ou do plano.","title":"Curvaturas de Superf\u00edcies"},{"location":"curvas/surface-curvature/#curvaturas-de-superficies","text":"","title":"Curvaturas de Superf\u00edcies"},{"location":"curvas/surface-curvature/#curvaturas-gaussiana-e-media","text":"Defini\u00e7\u00e3o: Seja \\mathcal{W} o mapa de Weingarten de uma superf\u00edcie orientada \\mathcal{S} e p \\in \\mathcal{S} . A curvatura Gaussiana K e a curvatura m\u00e9dia H de \\mathcal{S} em p s\u00e3o K = \\det(\\mathcal{W}), \\; \\; H = \\frac{1}{2}\\operatorname{tra\u00e7o}(\\mathcal{W}). Defina, considerando a primeira e segunda formas fundamentais, \\mathcal{F}_I = \\begin{pmatrix} E & F \\\\ F & G \\end{pmatrix}, \\; \\; \\mathcal{F}_{II} = \\begin{pmatrix} e & f \\\\ f & g \\end{pmatrix}. Proposi\u00e7\u00e3o: Seja \\sigma uma parametriza\u00e7\u00e3o da superf\u00edcie orientada \\mathcal{S} . Ent\u00e3o a matriz do mapa de Weingarten com respeito a base \\{\\sigma_u, \\sigma_v\\} de T_pS \u00e9 \\mathcal{F}_I^{-1}\\mathcal{F}_{II} . Corol\u00e1rio: H = \\frac{eG - 2fF + gE}{2(EF - F^2)}, \\; \\; K = \\frac{eg - f^2}{EG - F^2}","title":"Curvaturas Gaussiana e m\u00e9dia"},{"location":"curvas/surface-curvature/#curvaturas-principais","text":"Seja p \\in \\mathcal{S} . Existem \\kappa_1, \\kappa_2 e uma base \\{u, v\\} do plano tangente T_p\\mathcal{S} tal que \\mathcal{W}(u) = \\kappa_1u, \\; \\; \\mathcal{W}(v) = \\kappa_2v, em outras palavras, o mapa de Weingarten possui autovalores e autovetores. As curvaturas principais de \\mathcal{S} s\u00e3o os autovalores do mapa, e u, v s\u00e3o os vetores principais correspondentes. Pontos umb\u00edlicos (ou umbilicais): Pontos em que \\kappa_1 = \\kappa_2 . Em particular, p \u00e9 umb\u00edlico se, e somente se, o mapa de Weinngarten \u00e9 um mapa identidade multiplicado por um escalar. N Proposi\u00e7\u00e3o: As curvaturas principais em um ponto da superf\u00edcie s\u00e3o os valores m\u00e1ximo e m\u00ednimo da curvatura normal de todas as curvas da superf\u00edcie que passam pelo ponto. A demonstra\u00e7\u00e3o dessa proposi\u00e7\u00e3o utiliza o Teorema de Euler que afirma que \\kappa_n = \\kappa_1 \\cos^2(\\theta) + \\kappa_2 \\sin^2(\\theta), onde \\theta \u00e9 o \u00e2ngulo orientado \\hat{u\\dot{\\gamma}} . Proposi\u00e7\u00e3o: Seja \\mathcal{S} uma superf\u00edcie conectada em que todo ponto \u00e9 umb\u00edlico. Ent\u00e3o \\mathcal{S} \u00e9 um conjunto aberto da esfera ou do plano.","title":"Curvaturas principais"},{"location":"curvas/curvature/curvature/","text":"Curvatura de uma curva import sympy as sp import numpy as np from scipy.integrate import solve_ivp import matplotlib.pyplot as plt Curvatura Seja \\gamma uma curva parametrizada pelo comprimeto de arco. Definimos curvatura como a fun\u00e7\u00e3o \\kappa(t) = ||\\ddot{\\gamma}(t)|| . Essa defini\u00e7\u00e3o \u00e9 consistente com o que esper\u00e1vamos de uma reta (curvatura nula) e de um c\u00edrculo (curvatura constante). Al\u00e9m disso se \\gamma \u00e9 uma curva regular qualquer, ela tem uma reparametriza\u00e7\u00e3o pelo comprimento de arco. Portanto, podemos definir a sua curvatura como sendo a curvatura de sua reparametriza\u00e7\u00e3o pelo comprimento de arco. Isto \u00e9, seja \\hat{\\gamma} uma reparametriza\u00e7\u00e3o pelo comprimento de arco de \\gamma com curvatura \\kappa . Ent\u00e3o a curvatura de \\gamma ser\u00e1 \\kappa . Uma quest\u00e3o que se levanta \u00e9: e se houver outra reparametriza\u00e7\u00e3o pelo comprimento de arco para \\gamma ? Para isso, precisamos mostrar que a curvatura \u00e9 invariante (n\u00e3o muda) segundo a reparametriza\u00e7\u00e3o. Isso n\u00e3o \u00e9 dific\u00edl de ver pois as tangentes das reparametriza\u00e7\u00f5es t\u00eam mesmo tamanho e, possivelmente, diferentes sinal. Curvatura de uma curva regular Seja \\gamma(t) uma curva em \\mathbb{R}^3 , ent\u00e3o sua curvatura \u00e9 dada pela express\u00e3o \\kappa = \\frac{||\\ddot{\\gamma} \\times \\dot{\\gamma}||}{||\\dot{\\gamma}||^3} Observe que para curvas no plano essa express\u00e3o pode tamb\u00e9m ser utilizada, \\kappa = \\frac{|\\ddot{\\gamma}_1\\dot{\\gamma}_2 - \\ddot{\\gamma}_2\\dot{\\gamma}_1|}{||\\dot{\\gamma}||^3} Curvatura com sinal Definimos a normal unit\u00e1ria com sinal n(s) o vetor unit\u00e1rio que rotaciona o vetor tangente no sentido anti-hor\u00e1rio em \\pi/2 . Em particular, \\ddot{\\gamma}(s) e \\dot{\\gamma}(s) s\u00e3o perpendiculares (pois a curva \u00e9 parametrizada pelo comprimento de arco) e, portanto, \u00e9 paralelo a n(s) , e assim \\ddot{\\gamma}(s) = \\kappa_s(s) n(s) Chamamos \\kappa_s de curvatura com sinal . Em particular, \\kappa = |\\kappa_s| . Fun\u00e7\u00e3o \u00c2ngulo Dada uma curva diferenci\u00e1vel \\gamma: I \\to \\mathcal{S}^1 , onde \\mathcal{S}^1 \u00e9 o c\u00edrculo centrado na origem, dizemos que \\theta : I \\to \\mathbb{R} \u00e9 fun\u00e7\u00e3o \u00e2ngulo de \\gamma quando \\gamma(s) = (\\cos(\\theta(s)), \\sin(\\theta(s)), \\forall s \\in I Observe que nessa defini\u00e7\u00e3o, a imagem de \\gamma \u00e9 um subconjunto de \\mathcal{S}^1 , como se fosse um arco. Por exemplo, \\gamma(s) = (\\cos(2s), \\sin(2s)) \\implies \\theta(s) = 2s . Considere o operador que rotaciona no sentido anti-hor\u00e1rio em 90\u00b0 um vetor. Podemos descrev\u00ea-lo em forma matricial como J = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix} Defina o determinante entre dois vetores como det(v,w) = \\langle Jv , w \\rangle que \u00e9 o produto interno do vetor v rotacionado e w . Diferenciabilidade Seja \\gamma : I \\to \\mathcal{S}^1 uma curva diferenci\u00e1vel. Ent\u00e3o \\gamma admite uma fun\u00e7\u00e3o \u00e2ngulo \\theta diferenci\u00e1vel. Al\u00e9m disso, se \\hat{\\theta} \u00e9 fun\u00e7\u00e3o \u00e2ngulo diferenci\u00e1vel de \\gamma , ela difere de \\theta por uma constante. Note que supondo a exist\u00eancia dessa fun\u00e7\u00e3o diferenci\u00e1vel, temos que, por aplica\u00e7\u00e3o da Regra da Cadeia, \\gamma '(s) = \\theta '(s)(-\\sin(\\theta(s)), \\cos(\\theta(s))) = \\theta '(s) J\\gamma(s) Portanto, aplicando o produto interno em ambos os lados, observamos que \\theta '(s) = det(\\gamma(s), \\gamma '(s)) Assim, a demonstra\u00e7\u00e3o se d\u00e1 defininido \\theta com essa derivada (Teorema Fundamental do C\u00e1lculo). Agora seja \\alpha uma curva regular, sem perda de generalidade, parametrizada pelo comprimento de arco. Seja t(s) = \\alpha '(s) . Como ||t(s)|| = 1 , pela proposi\u00e7\u00e3o anterior, existe uma fun\u00e7\u00e3o \u00e2ngulo diferenci\u00e1vel \\theta de forma que definimos a curvatura de \\alpha como k(s) = \\theta '(s) = det(t(s), t'(s)) = det(\\alpha'(s), \\alpha''(s)) Observa\u00e7\u00e3o 1: Se \\alpha \u00e9 regular, sua curvatura \u00e9 \\kappa(s) = \\frac{det(\\alpha'(s), \\alpha''(s))}{||\\alpha '(s)||^3} Observa\u00e7\u00e3o 2: Estamos rotacionando o vetor tangente, obtendo o que chamamos de vetor normal unit\u00e1rio e fazendo o produto interno com a acelera\u00e7\u00e3o da curva, o que coincide com a defini\u00e7\u00e3o pr\u00e9via! Exemplo: Considere a parametriza\u00e7\u00e3o do c\u00edrculo \\alpha(s) = p + r(\\cos(s/r), \\sin(s/r)), s \\in \\mathbb{R} . Assim \\alpha'(s) = (-\\sin(s/r), \\cos(s/r)) \\alpha''(s) = -\\frac{1}{r}(\\cos(s/r), \\sin(s/r)) = \\frac{1}{r}J\\alpha '(s) \\kappa(s) = \\langle J\\alpha '(s), \\alpha ''(s) \\rangle = \\frac{1}{r} Exemplo 2: Vamos usar python para calcular a curvatura da espiral equiangular z(t) = e^{(a + i)t} onde a \u00e9 uma constante e i^2 = -1 . # Definimos as vari\u00e1veis t = sp . symbols ( 't' , real = True ) a = sp . symbols ( 'a' , real = True , constant = True ) # Definimos a fun\u00e7\u00e3o z z = sp . exp (( a + sp . I ) * t ) # Derivando zt = sp . diff ( z , t ) ztt = sp . diff ( zt , t ) # Rotaciona zt Jzt = zt * sp . exp ( sp . I * sp . pi / 2 ) Observe que nossa curva est\u00e1 definida no plano complexo (isomorfo ao plano real). Para calcular a curvatura de uma curva, rotacionamos o vetor e fazemos o produto dot entre esses n\u00fameros como produto escalar de dois vetores. Nesse caso, teremos que \\langle z_1, z_2 \\rangle = Re(z_1\\cdot\\bar{z}_2) . k = sp . re ( Jzt * sp . conjugate ( ztt )) / sp . Abs ( zt ) ** 3 k \\displaystyle \\frac{\\left(2 a^{2} e^{2 a t} - \\left(a^{2} - 1\\right) e^{2 a t}\\right) e^{- 3 a t}}{\\left(a^{2} + 1\\right)^{\\frac{3}{2}}} Precisamos indicar para o programa que queremos uma resposta fatorizada k = sp . factor ( k ) k \\displaystyle \\frac{e^{- a t}}{\\sqrt{a^{2} + 1}} Assim, essa \u00e9 a curvatura da espiral equiangular. Movimento R\u00edgido Isometria: Uma aplica\u00e7\u00e3o F: \\mathbb{R}^n \\to \\mathbb{R}^n que preserva dist\u00e2ncia, isto \u00e9, ||x - y|| = ||F(x) - F(y)||, \\forall x, y \\in \\mathbb{R}^n . Diremos uma movimento r\u00edgido \u00e9 uma isometria (a rigidez em mudar dist\u00e2ncias). Transla\u00e7\u00e3o: Uma aplica\u00e7\u00e3o F: \\mathbb{R}^n \\to \\mathbb{R}^n do tipo v \\mapsto F(v) := v + a , para algum a \\in \\mathbb{R}^n fixo. Transforma\u00e7\u00e3o Ortogonal: Uma transforma\u00e7\u00e3o linear que presetva o produto interno, isto \u00e9, \\langle u, v \\rangle = \\langle T(u), T(v) \\rangle . A matriz associada a essa transforma\u00e7\u00e3o \u00e9 ortogonal. Dizemos que o movimento \u00e9 direto se det(P) = 1 e oposto ou inverso quando det(P) = -1 . Teorema Seja P_{n\\times n} uma matriz ortogonal e a \\in \\mathbb{R}^n . Ent\u00e3o F: \\mathbb{R}^n \\to \\mathbb{R}^n definido como F(v) = Pv + a \u00e9 uma isometria. Reciprocamente, toda isometria pode ser escrito nessa forma. Esse teorema permite uma caracteriza\u00e7\u00e3o simples de um movimento r\u00edgido. Invari\u00e2ncia da curvatura Sejam \\Phi = A + p_0 um movimento r\u00edgido direto de R^2 e \\alpha : I \\to \\mathbb{R}^2 uma curva regular parametrizada por comprimento de arco. Ent\u00e3o, \\beta = \\Phi \\circ \\alpha : I \\to \\mathbb{R}^2 \u00e9 uma curva regular de \\mathbb{R}^2 , parametrizada por comprimento de arco, cuja fun\u00e7\u00e3o curvatura coincide com a de \\alpha , isto \u00e9, \\kappa_{\\alpha}(s) = \\kappa_{\\beta}(s) \\forall s \\in I . Observe que derivar \\beta ' = \\Phi'(\\alpha) = A\\alpha ' o que garante que \\beta \u00e9 parametrizada pelo comprimento de arco e que \\beta '' = A\\alpha '' . Portanto, como det(A) = 1 , vale que as curvaturas s\u00e3o as mesmas. Equa\u00e7\u00f5es de Frenet Seja \\alpha uma curva parametrizada pelo compimento de arco com vetor normal n(s) e vetor tangente t(s) . Observe que para cada s , esses vetores formam um base ortonormal para \\mathbb{R}^2 . Chamamos essa base de diedro de Frenet . J\u00e1 definimos curvatura com sinal \\kappa quando t'(s) = \\kappa(s) n(s) . Al\u00e9m disso, ||n(s)|| = 1 \\implies n(s) \\perp n'(s) . Portanto n'(s) \u00e9 paralelo a t(s) . Logo n'(s) = \\langle n'(s), t(s) \\rangle t(s) e: \\langle n'(s), t(s) \\rangle = \\langle Jt'(s), t(s) \\rangle = -\\langle t'(s), Jt(s) \\rangle = -det(t(s), t'(s)) = -\\kappa(s) Assim obtemos as equa\u00e7\u00f5es de Frenet : t' = \\kappa \\cdot n n' = -\\kappa \\cdot t As equa\u00e7\u00f5es de Frenet s\u00e3o, portanto, um sistema de equa\u00e7\u00f5es diferenciais envolvendo a base ortonormal para cada s . Teorema Fundamental das Curvas no Plano Sejam I um intervalo aberto e \\kappa : I \\to \\mathbb{R} uma fun\u00e7\u00e3o diferenci\u00e1vel. Ent\u00e3o, existe uma curva diferenci\u00e1vel, \\alpha : I \\to \\mathbb{R}^2 , parametrizada por comprimento de arco, cuja fun\u00e7\u00e3o curvatura \\kappa_{\\alpha} coincide com \\kappa . Al\u00e9m disso, para toda curva \\beta : I \\to \\mathbb{R}^2 , parametrizada por comprimento de arco, que cumpre \\kappa_{\\beta} = \\kappa , existe um movimento r\u00edgido \\Phi : \\mathbb{R}^2 \\to \\mathbb{R}^2 , tal que \\alpha = \\Phi \\circ \\beta . Exemplo: (Reconstru\u00e7\u00e3o de uma curva plana) Suponha que nos \u00e9 dado uma curvatura \\kappa . Como \\kappa = \\theta ' e \\alpha' = (\\cos(\\theta), \\sin(\\theta)) . Vamos supor que o intervalo \u00e9 do tipo [0,l] , onde l \u00e9 o comprimento da curva. def get_curve_from_curvature ( k , tf , x0 , y0 ): # \u00e2ngulo theta_ = solve_ivp ( fun = k , t_span = ( 0 , tf ), y0 = [ 0 ], t_eval = np . arange ( 0 , tf + 1e-4 , 0.01 ) ) . y [ 0 ] def theta ( t ): li = int ( t * ( len ( theta_ ) - 1 ) / tf ) gi = min ( int ( t * ( len ( theta_ ) - 1 ) / tf ) + 1 , len ( theta_ ) - 1 ) convex = t * len ( theta_ ) / tf - li return ( 1 - convex ) * theta_ [ li ] + convex * theta_ [ gi ] # Componentes def f ( t , x ): return [ np . cos ( theta ( t )), np . sin ( theta ( t ))] # Curva alpha = solve_ivp ( fun = f , t_span = ( 0 , tf ), y0 = [ x0 , y0 ], t_eval = np . arange ( 0 , tf + 1e-4 , 0.01 ) ) . y return alpha C\u00edrculo # Curvatura k = lambda t , x : 1 # Um ponto em que passa a curva x0 = 0 y0 = 0 # Comprimento da curva tf = 2 * np . pi + 0.1 alpha = get_curve_from_curvature ( k , tf , x0 , y0 ) plt . figure ( figsize = ( 6 , 6 )) plt . plot ( alpha [ 0 ,:], alpha [ 1 ,:]) plt . show () Reta # Curvatura k = lambda t , x : 0 # Um ponto em que passa a curva x0 = 0 y0 = 0 # Comprimento da curva tf = 10 * np . pi + 0.1 alpha = get_curve_from_curvature ( k , tf , x0 , y0 ) plt . figure ( figsize = ( 6 , 6 )) plt . plot ( alpha [ 0 ,:], alpha [ 1 ,:]) plt . show () Clotoide # Curvatura k = lambda t , x : t # Um ponto em que passa a curva x0 = 0 y0 = 0 # Comprimento da curva tf = 10 * np . pi + 0.1 alpha = get_curve_from_curvature ( k , tf , x0 , y0 ) plt . figure ( figsize = ( 6 , 6 )) plt . plot ( alpha [ 0 ,:], alpha [ 1 ,:]) plt . show ()","title":"Curvatura de uma curva"},{"location":"curvas/curvature/curvature/#curvatura-de-uma-curva","text":"import sympy as sp import numpy as np from scipy.integrate import solve_ivp import matplotlib.pyplot as plt","title":"Curvatura de uma curva"},{"location":"curvas/curvature/curvature/#curvatura","text":"Seja \\gamma uma curva parametrizada pelo comprimeto de arco. Definimos curvatura como a fun\u00e7\u00e3o \\kappa(t) = ||\\ddot{\\gamma}(t)|| . Essa defini\u00e7\u00e3o \u00e9 consistente com o que esper\u00e1vamos de uma reta (curvatura nula) e de um c\u00edrculo (curvatura constante). Al\u00e9m disso se \\gamma \u00e9 uma curva regular qualquer, ela tem uma reparametriza\u00e7\u00e3o pelo comprimento de arco. Portanto, podemos definir a sua curvatura como sendo a curvatura de sua reparametriza\u00e7\u00e3o pelo comprimento de arco. Isto \u00e9, seja \\hat{\\gamma} uma reparametriza\u00e7\u00e3o pelo comprimento de arco de \\gamma com curvatura \\kappa . Ent\u00e3o a curvatura de \\gamma ser\u00e1 \\kappa . Uma quest\u00e3o que se levanta \u00e9: e se houver outra reparametriza\u00e7\u00e3o pelo comprimento de arco para \\gamma ? Para isso, precisamos mostrar que a curvatura \u00e9 invariante (n\u00e3o muda) segundo a reparametriza\u00e7\u00e3o. Isso n\u00e3o \u00e9 dific\u00edl de ver pois as tangentes das reparametriza\u00e7\u00f5es t\u00eam mesmo tamanho e, possivelmente, diferentes sinal.","title":"Curvatura"},{"location":"curvas/curvature/curvature/#curvatura-de-uma-curva-regular","text":"Seja \\gamma(t) uma curva em \\mathbb{R}^3 , ent\u00e3o sua curvatura \u00e9 dada pela express\u00e3o \\kappa = \\frac{||\\ddot{\\gamma} \\times \\dot{\\gamma}||}{||\\dot{\\gamma}||^3} Observe que para curvas no plano essa express\u00e3o pode tamb\u00e9m ser utilizada, \\kappa = \\frac{|\\ddot{\\gamma}_1\\dot{\\gamma}_2 - \\ddot{\\gamma}_2\\dot{\\gamma}_1|}{||\\dot{\\gamma}||^3}","title":"Curvatura de uma curva regular"},{"location":"curvas/curvature/curvature/#curvatura-com-sinal","text":"Definimos a normal unit\u00e1ria com sinal n(s) o vetor unit\u00e1rio que rotaciona o vetor tangente no sentido anti-hor\u00e1rio em \\pi/2 . Em particular, \\ddot{\\gamma}(s) e \\dot{\\gamma}(s) s\u00e3o perpendiculares (pois a curva \u00e9 parametrizada pelo comprimento de arco) e, portanto, \u00e9 paralelo a n(s) , e assim \\ddot{\\gamma}(s) = \\kappa_s(s) n(s) Chamamos \\kappa_s de curvatura com sinal . Em particular, \\kappa = |\\kappa_s| .","title":"Curvatura com sinal"},{"location":"curvas/curvature/curvature/#funcao-angulo","text":"Dada uma curva diferenci\u00e1vel \\gamma: I \\to \\mathcal{S}^1 , onde \\mathcal{S}^1 \u00e9 o c\u00edrculo centrado na origem, dizemos que \\theta : I \\to \\mathbb{R} \u00e9 fun\u00e7\u00e3o \u00e2ngulo de \\gamma quando \\gamma(s) = (\\cos(\\theta(s)), \\sin(\\theta(s)), \\forall s \\in I Observe que nessa defini\u00e7\u00e3o, a imagem de \\gamma \u00e9 um subconjunto de \\mathcal{S}^1 , como se fosse um arco. Por exemplo, \\gamma(s) = (\\cos(2s), \\sin(2s)) \\implies \\theta(s) = 2s . Considere o operador que rotaciona no sentido anti-hor\u00e1rio em 90\u00b0 um vetor. Podemos descrev\u00ea-lo em forma matricial como J = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix} Defina o determinante entre dois vetores como det(v,w) = \\langle Jv , w \\rangle que \u00e9 o produto interno do vetor v rotacionado e w .","title":"Fun\u00e7\u00e3o \u00c2ngulo"},{"location":"curvas/curvature/curvature/#diferenciabilidade","text":"Seja \\gamma : I \\to \\mathcal{S}^1 uma curva diferenci\u00e1vel. Ent\u00e3o \\gamma admite uma fun\u00e7\u00e3o \u00e2ngulo \\theta diferenci\u00e1vel. Al\u00e9m disso, se \\hat{\\theta} \u00e9 fun\u00e7\u00e3o \u00e2ngulo diferenci\u00e1vel de \\gamma , ela difere de \\theta por uma constante. Note que supondo a exist\u00eancia dessa fun\u00e7\u00e3o diferenci\u00e1vel, temos que, por aplica\u00e7\u00e3o da Regra da Cadeia, \\gamma '(s) = \\theta '(s)(-\\sin(\\theta(s)), \\cos(\\theta(s))) = \\theta '(s) J\\gamma(s) Portanto, aplicando o produto interno em ambos os lados, observamos que \\theta '(s) = det(\\gamma(s), \\gamma '(s)) Assim, a demonstra\u00e7\u00e3o se d\u00e1 defininido \\theta com essa derivada (Teorema Fundamental do C\u00e1lculo). Agora seja \\alpha uma curva regular, sem perda de generalidade, parametrizada pelo comprimento de arco. Seja t(s) = \\alpha '(s) . Como ||t(s)|| = 1 , pela proposi\u00e7\u00e3o anterior, existe uma fun\u00e7\u00e3o \u00e2ngulo diferenci\u00e1vel \\theta de forma que definimos a curvatura de \\alpha como k(s) = \\theta '(s) = det(t(s), t'(s)) = det(\\alpha'(s), \\alpha''(s)) Observa\u00e7\u00e3o 1: Se \\alpha \u00e9 regular, sua curvatura \u00e9 \\kappa(s) = \\frac{det(\\alpha'(s), \\alpha''(s))}{||\\alpha '(s)||^3} Observa\u00e7\u00e3o 2: Estamos rotacionando o vetor tangente, obtendo o que chamamos de vetor normal unit\u00e1rio e fazendo o produto interno com a acelera\u00e7\u00e3o da curva, o que coincide com a defini\u00e7\u00e3o pr\u00e9via! Exemplo: Considere a parametriza\u00e7\u00e3o do c\u00edrculo \\alpha(s) = p + r(\\cos(s/r), \\sin(s/r)), s \\in \\mathbb{R} . Assim \\alpha'(s) = (-\\sin(s/r), \\cos(s/r)) \\alpha''(s) = -\\frac{1}{r}(\\cos(s/r), \\sin(s/r)) = \\frac{1}{r}J\\alpha '(s) \\kappa(s) = \\langle J\\alpha '(s), \\alpha ''(s) \\rangle = \\frac{1}{r} Exemplo 2: Vamos usar python para calcular a curvatura da espiral equiangular z(t) = e^{(a + i)t} onde a \u00e9 uma constante e i^2 = -1 . # Definimos as vari\u00e1veis t = sp . symbols ( 't' , real = True ) a = sp . symbols ( 'a' , real = True , constant = True ) # Definimos a fun\u00e7\u00e3o z z = sp . exp (( a + sp . I ) * t ) # Derivando zt = sp . diff ( z , t ) ztt = sp . diff ( zt , t ) # Rotaciona zt Jzt = zt * sp . exp ( sp . I * sp . pi / 2 ) Observe que nossa curva est\u00e1 definida no plano complexo (isomorfo ao plano real). Para calcular a curvatura de uma curva, rotacionamos o vetor e fazemos o produto dot entre esses n\u00fameros como produto escalar de dois vetores. Nesse caso, teremos que \\langle z_1, z_2 \\rangle = Re(z_1\\cdot\\bar{z}_2) . k = sp . re ( Jzt * sp . conjugate ( ztt )) / sp . Abs ( zt ) ** 3 k \\displaystyle \\frac{\\left(2 a^{2} e^{2 a t} - \\left(a^{2} - 1\\right) e^{2 a t}\\right) e^{- 3 a t}}{\\left(a^{2} + 1\\right)^{\\frac{3}{2}}} Precisamos indicar para o programa que queremos uma resposta fatorizada k = sp . factor ( k ) k \\displaystyle \\frac{e^{- a t}}{\\sqrt{a^{2} + 1}} Assim, essa \u00e9 a curvatura da espiral equiangular.","title":"Diferenciabilidade"},{"location":"curvas/curvature/curvature/#movimento-rigido","text":"Isometria: Uma aplica\u00e7\u00e3o F: \\mathbb{R}^n \\to \\mathbb{R}^n que preserva dist\u00e2ncia, isto \u00e9, ||x - y|| = ||F(x) - F(y)||, \\forall x, y \\in \\mathbb{R}^n . Diremos uma movimento r\u00edgido \u00e9 uma isometria (a rigidez em mudar dist\u00e2ncias). Transla\u00e7\u00e3o: Uma aplica\u00e7\u00e3o F: \\mathbb{R}^n \\to \\mathbb{R}^n do tipo v \\mapsto F(v) := v + a , para algum a \\in \\mathbb{R}^n fixo. Transforma\u00e7\u00e3o Ortogonal: Uma transforma\u00e7\u00e3o linear que presetva o produto interno, isto \u00e9, \\langle u, v \\rangle = \\langle T(u), T(v) \\rangle . A matriz associada a essa transforma\u00e7\u00e3o \u00e9 ortogonal. Dizemos que o movimento \u00e9 direto se det(P) = 1 e oposto ou inverso quando det(P) = -1 .","title":"Movimento R\u00edgido"},{"location":"curvas/curvature/curvature/#teorema","text":"Seja P_{n\\times n} uma matriz ortogonal e a \\in \\mathbb{R}^n . Ent\u00e3o F: \\mathbb{R}^n \\to \\mathbb{R}^n definido como F(v) = Pv + a \u00e9 uma isometria. Reciprocamente, toda isometria pode ser escrito nessa forma. Esse teorema permite uma caracteriza\u00e7\u00e3o simples de um movimento r\u00edgido.","title":"Teorema"},{"location":"curvas/curvature/curvature/#invariancia-da-curvatura","text":"Sejam \\Phi = A + p_0 um movimento r\u00edgido direto de R^2 e \\alpha : I \\to \\mathbb{R}^2 uma curva regular parametrizada por comprimento de arco. Ent\u00e3o, \\beta = \\Phi \\circ \\alpha : I \\to \\mathbb{R}^2 \u00e9 uma curva regular de \\mathbb{R}^2 , parametrizada por comprimento de arco, cuja fun\u00e7\u00e3o curvatura coincide com a de \\alpha , isto \u00e9, \\kappa_{\\alpha}(s) = \\kappa_{\\beta}(s) \\forall s \\in I . Observe que derivar \\beta ' = \\Phi'(\\alpha) = A\\alpha ' o que garante que \\beta \u00e9 parametrizada pelo comprimento de arco e que \\beta '' = A\\alpha '' . Portanto, como det(A) = 1 , vale que as curvaturas s\u00e3o as mesmas.","title":"Invari\u00e2ncia da curvatura"},{"location":"curvas/curvature/curvature/#equacoes-de-frenet","text":"Seja \\alpha uma curva parametrizada pelo compimento de arco com vetor normal n(s) e vetor tangente t(s) . Observe que para cada s , esses vetores formam um base ortonormal para \\mathbb{R}^2 . Chamamos essa base de diedro de Frenet . J\u00e1 definimos curvatura com sinal \\kappa quando t'(s) = \\kappa(s) n(s) . Al\u00e9m disso, ||n(s)|| = 1 \\implies n(s) \\perp n'(s) . Portanto n'(s) \u00e9 paralelo a t(s) . Logo n'(s) = \\langle n'(s), t(s) \\rangle t(s) e: \\langle n'(s), t(s) \\rangle = \\langle Jt'(s), t(s) \\rangle = -\\langle t'(s), Jt(s) \\rangle = -det(t(s), t'(s)) = -\\kappa(s) Assim obtemos as equa\u00e7\u00f5es de Frenet : t' = \\kappa \\cdot n n' = -\\kappa \\cdot t As equa\u00e7\u00f5es de Frenet s\u00e3o, portanto, um sistema de equa\u00e7\u00f5es diferenciais envolvendo a base ortonormal para cada s .","title":"Equa\u00e7\u00f5es de Frenet"},{"location":"curvas/curvature/curvature/#teorema-fundamental-das-curvas-no-plano","text":"Sejam I um intervalo aberto e \\kappa : I \\to \\mathbb{R} uma fun\u00e7\u00e3o diferenci\u00e1vel. Ent\u00e3o, existe uma curva diferenci\u00e1vel, \\alpha : I \\to \\mathbb{R}^2 , parametrizada por comprimento de arco, cuja fun\u00e7\u00e3o curvatura \\kappa_{\\alpha} coincide com \\kappa . Al\u00e9m disso, para toda curva \\beta : I \\to \\mathbb{R}^2 , parametrizada por comprimento de arco, que cumpre \\kappa_{\\beta} = \\kappa , existe um movimento r\u00edgido \\Phi : \\mathbb{R}^2 \\to \\mathbb{R}^2 , tal que \\alpha = \\Phi \\circ \\beta . Exemplo: (Reconstru\u00e7\u00e3o de uma curva plana) Suponha que nos \u00e9 dado uma curvatura \\kappa . Como \\kappa = \\theta ' e \\alpha' = (\\cos(\\theta), \\sin(\\theta)) . Vamos supor que o intervalo \u00e9 do tipo [0,l] , onde l \u00e9 o comprimento da curva. def get_curve_from_curvature ( k , tf , x0 , y0 ): # \u00e2ngulo theta_ = solve_ivp ( fun = k , t_span = ( 0 , tf ), y0 = [ 0 ], t_eval = np . arange ( 0 , tf + 1e-4 , 0.01 ) ) . y [ 0 ] def theta ( t ): li = int ( t * ( len ( theta_ ) - 1 ) / tf ) gi = min ( int ( t * ( len ( theta_ ) - 1 ) / tf ) + 1 , len ( theta_ ) - 1 ) convex = t * len ( theta_ ) / tf - li return ( 1 - convex ) * theta_ [ li ] + convex * theta_ [ gi ] # Componentes def f ( t , x ): return [ np . cos ( theta ( t )), np . sin ( theta ( t ))] # Curva alpha = solve_ivp ( fun = f , t_span = ( 0 , tf ), y0 = [ x0 , y0 ], t_eval = np . arange ( 0 , tf + 1e-4 , 0.01 ) ) . y return alpha","title":"Teorema Fundamental das Curvas no Plano"},{"location":"curvas/curvature/curvature/#circulo","text":"# Curvatura k = lambda t , x : 1 # Um ponto em que passa a curva x0 = 0 y0 = 0 # Comprimento da curva tf = 2 * np . pi + 0.1 alpha = get_curve_from_curvature ( k , tf , x0 , y0 ) plt . figure ( figsize = ( 6 , 6 )) plt . plot ( alpha [ 0 ,:], alpha [ 1 ,:]) plt . show ()","title":"C\u00edrculo"},{"location":"curvas/curvature/curvature/#reta","text":"# Curvatura k = lambda t , x : 0 # Um ponto em que passa a curva x0 = 0 y0 = 0 # Comprimento da curva tf = 10 * np . pi + 0.1 alpha = get_curve_from_curvature ( k , tf , x0 , y0 ) plt . figure ( figsize = ( 6 , 6 )) plt . plot ( alpha [ 0 ,:], alpha [ 1 ,:]) plt . show ()","title":"Reta"},{"location":"curvas/curvature/curvature/#clotoide","text":"# Curvatura k = lambda t , x : t # Um ponto em que passa a curva x0 = 0 y0 = 0 # Comprimento da curva tf = 10 * np . pi + 0.1 alpha = get_curve_from_curvature ( k , tf , x0 , y0 ) plt . figure ( figsize = ( 6 , 6 )) plt . plot ( alpha [ 0 ,:], alpha [ 1 ,:]) plt . show ()","title":"Clotoide"},{"location":"edo/info/","text":"Informa\u00e7\u00f5es Gerais Monitoria de Equa\u00e7\u00f5es Diferenciais Ordin\u00e1rias correspondente ao per\u00edodo de 2020.1. Os temas abordados s\u00e3o: Equa\u00e7\u00f5es diferenciais lineares de 1\u00aa ordem. Equa\u00e7\u00f5es com vari\u00e1veis separ\u00e1veis. Equa\u00e7\u00f5es exatas. Equa\u00e7\u00f5es de 2\u00aa ordem. Modelos de din\u00e2mica populacional e ci\u00eancias naturais. Sistemas lineares homog\u00eaneos e n\u00e3o homog\u00eaneos. Sistemas n\u00e3o lineares: an\u00e1lise qualitativa. M\u00e9todos num\u00e9ricos. Grava\u00e7\u00f5es Grava\u00e7\u00f5es das monitorias Resumos Os conte\u00fados referentes \u00e0 primeira parte do curso se encontram resumidos e escritos em Latex . Sistemas Lineares Exponencial de uma matriz : Desci\u00e7\u00e3o da exponencial de uma matriz para resolver sistemas lineares e caracteriza\u00e7\u00e3o no \\mathbb{R}^2 . Autovalores complexos e repetidos : An\u00e1lise no \\mathbb{R}^2 de sistemas lineares com autovalores complexos ou repetidos. Sistemas lineares, uma revis\u00e3o : Revis\u00e3o geral sobre o t\u00f3pico com exerc\u00edcios espec\u00edficos para cada caso de matriz em \\mathbb{R}^2 . Teorema de Exist\u00eancia e Unicidade Proposta de demonstra\u00e7\u00e3o : Rascunho de uma das demonstra\u00e7\u00f5es mais interessantes sobre o teorema. Sistemas n\u00e3o lineares Aproxima\u00e7\u00e3o pr\u00f3ximo \u00e0s singularidades : Uso de um variante do teorema de Hartman-Grobman. Plano Tra\u00e7o-Determinante : Revis\u00e3o do gr\u00e1fico que resume os sistemas lineares e nos ajudam a identificar o sistema pr\u00f3ximo do equil\u00edbrio. M\u00e9todos num\u00e9ricos : Simples revis\u00e3o dos m\u00e9todos de Euler e Runge-Kutta. Modelos com EDOs Modelos : Modelo predador-presa e modelos SEIR, com c\u00e1lculo do R_0 . Conte\u00fado Extra Resumos de assuntos do livro Differential equations, dynamical systems, and linear algebra , de Morris W. Hirsch and Stephen Smale. Sistema planar simples : Descri\u00e7\u00e3o dos sistemas lineares planares simples, como caso especial. Sistemas de alta dimens\u00e3o : Generaliza\u00e7\u00e3o do t\u00f3pico anterior. Teorema fundamental de exist\u00eancia e unicidade : Cap\u00edtulo 8 do livro que cobre o principal teorema, sendo uma varia\u00e7\u00e3o do apresentado na monitoria. Estabilidade : Estudos de estabilidade de sistemas n\u00e3o lineares. Cap\u00edtulo 9 do livro. Teorema de Pointcar\u00e9-Bendixson : Teorema sobre \u00f3rbitas que n\u00e3o foi descorrido no curso, mas tema interessante. Avalia\u00e7\u00f5es Per\u00edodo T\u00f3picos Cobertos PDF A1 Equa\u00e7\u00f5es exatas, de 1\u00aa e 2\u00aa ordem e modelagem de problemas [questoes] [respostas] A2 Sistemas lineares homog\u00eaneos e n\u00e3o homog\u00eaneos [respostas]","title":"EDO"},{"location":"edo/info/#informacoes-gerais","text":"Monitoria de Equa\u00e7\u00f5es Diferenciais Ordin\u00e1rias correspondente ao per\u00edodo de 2020.1. Os temas abordados s\u00e3o: Equa\u00e7\u00f5es diferenciais lineares de 1\u00aa ordem. Equa\u00e7\u00f5es com vari\u00e1veis separ\u00e1veis. Equa\u00e7\u00f5es exatas. Equa\u00e7\u00f5es de 2\u00aa ordem. Modelos de din\u00e2mica populacional e ci\u00eancias naturais. Sistemas lineares homog\u00eaneos e n\u00e3o homog\u00eaneos. Sistemas n\u00e3o lineares: an\u00e1lise qualitativa. M\u00e9todos num\u00e9ricos.","title":"Informa\u00e7\u00f5es Gerais"},{"location":"edo/info/#gravacoes","text":"Grava\u00e7\u00f5es das monitorias","title":"Grava\u00e7\u00f5es"},{"location":"edo/info/#resumos","text":"Os conte\u00fados referentes \u00e0 primeira parte do curso se encontram resumidos e escritos em Latex .","title":"Resumos"},{"location":"edo/info/#sistemas-lineares","text":"Exponencial de uma matriz : Desci\u00e7\u00e3o da exponencial de uma matriz para resolver sistemas lineares e caracteriza\u00e7\u00e3o no \\mathbb{R}^2 . Autovalores complexos e repetidos : An\u00e1lise no \\mathbb{R}^2 de sistemas lineares com autovalores complexos ou repetidos. Sistemas lineares, uma revis\u00e3o : Revis\u00e3o geral sobre o t\u00f3pico com exerc\u00edcios espec\u00edficos para cada caso de matriz em \\mathbb{R}^2 .","title":"Sistemas Lineares"},{"location":"edo/info/#teorema-de-existencia-e-unicidade","text":"Proposta de demonstra\u00e7\u00e3o : Rascunho de uma das demonstra\u00e7\u00f5es mais interessantes sobre o teorema.","title":"Teorema de Exist\u00eancia e Unicidade"},{"location":"edo/info/#sistemas-nao-lineares","text":"Aproxima\u00e7\u00e3o pr\u00f3ximo \u00e0s singularidades : Uso de um variante do teorema de Hartman-Grobman. Plano Tra\u00e7o-Determinante : Revis\u00e3o do gr\u00e1fico que resume os sistemas lineares e nos ajudam a identificar o sistema pr\u00f3ximo do equil\u00edbrio. M\u00e9todos num\u00e9ricos : Simples revis\u00e3o dos m\u00e9todos de Euler e Runge-Kutta.","title":"Sistemas n\u00e3o lineares"},{"location":"edo/info/#modelos-com-edos","text":"Modelos : Modelo predador-presa e modelos SEIR, com c\u00e1lculo do R_0 .","title":"Modelos com EDOs"},{"location":"edo/info/#conteudo-extra","text":"Resumos de assuntos do livro Differential equations, dynamical systems, and linear algebra , de Morris W. Hirsch and Stephen Smale. Sistema planar simples : Descri\u00e7\u00e3o dos sistemas lineares planares simples, como caso especial. Sistemas de alta dimens\u00e3o : Generaliza\u00e7\u00e3o do t\u00f3pico anterior. Teorema fundamental de exist\u00eancia e unicidade : Cap\u00edtulo 8 do livro que cobre o principal teorema, sendo uma varia\u00e7\u00e3o do apresentado na monitoria. Estabilidade : Estudos de estabilidade de sistemas n\u00e3o lineares. Cap\u00edtulo 9 do livro. Teorema de Pointcar\u00e9-Bendixson : Teorema sobre \u00f3rbitas que n\u00e3o foi descorrido no curso, mas tema interessante.","title":"Conte\u00fado Extra"},{"location":"edo/info/#avaliacoes","text":"Per\u00edodo T\u00f3picos Cobertos PDF A1 Equa\u00e7\u00f5es exatas, de 1\u00aa e 2\u00aa ordem e modelagem de problemas [questoes] [respostas] A2 Sistemas lineares homog\u00eaneos e n\u00e3o homog\u00eaneos [respostas]","title":"Avalia\u00e7\u00f5es"},{"location":"edp/calculus_of_variations/","text":"C\u00e1lculo de Varia\u00e7\u00f5es XXX: Import\u00e2ncia Equa\u00e7\u00f5es caracter\u00edsticas Hamilton-Jacobi A equa\u00e7\u00e3o de Hamilton Jacobi \u00e9 dada pela express\u00e3o u_t + H(Du, x) = 0, em que H : \\mathbb{R}^{2n} \\to \\mathbb{R} e Du = (u_{x_1,}, \\dots, u_{x_n}) . Reescreva p = Du , p_{n+1} = u_t e q = (p,p_{n+1}) (como se o tempo fosse a dimens\u00e3o n+1 ) e assim teremos y = (x,t) nossa vari\u00e1vel. Podemos reescrever como o sistema G(p, p_{n+1}, z, y) = p_{n+1} + H(p, x) = 0. Note que, em particular, D_qG = (D_pG(p,x), 1) e D_yG = (D_xH(p,x), 0) e D_zG = 0 . Remark: Lembre que a nota\u00e7\u00e3o D_vG \u00e9 o mesmo que dizer que voc\u00ea vai derivar G com respeito a cada componente de v . Se v \\in \\mathbb{R} , temos que D_vG = G'(v) . Com isso, n\u00f3s podemos montar as equa\u00e7\u00f5es caracter\u00edsticas (dadas pela curvas caracter\u00edsticas \\gamma(s) ), \\begin{cases} \\dot{x}_1(s) = D_{p_1}G = H_{p_1}(p(s), \\gamma(s)) \\\\ \\dot{x}_2(s) = D_{p_2}G = H_{p_2}(p(s), \\gamma(s)) \\\\ \\dots \\\\ \\dot{x}_n(s) = D_{p_n}G = H_{p_n}(p(s), \\gamma(s)) \\\\ \\dot{t}(s) = D_{p_{n+1}}G = 1 \\implies t = s, \\\\ \\end{cases} o que permite intercambiar os par\u00e2metros t e s . \\begin{cases} \\dot{p}_1(t) = -D_{x_1}G - D_zG\\cdot p_1 = -H_{x_1}(p(s), \\gamma(s)) \\\\ \\dot{p}_2(t) = -D_{x_2}G - D_zG\\cdot p_1 = -H_{x_2}(p(s), \\gamma(s)) \\\\ \\dots \\\\ \\dot{p}_n(t) = -D_{x_n}G - D_zG\\cdot p_1 = -H_{x_n}(p(s), \\gamma(s)) \\\\ \\dot{p}_{n+1}(t) = -D_{t}G - D_zG\\cdot p_1 = 0 \\\\ \\end{cases} e, por fim \\begin{split} \\dot{z} &= D_qG\\cdot q = D_pH(p(t), \\gamma(t))\\cdot p(t) + p^{n+1}(t) \\\\ &= D_pH(p(t), \\gamma(t))\\cdot p(t) - H(p(t), \\gamma(s)) \\end{split} Note portanto que podemos sumarizar essas equa\u00e7\u00f5es em \\begin{cases} \\dot{\\gamma}(t) = D_pH(p(t),\\gamma(t)) \\\\ \\dot{p}(t) = -D_{x}H(p(t), \\gamma(t)) \\\\ \\dot{z}(t) = D_pH(p(t), \\gamma(t)) - H(p(t), \\gamma(t)) \\end{cases} em que as primeiras duas equa\u00e7\u00f5es s\u00e3o as equa\u00e7\u00f5es de Hamilton . A fun\u00e7\u00e3o H tamb\u00e9m \u00e9 chamada de Hamiltoniano. Aplica\u00e7\u00f5es XXX: TODO O c\u00e1lculo das varia\u00e7\u00f5es Introduzimos o Lagrangiano \\begin{align*} L : \\mathbb{R}^n \\times \\mathbb{R}^n &\\to \\mathbb{R} \\\\ (v_1,\\dots,v_n,x_1,\\dots,x_n) &\\mapsto L(v,x). \\end{align*} Al\u00e9m disso, definimos o funcional a\u00e7\u00e3o como I[w(\\cdot)] := \\int_0^t L(\\dot{w}(s), w(s))\\, ds, em que as fun\u00e7\u00f5es w s\u00e3o duas vezes continuamente diferenci\u00e1veis com w(0) = y e w(t) = x fixados anteriormente. Um problema geral do c\u00e1lculo das varia\u00e7\u00f5es \u00e9 encontrar uma fun\u00e7\u00e3o x(\\cdot) que minimiza I[w(\\cdot)] dentre todas as w . Exemplo Qual a fun\u00e7\u00e3o y = f(x) cuja curva (diferenci\u00e1vel) entre os pontos (x_1, y_1) e (x_2, y_2) tem o comprimento. Dada uma fun\u00e7\u00e3o, sabemos que seu comprimento de arco \u00e9 Comp[y] = \\int_{x_1}^{x_2} \\sqrt{1 + y'(x)^2} \\, dx. Nesse caso, L(f'(x), f(x)) = \\sqrt{1 + f'(x)^2} e sabemos que sua solu\u00e7\u00e3o \u00e9 o segmento de reta entre esses pontos. Equa\u00e7\u00f5es de Euler-Lagrange A solu\u00e7\u00e3o \u00f3tima para o problema de c\u00e1lculo das varia\u00e7\u00f5es resolve o sistema de equa\u00e7\u00f5es Euler-Lagrange: -\\frac{d}{ds}(D_v L(\\dot{x}(s), x(s))) + D_xL(\\dot{x}(s), x(s)) = 0, 0 \\le s \\le t. Note que resolver as equa\u00e7\u00f5es n\u00e3o nos d\u00e1 as fun\u00e7\u00f5es \u00f3timas, da mesma forma que derivada igual a zero n\u00e3o implica m\u00ednimo local. O interessante \u00e9 que, dado uma Lagrangiano L , podemos introduzir o Hamiltoniano H(p,x) := p\\cdot v(p,x) - L(v(p,x), x), em que v \u00e9 definido como a fun\u00e7\u00e3o tal que p = D_vL(v,x) . Podemos demonstrar que a partir desse H , temos as equa\u00e7\u00f5es de Hamilton, o que permite uma conex\u00e3o entre as duas teorias. Aplica\u00e7\u00e3o ao exemplo XXX: TODO","title":"C\u00e1lculo de Varia\u00e7\u00f5es"},{"location":"edp/calculus_of_variations/#calculo-de-variacoes","text":"XXX: Import\u00e2ncia","title":"C\u00e1lculo de Varia\u00e7\u00f5es"},{"location":"edp/calculus_of_variations/#equacoes-caracteristicas-hamilton-jacobi","text":"A equa\u00e7\u00e3o de Hamilton Jacobi \u00e9 dada pela express\u00e3o u_t + H(Du, x) = 0, em que H : \\mathbb{R}^{2n} \\to \\mathbb{R} e Du = (u_{x_1,}, \\dots, u_{x_n}) . Reescreva p = Du , p_{n+1} = u_t e q = (p,p_{n+1}) (como se o tempo fosse a dimens\u00e3o n+1 ) e assim teremos y = (x,t) nossa vari\u00e1vel. Podemos reescrever como o sistema G(p, p_{n+1}, z, y) = p_{n+1} + H(p, x) = 0. Note que, em particular, D_qG = (D_pG(p,x), 1) e D_yG = (D_xH(p,x), 0) e D_zG = 0 . Remark: Lembre que a nota\u00e7\u00e3o D_vG \u00e9 o mesmo que dizer que voc\u00ea vai derivar G com respeito a cada componente de v . Se v \\in \\mathbb{R} , temos que D_vG = G'(v) . Com isso, n\u00f3s podemos montar as equa\u00e7\u00f5es caracter\u00edsticas (dadas pela curvas caracter\u00edsticas \\gamma(s) ), \\begin{cases} \\dot{x}_1(s) = D_{p_1}G = H_{p_1}(p(s), \\gamma(s)) \\\\ \\dot{x}_2(s) = D_{p_2}G = H_{p_2}(p(s), \\gamma(s)) \\\\ \\dots \\\\ \\dot{x}_n(s) = D_{p_n}G = H_{p_n}(p(s), \\gamma(s)) \\\\ \\dot{t}(s) = D_{p_{n+1}}G = 1 \\implies t = s, \\\\ \\end{cases} o que permite intercambiar os par\u00e2metros t e s . \\begin{cases} \\dot{p}_1(t) = -D_{x_1}G - D_zG\\cdot p_1 = -H_{x_1}(p(s), \\gamma(s)) \\\\ \\dot{p}_2(t) = -D_{x_2}G - D_zG\\cdot p_1 = -H_{x_2}(p(s), \\gamma(s)) \\\\ \\dots \\\\ \\dot{p}_n(t) = -D_{x_n}G - D_zG\\cdot p_1 = -H_{x_n}(p(s), \\gamma(s)) \\\\ \\dot{p}_{n+1}(t) = -D_{t}G - D_zG\\cdot p_1 = 0 \\\\ \\end{cases} e, por fim \\begin{split} \\dot{z} &= D_qG\\cdot q = D_pH(p(t), \\gamma(t))\\cdot p(t) + p^{n+1}(t) \\\\ &= D_pH(p(t), \\gamma(t))\\cdot p(t) - H(p(t), \\gamma(s)) \\end{split} Note portanto que podemos sumarizar essas equa\u00e7\u00f5es em \\begin{cases} \\dot{\\gamma}(t) = D_pH(p(t),\\gamma(t)) \\\\ \\dot{p}(t) = -D_{x}H(p(t), \\gamma(t)) \\\\ \\dot{z}(t) = D_pH(p(t), \\gamma(t)) - H(p(t), \\gamma(t)) \\end{cases} em que as primeiras duas equa\u00e7\u00f5es s\u00e3o as equa\u00e7\u00f5es de Hamilton . A fun\u00e7\u00e3o H tamb\u00e9m \u00e9 chamada de Hamiltoniano.","title":"Equa\u00e7\u00f5es caracter\u00edsticas Hamilton-Jacobi"},{"location":"edp/calculus_of_variations/#aplicacoes","text":"XXX: TODO","title":"Aplica\u00e7\u00f5es"},{"location":"edp/calculus_of_variations/#o-calculo-das-variacoes","text":"Introduzimos o Lagrangiano \\begin{align*} L : \\mathbb{R}^n \\times \\mathbb{R}^n &\\to \\mathbb{R} \\\\ (v_1,\\dots,v_n,x_1,\\dots,x_n) &\\mapsto L(v,x). \\end{align*} Al\u00e9m disso, definimos o funcional a\u00e7\u00e3o como I[w(\\cdot)] := \\int_0^t L(\\dot{w}(s), w(s))\\, ds, em que as fun\u00e7\u00f5es w s\u00e3o duas vezes continuamente diferenci\u00e1veis com w(0) = y e w(t) = x fixados anteriormente. Um problema geral do c\u00e1lculo das varia\u00e7\u00f5es \u00e9 encontrar uma fun\u00e7\u00e3o x(\\cdot) que minimiza I[w(\\cdot)] dentre todas as w .","title":"O c\u00e1lculo das varia\u00e7\u00f5es"},{"location":"edp/calculus_of_variations/#exemplo","text":"Qual a fun\u00e7\u00e3o y = f(x) cuja curva (diferenci\u00e1vel) entre os pontos (x_1, y_1) e (x_2, y_2) tem o comprimento. Dada uma fun\u00e7\u00e3o, sabemos que seu comprimento de arco \u00e9 Comp[y] = \\int_{x_1}^{x_2} \\sqrt{1 + y'(x)^2} \\, dx. Nesse caso, L(f'(x), f(x)) = \\sqrt{1 + f'(x)^2} e sabemos que sua solu\u00e7\u00e3o \u00e9 o segmento de reta entre esses pontos.","title":"Exemplo"},{"location":"edp/calculus_of_variations/#equacoes-de-euler-lagrange","text":"A solu\u00e7\u00e3o \u00f3tima para o problema de c\u00e1lculo das varia\u00e7\u00f5es resolve o sistema de equa\u00e7\u00f5es Euler-Lagrange: -\\frac{d}{ds}(D_v L(\\dot{x}(s), x(s))) + D_xL(\\dot{x}(s), x(s)) = 0, 0 \\le s \\le t. Note que resolver as equa\u00e7\u00f5es n\u00e3o nos d\u00e1 as fun\u00e7\u00f5es \u00f3timas, da mesma forma que derivada igual a zero n\u00e3o implica m\u00ednimo local. O interessante \u00e9 que, dado uma Lagrangiano L , podemos introduzir o Hamiltoniano H(p,x) := p\\cdot v(p,x) - L(v(p,x), x), em que v \u00e9 definido como a fun\u00e7\u00e3o tal que p = D_vL(v,x) . Podemos demonstrar que a partir desse H , temos as equa\u00e7\u00f5es de Hamilton, o que permite uma conex\u00e3o entre as duas teorias.","title":"Equa\u00e7\u00f5es de Euler-Lagrange"},{"location":"edp/calculus_of_variations/#aplicacao-ao-exemplo","text":"XXX: TODO","title":"Aplica\u00e7\u00e3o ao exemplo"},{"location":"edp/info/","text":"Informa\u00e7\u00f5es Gerais Monitoria de Equa\u00e7\u00f5es Diferenciais Parciais correspondente ao per\u00edodo de 2021.2. Dia: Quinta-feira, 18h at\u00e9 20h Link Google Meet Monitorias gravadas T\u00f3picos Conceitos introdut\u00f3rios Teorema de Picard-Lindel\u00f6f aplicado Defini\u00e7\u00f5es preliminares M\u00e9todos de solu\u00e7\u00e3o M\u00e9todo das caracter\u00edsticas C\u00e1lculo das varia\u00e7\u00f5es e Hamiltoniano Listas N\u00famero Itens discutidos Solu\u00e7\u00e3o 1 Equa\u00e7\u00f5es diferenciais ordin\u00e1rias 1 2 M\u00e9todo das caracter\u00edsticas 2 3 M\u00e9todo das caracter\u00edsticas e energia 3 Notas Monitoria Itens discutidos Arquivo V\u00eddeo 19/08/2021 Teorema da Retifica\u00e7\u00e3o Visualizar N\u00e3o 09/09/2021 Exemplos equa\u00e7\u00f5es caracter\u00edsticas Visualizar N\u00e3o Provas Ano Bimestre Quest\u00f5es Solu\u00e7\u00f5es Observa\u00e7\u00f5es 2021 A1 ver arquivo ver arquivo ver c\u00f3digo Sugest\u00f5es Adicionais Differential equations 3b1b Livro mais avan\u00e7ado por Rafael Jos\u00e9 Iorio Jr e Val\u00e9ria de Magalh\u00e3es Iorio Livro Partial Differential Equations de Lawrence C.Evans","title":"EDP"},{"location":"edp/info/#informacoes-gerais","text":"Monitoria de Equa\u00e7\u00f5es Diferenciais Parciais correspondente ao per\u00edodo de 2021.2. Dia: Quinta-feira, 18h at\u00e9 20h Link Google Meet Monitorias gravadas","title":"Informa\u00e7\u00f5es Gerais"},{"location":"edp/info/#topicos","text":"Conceitos introdut\u00f3rios Teorema de Picard-Lindel\u00f6f aplicado Defini\u00e7\u00f5es preliminares M\u00e9todos de solu\u00e7\u00e3o M\u00e9todo das caracter\u00edsticas C\u00e1lculo das varia\u00e7\u00f5es e Hamiltoniano","title":"T\u00f3picos"},{"location":"edp/info/#listas","text":"N\u00famero Itens discutidos Solu\u00e7\u00e3o 1 Equa\u00e7\u00f5es diferenciais ordin\u00e1rias 1 2 M\u00e9todo das caracter\u00edsticas 2 3 M\u00e9todo das caracter\u00edsticas e energia 3","title":"Listas"},{"location":"edp/info/#notas","text":"Monitoria Itens discutidos Arquivo V\u00eddeo 19/08/2021 Teorema da Retifica\u00e7\u00e3o Visualizar N\u00e3o 09/09/2021 Exemplos equa\u00e7\u00f5es caracter\u00edsticas Visualizar N\u00e3o","title":"Notas"},{"location":"edp/info/#provas","text":"Ano Bimestre Quest\u00f5es Solu\u00e7\u00f5es Observa\u00e7\u00f5es 2021 A1 ver arquivo ver arquivo ver c\u00f3digo","title":"Provas"},{"location":"edp/info/#sugestoes-adicionais","text":"Differential equations 3b1b Livro mais avan\u00e7ado por Rafael Jos\u00e9 Iorio Jr e Val\u00e9ria de Magalh\u00e3es Iorio Livro Partial Differential Equations de Lawrence C.Evans","title":"Sugest\u00f5es Adicionais"},{"location":"edp/introduction/","text":"Defini\u00e7\u00f5es iniciais Uma equa\u00e7\u00e3o diferencial parcial (EDP) ou partial differential equation (PDE) \u00e9 uma equa\u00e7\u00e3o que envolve duas ou mais vari\u00e1veis independentes e derivadas parciais de uma fun\u00e7\u00e3o que depende dessas vari\u00e1veis. De forma bem geral, \u00e9 uma equa\u00e7\u00e3o com forma F\\left(x_1, \\dots, x_n, u, \\frac{\\partial u}{\\partial x_1}, \\dots, \\frac{\\partial u}{\\partial x_n}, \\dots, \\frac{\\partial^k u}{\\partial x_{i_1} \\dots \\partial x_{i_k}}, \\dots, \\frac{\\partial^k u}{\\partial x_n^k}\\right) = 0, em que u = u(x_1, ..., x_n) e na express\u00e3o aparece a k -\u00e9sima derivada de u com respeito a k vari\u00e1veis x_{i_1}, \\dots, x_{i_k} . A fun\u00e7\u00e3o u \u00e9 em geral desconhecida e assumidamente de classe C^k , isto \u00e9, k vezes continuamente diferenci\u00e1vel. Denotamos tamb\u00e9m, \\nabla^2 u := \\frac{\\partial u}{\\partial x_1^2} + \\dots + \\frac{\\partial u}{\\partial x_n^2} = \\Delta u, o operador de Laplace . Exemplos Equa\u00e7\u00e3o da Onda : \\Delta u = \\frac{1}{c^2}\\frac{\\partial^2 u}{\\partial t^2}, em que c mede a velocidade da propaga\u00e7\u00e3o da onda. Nesse exemplo temos n vari\u00e1veis espaciais x_1, \\dots, x_n e uma vari\u00e1vel temporal t . Podemos definir F\\left(x_1, \\dots, x_n, t, u, \\frac{\\partial^2 u}{\\partial x_1^2}, \\dots, \\frac{\\partial^2 u}{\\partial x_n^2}, \\frac{\\partial^2 u}{\\partial t^2}\\right) = c^2\\Delta u - u_{tt} Equa\u00e7\u00e3o do calor : \\Delta u = \\frac{1}{k}\\frac{\\partial u}{\\partial t}, Equa\u00e7\u00e3o de Laplace \\Delta u = 0 As solu\u00e7\u00f5es dessa equa\u00e7\u00e3o s\u00e3o as fun\u00e7\u00f5es harm\u00f4nicas . Equa\u00e7\u00e3o de Schr\u00f6dinger iu_t + \\Delta u = 0 \u00e9 a fun\u00e7\u00e3o que governa a fun\u00e7\u00e3o de onda no sistema mec\u00e2nico qu\u00e2ntico. Classifica\u00e7\u00e3o de uma EDP Ordem: Dada pela ordem parcial de maior ordem que ocorre na equa\u00e7\u00e3o. Nos exemplos anteriores, as equa\u00e7\u00f5es tem ordem 2. Linear: Ela \u00e9 dita linear se \u00e9 de primeiro grau em u e em todas suas derivadas parciais. Caso contr\u00e1rio ela \u00e9 dita n\u00e3o linear . Por exemplo, uma EDP linear de ordem 1 \u00e9 dada pela express\u00e3o (veja as diferentes express\u00f5es para derivada parcial ) \\sum_{j=1}^n a_j(x) D_{x_j} u + b(x) u + c(x) = 0 Homog\u00eanea: Os termos das vari\u00e1veis independentes s\u00e3o nulos, isto \u00e9, as vari\u00e1veis independentes n\u00e3o aparecem na express\u00e3o da fun\u00e7\u00e3o F . Parte principal: A parte da equa\u00e7\u00e3o que cont\u00e9m as derivadas de maior ordem. Se uma equa\u00e7\u00e3o n\u00e3o linear possui parte principal linear, ela \u00e9 dita semilinear . Exemplos Equa\u00e7\u00e3o telegrafo u_{tt} + 2du_t - u_{xx} = 0, \u00e9 uma equa\u00e7\u00e3o linear de ordem 2. Equa\u00e7\u00e3o de beam u_{tt} + u_{xxxx} = 0 \u00e9 uma equa\u00e7\u00e3o linear de ordem 4 Equa\u00e7\u00e3o da onda (n\u00e3o linear) u_{tt} - \\Delta u + f(u) = 0, em que f \u00e9 uma fun\u00e7\u00e3o n\u00e3o linear de u . Observe que a parte principal \u00e9 u_{tt} - \\Delta u que \u00e9 linear e, portanto, a equa\u00e7\u00e3o \u00e9 semilinear. Exemplo: Suponha que u e v s\u00e3o duas fun\u00e7\u00f5es solu\u00e7\u00e3o da equa\u00e7\u00e3o (linear de ordem 1) \\frac{\\partial u}{\\partial x} + xu = 0 Seja z := \\alpha u + \\beta v , em que \\alpha, \\beta \\in \\mathbb{R} . Assim, \\frac{\\partial z}{\\partial x} + xz = \\alpha\\frac{\\partial u}{\\partial x} + \\beta\\frac{\\partial v}{\\partial x} + \\alpha x u + \\beta x v = \\alpha\\left(\\frac{\\partial u}{\\partial x} + xu\\right) + \\beta\\left(\\frac{\\partial v}{\\partial x} + xv\\right) = 0, o que implica que z tamb\u00e9m \u00e9 solu\u00e7\u00e3o. Essa propriedade n\u00e3o \u00e9 exclusividade do exemplo acima. Na verdade, vale para toda equa\u00e7\u00e3o linear, isto \u00e9, combina\u00e7\u00e3o linear de solu\u00e7\u00f5es \u00e9 solu\u00e7\u00e3o em problemas lineares. Princ\u00edpio da Superposi\u00e7\u00e3o Considere uma EDP linear de ordem 2: \\sum_{i,j}^n a_{ij}(x)D_{x_i}D_{x_j} u(x) + \\sum_{j=1}^n b_{j}(x)D_{x_j}u(x) + c(x)u(x) + d(x) = 0. Podemos reescrever essa equa\u00e7\u00e3o na forma (Lu)(x) = \\sum_{i,j}^n a_{ij}(x)D_{x_i}D_{x_j} u(x) + \\sum_{j=1}^n b_{j}(x)D_{x_j}u(x) + c(x)u(x) e Lu = -d . Dizemos que L \u00e9 um operador (no espa\u00e7o de fun\u00e7\u00f5es de classe C^k para o espa\u00e7o de fun\u00e7\u00f5es cont\u00ednuas). no caso ele \u00e9 linear de ordem 2. Podemos generalizar para um de ordem k qualquer. Seja L um operador diferencial linear de ordem k cujos coeficientes est\u00e3o definidos em \\mathbb{R}^n . Suponha que \\{u_m\\}_{m=1}^{+\\infty} \u00e9 um conjunto de fun\u00e7\u00f5es k vezes continuamente diferenci\u00e1veis satisfazendo Lu = 0, e que \\{\\alpha_m\\}_{m=1}^{+\\infty} \u00e9 uma sequ\u00eancia tal que u(x) = \\sum_{m=1}^{+\\infty} \\alpha_m u_m(x) \u00e9 convergente e k vezes diferenci\u00e1vel termo a t . Ent\u00e3o u tamb\u00e9m \u00e9 solu\u00e7\u00e3o. Condi\u00e7\u00f5es de Contorno e Iniciais Observe que para EDPs, estamos interessados em definir uma fun\u00e7\u00e3o em uma regi\u00e3o em um espa\u00e7o de dimens\u00e3o pelo menos 2. Por isso, n\u00e3o \u00e9 mais suficiente atribuir uma condi\u00e7\u00e3o inicial ao problema, como se fazia aos problemas de EDO, para determinar uma solu\u00e7\u00e3o. Nesse caso, substitu\u00edmos os extremos do intervalo pelo fronteira do conjunto que queremos estudar. Nesse caso, o problema \u00e9 dito problema de contorno . \u00c9 comum fixar, por exemplo u(x,0) = f(x) e u(x,1) = g(x) , em que a segunda vari\u00e1vel \u00e9 o tempo.","title":"Defini\u00e7\u00f5es iniciais"},{"location":"edp/introduction/#definicoes-iniciais","text":"Uma equa\u00e7\u00e3o diferencial parcial (EDP) ou partial differential equation (PDE) \u00e9 uma equa\u00e7\u00e3o que envolve duas ou mais vari\u00e1veis independentes e derivadas parciais de uma fun\u00e7\u00e3o que depende dessas vari\u00e1veis. De forma bem geral, \u00e9 uma equa\u00e7\u00e3o com forma F\\left(x_1, \\dots, x_n, u, \\frac{\\partial u}{\\partial x_1}, \\dots, \\frac{\\partial u}{\\partial x_n}, \\dots, \\frac{\\partial^k u}{\\partial x_{i_1} \\dots \\partial x_{i_k}}, \\dots, \\frac{\\partial^k u}{\\partial x_n^k}\\right) = 0, em que u = u(x_1, ..., x_n) e na express\u00e3o aparece a k -\u00e9sima derivada de u com respeito a k vari\u00e1veis x_{i_1}, \\dots, x_{i_k} . A fun\u00e7\u00e3o u \u00e9 em geral desconhecida e assumidamente de classe C^k , isto \u00e9, k vezes continuamente diferenci\u00e1vel. Denotamos tamb\u00e9m, \\nabla^2 u := \\frac{\\partial u}{\\partial x_1^2} + \\dots + \\frac{\\partial u}{\\partial x_n^2} = \\Delta u, o operador de Laplace .","title":"Defini\u00e7\u00f5es iniciais"},{"location":"edp/introduction/#exemplos","text":"Equa\u00e7\u00e3o da Onda : \\Delta u = \\frac{1}{c^2}\\frac{\\partial^2 u}{\\partial t^2}, em que c mede a velocidade da propaga\u00e7\u00e3o da onda. Nesse exemplo temos n vari\u00e1veis espaciais x_1, \\dots, x_n e uma vari\u00e1vel temporal t . Podemos definir F\\left(x_1, \\dots, x_n, t, u, \\frac{\\partial^2 u}{\\partial x_1^2}, \\dots, \\frac{\\partial^2 u}{\\partial x_n^2}, \\frac{\\partial^2 u}{\\partial t^2}\\right) = c^2\\Delta u - u_{tt} Equa\u00e7\u00e3o do calor : \\Delta u = \\frac{1}{k}\\frac{\\partial u}{\\partial t}, Equa\u00e7\u00e3o de Laplace \\Delta u = 0 As solu\u00e7\u00f5es dessa equa\u00e7\u00e3o s\u00e3o as fun\u00e7\u00f5es harm\u00f4nicas . Equa\u00e7\u00e3o de Schr\u00f6dinger iu_t + \\Delta u = 0 \u00e9 a fun\u00e7\u00e3o que governa a fun\u00e7\u00e3o de onda no sistema mec\u00e2nico qu\u00e2ntico.","title":"Exemplos"},{"location":"edp/introduction/#classificacao-de-uma-edp","text":"Ordem: Dada pela ordem parcial de maior ordem que ocorre na equa\u00e7\u00e3o. Nos exemplos anteriores, as equa\u00e7\u00f5es tem ordem 2. Linear: Ela \u00e9 dita linear se \u00e9 de primeiro grau em u e em todas suas derivadas parciais. Caso contr\u00e1rio ela \u00e9 dita n\u00e3o linear . Por exemplo, uma EDP linear de ordem 1 \u00e9 dada pela express\u00e3o (veja as diferentes express\u00f5es para derivada parcial ) \\sum_{j=1}^n a_j(x) D_{x_j} u + b(x) u + c(x) = 0 Homog\u00eanea: Os termos das vari\u00e1veis independentes s\u00e3o nulos, isto \u00e9, as vari\u00e1veis independentes n\u00e3o aparecem na express\u00e3o da fun\u00e7\u00e3o F . Parte principal: A parte da equa\u00e7\u00e3o que cont\u00e9m as derivadas de maior ordem. Se uma equa\u00e7\u00e3o n\u00e3o linear possui parte principal linear, ela \u00e9 dita semilinear .","title":"Classifica\u00e7\u00e3o de uma EDP"},{"location":"edp/introduction/#exemplos_1","text":"Equa\u00e7\u00e3o telegrafo u_{tt} + 2du_t - u_{xx} = 0, \u00e9 uma equa\u00e7\u00e3o linear de ordem 2. Equa\u00e7\u00e3o de beam u_{tt} + u_{xxxx} = 0 \u00e9 uma equa\u00e7\u00e3o linear de ordem 4 Equa\u00e7\u00e3o da onda (n\u00e3o linear) u_{tt} - \\Delta u + f(u) = 0, em que f \u00e9 uma fun\u00e7\u00e3o n\u00e3o linear de u . Observe que a parte principal \u00e9 u_{tt} - \\Delta u que \u00e9 linear e, portanto, a equa\u00e7\u00e3o \u00e9 semilinear. Exemplo: Suponha que u e v s\u00e3o duas fun\u00e7\u00f5es solu\u00e7\u00e3o da equa\u00e7\u00e3o (linear de ordem 1) \\frac{\\partial u}{\\partial x} + xu = 0 Seja z := \\alpha u + \\beta v , em que \\alpha, \\beta \\in \\mathbb{R} . Assim, \\frac{\\partial z}{\\partial x} + xz = \\alpha\\frac{\\partial u}{\\partial x} + \\beta\\frac{\\partial v}{\\partial x} + \\alpha x u + \\beta x v = \\alpha\\left(\\frac{\\partial u}{\\partial x} + xu\\right) + \\beta\\left(\\frac{\\partial v}{\\partial x} + xv\\right) = 0, o que implica que z tamb\u00e9m \u00e9 solu\u00e7\u00e3o. Essa propriedade n\u00e3o \u00e9 exclusividade do exemplo acima. Na verdade, vale para toda equa\u00e7\u00e3o linear, isto \u00e9, combina\u00e7\u00e3o linear de solu\u00e7\u00f5es \u00e9 solu\u00e7\u00e3o em problemas lineares.","title":"Exemplos"},{"location":"edp/introduction/#principio-da-superposicao","text":"Considere uma EDP linear de ordem 2: \\sum_{i,j}^n a_{ij}(x)D_{x_i}D_{x_j} u(x) + \\sum_{j=1}^n b_{j}(x)D_{x_j}u(x) + c(x)u(x) + d(x) = 0. Podemos reescrever essa equa\u00e7\u00e3o na forma (Lu)(x) = \\sum_{i,j}^n a_{ij}(x)D_{x_i}D_{x_j} u(x) + \\sum_{j=1}^n b_{j}(x)D_{x_j}u(x) + c(x)u(x) e Lu = -d . Dizemos que L \u00e9 um operador (no espa\u00e7o de fun\u00e7\u00f5es de classe C^k para o espa\u00e7o de fun\u00e7\u00f5es cont\u00ednuas). no caso ele \u00e9 linear de ordem 2. Podemos generalizar para um de ordem k qualquer. Seja L um operador diferencial linear de ordem k cujos coeficientes est\u00e3o definidos em \\mathbb{R}^n . Suponha que \\{u_m\\}_{m=1}^{+\\infty} \u00e9 um conjunto de fun\u00e7\u00f5es k vezes continuamente diferenci\u00e1veis satisfazendo Lu = 0, e que \\{\\alpha_m\\}_{m=1}^{+\\infty} \u00e9 uma sequ\u00eancia tal que u(x) = \\sum_{m=1}^{+\\infty} \\alpha_m u_m(x) \u00e9 convergente e k vezes diferenci\u00e1vel termo a t . Ent\u00e3o u tamb\u00e9m \u00e9 solu\u00e7\u00e3o.","title":"Princ\u00edpio da Superposi\u00e7\u00e3o"},{"location":"edp/introduction/#condicoes-de-contorno-e-iniciais","text":"Observe que para EDPs, estamos interessados em definir uma fun\u00e7\u00e3o em uma regi\u00e3o em um espa\u00e7o de dimens\u00e3o pelo menos 2. Por isso, n\u00e3o \u00e9 mais suficiente atribuir uma condi\u00e7\u00e3o inicial ao problema, como se fazia aos problemas de EDO, para determinar uma solu\u00e7\u00e3o. Nesse caso, substitu\u00edmos os extremos do intervalo pelo fronteira do conjunto que queremos estudar. Nesse caso, o problema \u00e9 dito problema de contorno . \u00c9 comum fixar, por exemplo u(x,0) = f(x) e u(x,1) = g(x) , em que a segunda vari\u00e1vel \u00e9 o tempo.","title":"Condi\u00e7\u00f5es de Contorno e Iniciais"},{"location":"edp/characteristics/characteristics/","text":"O M\u00e9todo das Caracter\u00edsticas Considere a Equa\u00e7\u00e3o Diferencial Parcial (EDP) F\\left(\\frac{\\partial u}{\\partial x_1}, \\dots, u, \\frac{\\partial u}{\\partial x_n}, x_1, \\dots, x_n\\right) = F(x, u, \\nabla u) = F(x, u, Du) = 0, definida em um conjunto U . Al\u00e9m disso, suponha que u = g na fronteira de U , em que g \u00e9 dada fun\u00e7\u00e3o suave. Descri\u00e7\u00e3o do m\u00e9todo A ideia geral desse m\u00e9todo \u00e9 transformar a EDP em um sistema de EDOs, em que temos uma teoria de resolu\u00e7\u00e3o bem estabelecida. Nisso, vamos construir curvas da superf\u00edcie formada por u e integrar nessas curvas. Seja \\gamma(s) = (a_1(s), \\dots, a_n(s)) essa curva (definida em U ). Assumindo que u \u00e9 duas vezes continuamente diferenci\u00e1vel, defina z(s) := u(\\gamma(s)). Tamb\u00e9m defina p(s) = \\nabla u(\\gamma(s)) = (u_{x_1}(\\gamma(s)), \\dots, u_{x_n}(\\gamma(s))) . Temos que (Regra da Cadeia) \\frac{d}{ds}p_i(s) = \\sum_{j=1}^n u_{x_ix_j}(\\gamma(s))\\frac{d}{ds}a_j(s). Voltando \u00e0 EDP F(x, u, Du) = 0 , derivando com respeito a x_i , \\sum_{j=1}^n F_{u_j} (Du, u, x)u_{x_j x_i} + F_z(Du, u, x)u_{x_i} + F_{x_i}(Du, u, x) = 0. Vamos usar essa express\u00e3o para remover as segundas derivadas de u (que s\u00e3o em geral complicadas de se encontrar) Para isso, definimos \\frac{d}{ds}a_i(s) = F_{p_i}(p(s), z(s), \\gamma(s)), i = 1, \\dots n, isto \u00e9, estamos definindo uma curva a partir de sua fun\u00e7\u00e3o tangente. Assumindo isso e avaliando a express\u00e3o em \\gamma(s) , obtemos que u_{x_i}(\\gamma(s)) = p_i(s) , logo \\sum_{j=1}^n F_{p_j} (p(s), z(s), \\gamma(s))u_{x_j x_i}(\\gamma(s)) + F_z(p(s), z(s), \\gamma(s))p_i(s) + F_{x_i}(p(s), z(s), \\gamma(s)) = 0. Usando a express\u00e3o de \\frac{d}{ds}p_i(s) dada mais acima, teremos que \\frac{d}{ds}p_i(s) = - F_{x_i}(p(s), z(s), \\gamma(s)) - F_z(p(s), z(s), \\gamma(s))p_i(s), o que nos d\u00e1 uma EDO para a fun\u00e7\u00e3o p(s) = \\nabla u(\\gamma(s)) . Al\u00e9m disso, diferenciando z obtemos \\frac{d}{ds}z(s) = \\sum_{j=1}^n u_{x_j}(\\gamma(s))\\frac{d}{ds}a_j(s) = \\sum_{j=1}^n p_j(s)F_{p_j}(p(s), z(s), \\gamma(s)). Isso nos reduz a um sistema de EDOs: \\begin{cases} \\dot{p}(s) = - D_x F(p(s), z(s), \\gamma(s)) - D_z F(p(s), z(s), \\gamma(s))p(s) \\\\ \\dot{z}(s) = D_p F(p(s), z(s), \\gamma(s))\\cdot p(s) \\\\ \\dot{\\gamma}(s) = D_pF(p(s), z(s), \\gamma(s)), \\end{cases} em que D \u00e9 a derivada (no caso vetorial, mas voc\u00ea pode pensar indiv\u00edduo a ind\u00edviduo usando as express\u00f5es derivadas acima). Al\u00e9m disso, ao longo da curva \\gamma(s) , F(p(s), z(s), \\gamma(s)) = 0, pela pr\u00f3pria defini\u00e7\u00e3o da F . F \u00e9 linear Considere F(Du, u, x) = b(x)\\cdot Du(x) + c(x)u(x) = 0 , isto \u00e9, o caso linear. Ao longo das curvas caracter\u00edsticas, F(p,z,x) = b(x)\\cdot p + c(x)z . Assim, \\begin{cases} \\dot{p}(s) = - D_x F(p(s), z(s), \\gamma(s)) - D_z F(p(s), z(s), \\gamma(s))p(s) \\\\ \\dot{z}(s) = D_p F(p(s), z(s), \\gamma(s))\\cdot p(s) = b(\\gamma(s))\\cdot p(s) = -c(\\gamma(s))z(s) \\\\ \\dot{\\gamma}(s) = D_p(p(s), z(s), \\gamma(s)) = b(\\gamma(s)), \\end{cases} Com isso, mesmo sem saber p , ainda conseguimos derivar z , o que simplifica bastante o problema. Condi\u00e7\u00f5es de fronteira Anteriormente, definimos um sistema de equa\u00e7\u00f5es diferenciais para resolver u(x) . Todavia, esse sistema admite infinitas solu\u00e7\u00f5es quando n\u00e3o especificado uma condi\u00e7\u00e3o inicial. Para isso, tome x_0 na fronteira de U , onde sabemos que u = g . Em geral, assumimos que essa fronteira fica no plano {x_n = 0} pr\u00f3ximo de x_0 . Como assim? Suponha que estamos com U \\subseteq \\mathbb{R}^2 e que a solu\u00e7\u00e3o seja dada pela fun\u00e7\u00e3o u(x,t) . Estamos dizendo que se \\gamma(0) = (x_0, 0) , sendo \\gamma a curva caracter\u00edstica. Suponha que U \\subseteq \\mathbb{R}^4 e que a solu\u00e7\u00e3o seja dada pela fun\u00e7\u00e3o u(x_1, \\dots, x_4) . Estamos dizendo que se \\gamma(0) = (x^{1}_0, x^{2}_0, x^{3}_0, 0) . Quando temos uma vari\u00e1vel temporal, em geral denotada po t , dizemos que ela sempre come\u00e7a em 0, uma forma de \"padronizar\". Sugiro o livro do Lawrence, se\u00e7\u00e3o 3.2.3 para uma demonstra\u00e7\u00e3o de que essa suposi\u00e7\u00e3o faz sentido. Dado um x_0 , falta agora definir p(0) = p_0, z(0) = z_0, \\gamma(0) = (x_0,0). Est\u00e1 claro que z_0 = u(\\gamma(0)) = u(x_0, 0) = g(x_0) . Al\u00e9m disso, u(x_1, \\dots, x_{n-1}, 0) = g(x_1, \\dots, x_{n-1}) na vizinhan\u00e7a de x_0 e, portanto, podemos diferenciar para obter u_{x_i}(x_0,0) = g_{x_i}(x_0), \\text{ para } i = 1,\\dots, n-1. Dessa fora, (p_0)^i = g_{x_i}(x_0) para cada i . Para determinar (p_0)^n , usamos a rela\u00e7\u00e3o dada por F , isto \u00e9, F(p_0, z_0, x_0) = 0, por defini\u00e7\u00e3o. As rela\u00e7\u00f5es de z(0) e p(0) s\u00e3o chamadas de condi\u00e7\u00f5es de compatibilidade . Note que pode n\u00e3o existir ou pode n\u00e3o ser \u00fanico a solu\u00e7\u00e3o de p_0 atrav\u00e9s da equa\u00e7\u00e3o F = 0 . Exist\u00eancia local de solu\u00e7\u00f5es Dado y = (y_1, \\dots, y_{n-1}, 0) , queremos resolver \\begin{cases} \\dot{p}(s) = - D_x F(p(s), z(s), \\gamma(s)) - D_z F(p(s), z(s), \\gamma(s))p(s) \\\\ \\dot{z}(s) = D_p F(p(s), z(s), \\gamma(s))\\cdot p(s) = b(\\gamma(s))\\cdot p(s) = -c(\\gamma(s))z(s) \\\\ \\dot{\\gamma}(s) = D_p(p(s), z(s), \\gamma(s)) = b(\\gamma(s)), \\end{cases} com \\gamma(0) = y, p(0) = p_0, z(0) = z_0 , com as express\u00f5es de compatibilidade derivadas acima. Lema (Uma aplica\u00e7\u00e3o do Teorema da Fun\u00e7\u00e3o Inversa): Assuma que F_{p_n}(p_0, z_0, x_0) \\neq 0 . Ent\u00e3o existe I \\ni 0 \\subseteq \\mathbb{R} , uma vizinhan\u00e7a W de x_0 na fronteira de U e uma vizinhan\u00e7a V de x_0 em \\mathbb{R}^n tal que para cada x \\in V , existe um \u00fanico s \\in I, y \\in W tal que x = \\gamma(y,s) = \\gamma(y_1, \\dots, y_{n-1},s). Essa lema \u00e9 uma consequ\u00eancia do Teorema da Fun\u00e7\u00e3o Inversa. N\u00e3o se preocupe tanto com a demonstra\u00e7\u00e3o. Mas a ideia \u00e9 que se provarmos que \\det D\\gamma(x_0, 0) \\neq 0 , valer\u00e1 a invertibilidade que estamos propondo, isto \u00e9, um mapa x \\mapsto (y,s) . Assim, seja y = y(x) e s = s(x) de forma que x = \\gamma(y,s) (que exite pelo que o lema prova). Assim, obtemos o seguinte Teorema: Teorema da Exist\u00eancia Local A fun\u00e7\u00e3o u(x) := z(y(x), s(x)) \u00e9 solu\u00e7\u00e3o para a EDP F(Du(x), u(x), x) = 0, em que x \\in V e u(x) = g(x) para x \\in \\partial U \\cap V , lembrando que \\partial U \u00e9 a fronteira de U . Leis da Conserva\u00e7\u00e3o Considere o problema da lei de conserva\u00e7\u00e3o para a dimens\u00e3o 1 u_t + [f(u)]_x = 0, x \\in \\mathbb{R}, t > 0 \\\\ u(x,0) = \\phi(x). Como nem sempre temos solu\u00e7\u00f5es diferenci\u00e1veis para u , temos que relaxar um pouco nossa defini\u00e7\u00e3o de solu\u00e7\u00e3o, e para isso introduzimos as solu\u00e7\u00f5es fracas . Solu\u00e7\u00e3o fraca (ou integral) Lembre que um conjunto em \\mathbb{R}^n \u00e9 compacto quando \u00e9 fechado e limitado. Uma fun\u00e7\u00e3o tem suporte compacto quando existe um compacto \\Lambda tal que para todo x \\in \\mathbb{R}^n / \\Lambda a fun\u00e7\u00e3o se anula. Definimos uma solu\u00e7\u00e3o fraca u quando \\int_0^{+\\infty}\\int_{-\\infty}^{+\\infty} [uv_t + f(u)v_x] \\, dx \\, dt + \\int_{-\\infty}^{+\\infty} \\phi(x) v(x,0) \\, dx = 0, para todas as fun\u00e7\u00f5es infinitamente diferenci\u00e1veis definidas em um conjunto compacto v (que chamamos de fun\u00e7\u00e3o teste ). Assim, a suavidade \u00e9 transferida para a fun\u00e7\u00e3o v . De forma equivalente, \\int_0^{+\\infty}\\int_{-\\infty}^{+\\infty} v[u + f(u)_x] \\, dx \\, dt = 0, Teorema: Se u \u00e9 uma solu\u00e7\u00e3o forte (no sentido de ser k vezes continuamente diferenci\u00e1vel), ent\u00e3o u ser\u00e1 uma solu\u00e7\u00e3o fraca. Como v \u00e9 um solu\u00e7\u00e3o que \u00e9 nula para um valor suficientemente grande e o integrando \u00e9 zero na solu\u00e7\u00e3o, ent\u00e3o a integral converge e, em particular, ser\u00e1 zero. Claro que precisamos primeiro mostrar a equival\u00eancia acima usando Integral por partes. Agora suponha que u \u00e9 uma fun\u00e7\u00e3o n\u00e3o cont\u00ednua em uma curva x = \\xi(t) , mas u \u00e9 suave em ambos os lados da curva (pensando em \\mathbb{R}^2 ). Denotamos u^{+}(x,t) para o limite de u quando se aproxima de (x,t) pela direita e u^{-}(x,t) pela esquerda. Vamos mostrar que existe uma rela\u00e7\u00e3o entre \\xi(t) , u^{-} , e u^+ . Teorema: Se u \u00e9 uma solu\u00e7\u00e3o fraca com a descontinuidade mencionada acima, ent\u00e3o, \\frac{f(u^{-}) - f(u^{+})}{u^{-} - u^{+}} = \\xi'(t) na curva de descontinuidade. Chamamos \\xi '(t) de velocidade da curva de descontinuidade . O denominador e o numerador s\u00e3o chamados de saltos . Essa condi\u00e7\u00e3o \u00e9 chamada de Condi\u00e7\u00e3o de Salto Rankine-Hugoniot . Nas imagens voc\u00ea confere um exemplo de quando u n\u00e3o \u00e9 cont\u00ednua da Equa\u00e7\u00e3o de Berger.","title":"O M\u00e9todo das Caracter\u00edsticas"},{"location":"edp/characteristics/characteristics/#o-metodo-das-caracteristicas","text":"Considere a Equa\u00e7\u00e3o Diferencial Parcial (EDP) F\\left(\\frac{\\partial u}{\\partial x_1}, \\dots, u, \\frac{\\partial u}{\\partial x_n}, x_1, \\dots, x_n\\right) = F(x, u, \\nabla u) = F(x, u, Du) = 0, definida em um conjunto U . Al\u00e9m disso, suponha que u = g na fronteira de U , em que g \u00e9 dada fun\u00e7\u00e3o suave.","title":"O M\u00e9todo das Caracter\u00edsticas"},{"location":"edp/characteristics/characteristics/#descricao-do-metodo","text":"A ideia geral desse m\u00e9todo \u00e9 transformar a EDP em um sistema de EDOs, em que temos uma teoria de resolu\u00e7\u00e3o bem estabelecida. Nisso, vamos construir curvas da superf\u00edcie formada por u e integrar nessas curvas. Seja \\gamma(s) = (a_1(s), \\dots, a_n(s)) essa curva (definida em U ). Assumindo que u \u00e9 duas vezes continuamente diferenci\u00e1vel, defina z(s) := u(\\gamma(s)). Tamb\u00e9m defina p(s) = \\nabla u(\\gamma(s)) = (u_{x_1}(\\gamma(s)), \\dots, u_{x_n}(\\gamma(s))) . Temos que (Regra da Cadeia) \\frac{d}{ds}p_i(s) = \\sum_{j=1}^n u_{x_ix_j}(\\gamma(s))\\frac{d}{ds}a_j(s). Voltando \u00e0 EDP F(x, u, Du) = 0 , derivando com respeito a x_i , \\sum_{j=1}^n F_{u_j} (Du, u, x)u_{x_j x_i} + F_z(Du, u, x)u_{x_i} + F_{x_i}(Du, u, x) = 0. Vamos usar essa express\u00e3o para remover as segundas derivadas de u (que s\u00e3o em geral complicadas de se encontrar) Para isso, definimos \\frac{d}{ds}a_i(s) = F_{p_i}(p(s), z(s), \\gamma(s)), i = 1, \\dots n, isto \u00e9, estamos definindo uma curva a partir de sua fun\u00e7\u00e3o tangente. Assumindo isso e avaliando a express\u00e3o em \\gamma(s) , obtemos que u_{x_i}(\\gamma(s)) = p_i(s) , logo \\sum_{j=1}^n F_{p_j} (p(s), z(s), \\gamma(s))u_{x_j x_i}(\\gamma(s)) + F_z(p(s), z(s), \\gamma(s))p_i(s) + F_{x_i}(p(s), z(s), \\gamma(s)) = 0. Usando a express\u00e3o de \\frac{d}{ds}p_i(s) dada mais acima, teremos que \\frac{d}{ds}p_i(s) = - F_{x_i}(p(s), z(s), \\gamma(s)) - F_z(p(s), z(s), \\gamma(s))p_i(s), o que nos d\u00e1 uma EDO para a fun\u00e7\u00e3o p(s) = \\nabla u(\\gamma(s)) . Al\u00e9m disso, diferenciando z obtemos \\frac{d}{ds}z(s) = \\sum_{j=1}^n u_{x_j}(\\gamma(s))\\frac{d}{ds}a_j(s) = \\sum_{j=1}^n p_j(s)F_{p_j}(p(s), z(s), \\gamma(s)). Isso nos reduz a um sistema de EDOs: \\begin{cases} \\dot{p}(s) = - D_x F(p(s), z(s), \\gamma(s)) - D_z F(p(s), z(s), \\gamma(s))p(s) \\\\ \\dot{z}(s) = D_p F(p(s), z(s), \\gamma(s))\\cdot p(s) \\\\ \\dot{\\gamma}(s) = D_pF(p(s), z(s), \\gamma(s)), \\end{cases} em que D \u00e9 a derivada (no caso vetorial, mas voc\u00ea pode pensar indiv\u00edduo a ind\u00edviduo usando as express\u00f5es derivadas acima). Al\u00e9m disso, ao longo da curva \\gamma(s) , F(p(s), z(s), \\gamma(s)) = 0, pela pr\u00f3pria defini\u00e7\u00e3o da F .","title":"Descri\u00e7\u00e3o do m\u00e9todo"},{"location":"edp/characteristics/characteristics/#f-e-linear","text":"Considere F(Du, u, x) = b(x)\\cdot Du(x) + c(x)u(x) = 0 , isto \u00e9, o caso linear. Ao longo das curvas caracter\u00edsticas, F(p,z,x) = b(x)\\cdot p + c(x)z . Assim, \\begin{cases} \\dot{p}(s) = - D_x F(p(s), z(s), \\gamma(s)) - D_z F(p(s), z(s), \\gamma(s))p(s) \\\\ \\dot{z}(s) = D_p F(p(s), z(s), \\gamma(s))\\cdot p(s) = b(\\gamma(s))\\cdot p(s) = -c(\\gamma(s))z(s) \\\\ \\dot{\\gamma}(s) = D_p(p(s), z(s), \\gamma(s)) = b(\\gamma(s)), \\end{cases} Com isso, mesmo sem saber p , ainda conseguimos derivar z , o que simplifica bastante o problema.","title":"F \u00e9 linear"},{"location":"edp/characteristics/characteristics/#condicoes-de-fronteira","text":"Anteriormente, definimos um sistema de equa\u00e7\u00f5es diferenciais para resolver u(x) . Todavia, esse sistema admite infinitas solu\u00e7\u00f5es quando n\u00e3o especificado uma condi\u00e7\u00e3o inicial. Para isso, tome x_0 na fronteira de U , onde sabemos que u = g . Em geral, assumimos que essa fronteira fica no plano {x_n = 0} pr\u00f3ximo de x_0 . Como assim? Suponha que estamos com U \\subseteq \\mathbb{R}^2 e que a solu\u00e7\u00e3o seja dada pela fun\u00e7\u00e3o u(x,t) . Estamos dizendo que se \\gamma(0) = (x_0, 0) , sendo \\gamma a curva caracter\u00edstica. Suponha que U \\subseteq \\mathbb{R}^4 e que a solu\u00e7\u00e3o seja dada pela fun\u00e7\u00e3o u(x_1, \\dots, x_4) . Estamos dizendo que se \\gamma(0) = (x^{1}_0, x^{2}_0, x^{3}_0, 0) . Quando temos uma vari\u00e1vel temporal, em geral denotada po t , dizemos que ela sempre come\u00e7a em 0, uma forma de \"padronizar\". Sugiro o livro do Lawrence, se\u00e7\u00e3o 3.2.3 para uma demonstra\u00e7\u00e3o de que essa suposi\u00e7\u00e3o faz sentido. Dado um x_0 , falta agora definir p(0) = p_0, z(0) = z_0, \\gamma(0) = (x_0,0). Est\u00e1 claro que z_0 = u(\\gamma(0)) = u(x_0, 0) = g(x_0) . Al\u00e9m disso, u(x_1, \\dots, x_{n-1}, 0) = g(x_1, \\dots, x_{n-1}) na vizinhan\u00e7a de x_0 e, portanto, podemos diferenciar para obter u_{x_i}(x_0,0) = g_{x_i}(x_0), \\text{ para } i = 1,\\dots, n-1. Dessa fora, (p_0)^i = g_{x_i}(x_0) para cada i . Para determinar (p_0)^n , usamos a rela\u00e7\u00e3o dada por F , isto \u00e9, F(p_0, z_0, x_0) = 0, por defini\u00e7\u00e3o. As rela\u00e7\u00f5es de z(0) e p(0) s\u00e3o chamadas de condi\u00e7\u00f5es de compatibilidade . Note que pode n\u00e3o existir ou pode n\u00e3o ser \u00fanico a solu\u00e7\u00e3o de p_0 atrav\u00e9s da equa\u00e7\u00e3o F = 0 .","title":"Condi\u00e7\u00f5es de fronteira"},{"location":"edp/characteristics/characteristics/#existencia-local-de-solucoes","text":"Dado y = (y_1, \\dots, y_{n-1}, 0) , queremos resolver \\begin{cases} \\dot{p}(s) = - D_x F(p(s), z(s), \\gamma(s)) - D_z F(p(s), z(s), \\gamma(s))p(s) \\\\ \\dot{z}(s) = D_p F(p(s), z(s), \\gamma(s))\\cdot p(s) = b(\\gamma(s))\\cdot p(s) = -c(\\gamma(s))z(s) \\\\ \\dot{\\gamma}(s) = D_p(p(s), z(s), \\gamma(s)) = b(\\gamma(s)), \\end{cases} com \\gamma(0) = y, p(0) = p_0, z(0) = z_0 , com as express\u00f5es de compatibilidade derivadas acima. Lema (Uma aplica\u00e7\u00e3o do Teorema da Fun\u00e7\u00e3o Inversa): Assuma que F_{p_n}(p_0, z_0, x_0) \\neq 0 . Ent\u00e3o existe I \\ni 0 \\subseteq \\mathbb{R} , uma vizinhan\u00e7a W de x_0 na fronteira de U e uma vizinhan\u00e7a V de x_0 em \\mathbb{R}^n tal que para cada x \\in V , existe um \u00fanico s \\in I, y \\in W tal que x = \\gamma(y,s) = \\gamma(y_1, \\dots, y_{n-1},s). Essa lema \u00e9 uma consequ\u00eancia do Teorema da Fun\u00e7\u00e3o Inversa. N\u00e3o se preocupe tanto com a demonstra\u00e7\u00e3o. Mas a ideia \u00e9 que se provarmos que \\det D\\gamma(x_0, 0) \\neq 0 , valer\u00e1 a invertibilidade que estamos propondo, isto \u00e9, um mapa x \\mapsto (y,s) . Assim, seja y = y(x) e s = s(x) de forma que x = \\gamma(y,s) (que exite pelo que o lema prova). Assim, obtemos o seguinte Teorema:","title":"Exist\u00eancia local de solu\u00e7\u00f5es"},{"location":"edp/characteristics/characteristics/#teorema-da-existencia-local","text":"A fun\u00e7\u00e3o u(x) := z(y(x), s(x)) \u00e9 solu\u00e7\u00e3o para a EDP F(Du(x), u(x), x) = 0, em que x \\in V e u(x) = g(x) para x \\in \\partial U \\cap V , lembrando que \\partial U \u00e9 a fronteira de U .","title":"Teorema da Exist\u00eancia Local"},{"location":"edp/characteristics/characteristics/#leis-da-conservacao","text":"Considere o problema da lei de conserva\u00e7\u00e3o para a dimens\u00e3o 1 u_t + [f(u)]_x = 0, x \\in \\mathbb{R}, t > 0 \\\\ u(x,0) = \\phi(x). Como nem sempre temos solu\u00e7\u00f5es diferenci\u00e1veis para u , temos que relaxar um pouco nossa defini\u00e7\u00e3o de solu\u00e7\u00e3o, e para isso introduzimos as solu\u00e7\u00f5es fracas .","title":"Leis da Conserva\u00e7\u00e3o"},{"location":"edp/characteristics/characteristics/#solucao-fraca-ou-integral","text":"Lembre que um conjunto em \\mathbb{R}^n \u00e9 compacto quando \u00e9 fechado e limitado. Uma fun\u00e7\u00e3o tem suporte compacto quando existe um compacto \\Lambda tal que para todo x \\in \\mathbb{R}^n / \\Lambda a fun\u00e7\u00e3o se anula. Definimos uma solu\u00e7\u00e3o fraca u quando \\int_0^{+\\infty}\\int_{-\\infty}^{+\\infty} [uv_t + f(u)v_x] \\, dx \\, dt + \\int_{-\\infty}^{+\\infty} \\phi(x) v(x,0) \\, dx = 0, para todas as fun\u00e7\u00f5es infinitamente diferenci\u00e1veis definidas em um conjunto compacto v (que chamamos de fun\u00e7\u00e3o teste ). Assim, a suavidade \u00e9 transferida para a fun\u00e7\u00e3o v . De forma equivalente, \\int_0^{+\\infty}\\int_{-\\infty}^{+\\infty} v[u + f(u)_x] \\, dx \\, dt = 0, Teorema: Se u \u00e9 uma solu\u00e7\u00e3o forte (no sentido de ser k vezes continuamente diferenci\u00e1vel), ent\u00e3o u ser\u00e1 uma solu\u00e7\u00e3o fraca. Como v \u00e9 um solu\u00e7\u00e3o que \u00e9 nula para um valor suficientemente grande e o integrando \u00e9 zero na solu\u00e7\u00e3o, ent\u00e3o a integral converge e, em particular, ser\u00e1 zero. Claro que precisamos primeiro mostrar a equival\u00eancia acima usando Integral por partes. Agora suponha que u \u00e9 uma fun\u00e7\u00e3o n\u00e3o cont\u00ednua em uma curva x = \\xi(t) , mas u \u00e9 suave em ambos os lados da curva (pensando em \\mathbb{R}^2 ). Denotamos u^{+}(x,t) para o limite de u quando se aproxima de (x,t) pela direita e u^{-}(x,t) pela esquerda. Vamos mostrar que existe uma rela\u00e7\u00e3o entre \\xi(t) , u^{-} , e u^+ . Teorema: Se u \u00e9 uma solu\u00e7\u00e3o fraca com a descontinuidade mencionada acima, ent\u00e3o, \\frac{f(u^{-}) - f(u^{+})}{u^{-} - u^{+}} = \\xi'(t) na curva de descontinuidade. Chamamos \\xi '(t) de velocidade da curva de descontinuidade . O denominador e o numerador s\u00e3o chamados de saltos . Essa condi\u00e7\u00e3o \u00e9 chamada de Condi\u00e7\u00e3o de Salto Rankine-Hugoniot . Nas imagens voc\u00ea confere um exemplo de quando u n\u00e3o \u00e9 cont\u00ednua da Equa\u00e7\u00e3o de Berger.","title":"Solu\u00e7\u00e3o fraca (ou integral)"},{"location":"edp/existence_theorem/existence_theorem/","text":"Teorema de Picard-Lindelof Este \u00e9 um belo teorema de exst\u00eancia e unicidade de equa\u00e7\u00f5es diferenciais ordin\u00e1rias da forma y'(t) = f(t, y(t)), y(t_0) = y_0. import numpy as np from scipy.integrate import solve_ivp , quad import matplotlib.pyplot as plt import seaborn as sns sns . set () O teorema basicamente consiste em construir um operador da seguinte forma: L(y(t)) = y_0 + \\int_{t_0}^t f(s,y(s)) \\, ds, em que L \u00e9 um operador funcional. Note que se se L(y(t)) = y(t) para todo t , teremos resolvido o problema, dado que y(t) = y_0 + \\int_{t_0}^t f(s,y(s)) \\, ds, \u00e9 uma solu\u00e7\u00e3o para y' = f(t,y) . Nesse caso, dizemos que queremos encontrar um ponto fixo do operador L . Se demonstrarmos que L possui um ponto fixo e ele \u00e9 \u00fanico em algum intevalo suficientemente pequeno, ent\u00e3o teremos demonstrado nosso teorema. Um teorema de exist\u00eancia (e unicidade local tamb\u00e9m) \u00e9 o Teorema de Banach . Para isso, bastaria mostrar que L leva fun\u00e7\u00f5es de um espa\u00e7o no mesmo espa\u00e7o e que \u00e9 uma contra\u00e7\u00e3o, isto \u00e9, intuitivamente, se a cada vez que aplicamos L , as fun\u00e7\u00f5es se aproximam e continuam no mesmo espa\u00e7o, ent\u00e3o vai existir um ponto fixo para ele. Nossa ideia aqui n\u00e3o \u00e9 demonstrar o Teorema, mas sim mostrar a intera\u00e7\u00e3o de Picard que \u00e9 escrita da seguinte forma: y_{k+1}(t) = y_0 + \\int_{0}^t f(s,y_k(s)) \\, ds, \\\\ y_0(t) = y_0. O teorema de ponto fixo de Banach nos mostra que y_{k} converge para a solu\u00e7\u00e3o y para todo ponto t em uma vizinhan\u00e7a de 0. Usaremos a fun\u00e7\u00e3o de quadratura para integrar a fun\u00e7\u00e3o f . a = ( 1 , 2 ) ( 1 , * a ) (1, 1, 2) def picard_iteration ( fun , t_eval , y0 , yk , args ): pieces = [ quad ( func = fun , a = t_eval [ i ], b = t_eval [ i + 1 ], args = (( yk [ i ] + yk [ i + 1 ]) / 2 , * args ) )[ 0 ] for i in range ( len ( t_eval ) - 1 )] pieces . insert ( 0 , 0 ) yk = np . cumsum ( np . array ( pieces )) + y0 return yk Considere o problema y' = ay, \\\\ y(0) = y_0 # defining the function f = lambda t , y , a : a * y a = 2 # time of definition upper = 1 n = 100 t = np . linspace ( 0 , upper , n ) Esta \u00e9 a solu\u00e7\u00e3o usando Runge-Kutta. solution = solve_ivp ( fun = f , t_span = ( 0 , upper ), t_eval = t , y0 = ( 1 ,), method = 'RK45' , args = ( a ,)) Aqui n\u00f3s apresentamos as primeiras itera\u00e7\u00f5es de Picard. y0 = np . ones ( n ) y1 = picard_iteration ( fun = f , t_eval = t , y0 = ( 1 ,), yk = y0 , args = ( a ,)) y2 = picard_iteration ( fun = f , t_eval = t , y0 = ( 1 ,), yk = y1 , args = ( a ,)) y3 = picard_iteration ( fun = f , t_eval = t , y0 = ( 1 ,), yk = y2 , args = ( a ,)) y4 = picard_iteration ( fun = f , t_eval = t , y0 = ( 1 ,), yk = y3 , args = ( a ,)) plt . figure ( figsize = ( 10 , 6 )) plt . title ( 'Picard iterations to approximate a function' ) plt . plot ( solution . t , solution . y [ 0 ], label = 'Solution y(t)' ) plt . plot ( solution . t , y0 , label = r '$y_0(t)$' ) plt . plot ( solution . t , y1 , label = r '$y_1(t)$' ) plt . plot ( solution . t , y2 , label = r '$y_2(t)$' ) plt . plot ( solution . t , y3 , label = r '$y_3(t)$' ) plt . plot ( solution . t , y4 , label = r '$y_4(t)$' ) plt . legend () plt . show ()","title":"Teorema de Picard-Lindelof"},{"location":"edp/existence_theorem/existence_theorem/#teorema-de-picard-lindelof","text":"Este \u00e9 um belo teorema de exst\u00eancia e unicidade de equa\u00e7\u00f5es diferenciais ordin\u00e1rias da forma y'(t) = f(t, y(t)), y(t_0) = y_0. import numpy as np from scipy.integrate import solve_ivp , quad import matplotlib.pyplot as plt import seaborn as sns sns . set () O teorema basicamente consiste em construir um operador da seguinte forma: L(y(t)) = y_0 + \\int_{t_0}^t f(s,y(s)) \\, ds, em que L \u00e9 um operador funcional. Note que se se L(y(t)) = y(t) para todo t , teremos resolvido o problema, dado que y(t) = y_0 + \\int_{t_0}^t f(s,y(s)) \\, ds, \u00e9 uma solu\u00e7\u00e3o para y' = f(t,y) . Nesse caso, dizemos que queremos encontrar um ponto fixo do operador L . Se demonstrarmos que L possui um ponto fixo e ele \u00e9 \u00fanico em algum intevalo suficientemente pequeno, ent\u00e3o teremos demonstrado nosso teorema. Um teorema de exist\u00eancia (e unicidade local tamb\u00e9m) \u00e9 o Teorema de Banach . Para isso, bastaria mostrar que L leva fun\u00e7\u00f5es de um espa\u00e7o no mesmo espa\u00e7o e que \u00e9 uma contra\u00e7\u00e3o, isto \u00e9, intuitivamente, se a cada vez que aplicamos L , as fun\u00e7\u00f5es se aproximam e continuam no mesmo espa\u00e7o, ent\u00e3o vai existir um ponto fixo para ele. Nossa ideia aqui n\u00e3o \u00e9 demonstrar o Teorema, mas sim mostrar a intera\u00e7\u00e3o de Picard que \u00e9 escrita da seguinte forma: y_{k+1}(t) = y_0 + \\int_{0}^t f(s,y_k(s)) \\, ds, \\\\ y_0(t) = y_0. O teorema de ponto fixo de Banach nos mostra que y_{k} converge para a solu\u00e7\u00e3o y para todo ponto t em uma vizinhan\u00e7a de 0. Usaremos a fun\u00e7\u00e3o de quadratura para integrar a fun\u00e7\u00e3o f . a = ( 1 , 2 ) ( 1 , * a ) (1, 1, 2) def picard_iteration ( fun , t_eval , y0 , yk , args ): pieces = [ quad ( func = fun , a = t_eval [ i ], b = t_eval [ i + 1 ], args = (( yk [ i ] + yk [ i + 1 ]) / 2 , * args ) )[ 0 ] for i in range ( len ( t_eval ) - 1 )] pieces . insert ( 0 , 0 ) yk = np . cumsum ( np . array ( pieces )) + y0 return yk Considere o problema y' = ay, \\\\ y(0) = y_0 # defining the function f = lambda t , y , a : a * y a = 2 # time of definition upper = 1 n = 100 t = np . linspace ( 0 , upper , n ) Esta \u00e9 a solu\u00e7\u00e3o usando Runge-Kutta. solution = solve_ivp ( fun = f , t_span = ( 0 , upper ), t_eval = t , y0 = ( 1 ,), method = 'RK45' , args = ( a ,)) Aqui n\u00f3s apresentamos as primeiras itera\u00e7\u00f5es de Picard. y0 = np . ones ( n ) y1 = picard_iteration ( fun = f , t_eval = t , y0 = ( 1 ,), yk = y0 , args = ( a ,)) y2 = picard_iteration ( fun = f , t_eval = t , y0 = ( 1 ,), yk = y1 , args = ( a ,)) y3 = picard_iteration ( fun = f , t_eval = t , y0 = ( 1 ,), yk = y2 , args = ( a ,)) y4 = picard_iteration ( fun = f , t_eval = t , y0 = ( 1 ,), yk = y3 , args = ( a ,)) plt . figure ( figsize = ( 10 , 6 )) plt . title ( 'Picard iterations to approximate a function' ) plt . plot ( solution . t , solution . y [ 0 ], label = 'Solution y(t)' ) plt . plot ( solution . t , y0 , label = r '$y_0(t)$' ) plt . plot ( solution . t , y1 , label = r '$y_1(t)$' ) plt . plot ( solution . t , y2 , label = r '$y_2(t)$' ) plt . plot ( solution . t , y3 , label = r '$y_3(t)$' ) plt . plot ( solution . t , y4 , label = r '$y_4(t)$' ) plt . legend () plt . show ()","title":"Teorema de Picard-Lindelof"},{"location":"infestatistica/SufficientStatistics/","text":"Estat\u00edsticas Suficientes A ideia por tra\u015b da estat\u00edstica \u00e9, como o nome diz, ser suficiente. Uma estat\u00edstica \u00e9 uma fun\u00e7\u00e3o das vari\u00e1veis aleat\u00f3rias, como, por exemplo, T = r(X_1, ..., X_n) . M\u00e9dia amostral, vari\u00e2ncia amostral, valor m\u00e1ximo, s\u00e3o todos exemplos. Imagine que temos um problema como o seguinte: Vamos imaginar que um estat\u00edstico d\u00e1 um trabalho para seu estagi\u00e1rio para organizar os dados de forma mais eficiente poss\u00edvel, enquanto ele pensa no modelo. O estagi\u00e1rio de forma muito ing\u00eanua cria uma lista em seu Jupyter Notebook e salva o notebook com os dados na sua lista. Depois ele salva num arquivo .txt e vai para casa tranquilo que o trabalho acabou mais cedo. Ser\u00e1 que era necess\u00e1rio ter salvo todos os dados? O estat\u00edstico no dia seuinte diz que n\u00e3o! E manda o estagi\u00e1rio estudar novamente estat\u00edstica. Ele disse para estudar Estat\u00edstica Suficientes . Defini\u00e7\u00e3o Unidimensional Seja X_1, ..., X_n uma amostra aleat\u00f3ria de distribui\u00e7\u00e3o indexada pelo par\u00e2metro \\theta . Suponha que para todo valor que \\theta assume e para todo valor que T assume (vamos chamar de t = r(x_1, ..., x_n) , nesse caso j\u00e1 observamos o processo e calculamos t ), a distribui\u00e7\u00e3o conjunta condicional de X_1, ...., X_n dado T=t e \\theta , isto \u00e9, dado que voc\u00ea observou uma estat\u00edstica (a m\u00e9dia de temperaturas, por exemplo) depende apenas de t , mas n\u00e3o de \\theta . Isso significa que a distribui\u00e7\u00e3o \u00e9 constante para todos os valores de \\theta . Chamaremos essa estat\u00edstica T de suficiente para \\theta . Obs.: Para quem estudou fun\u00e7\u00f5es mensur\u00e1veis, podemos definir estat\u00edstica como fun\u00e7\u00e3o mensur\u00e1vel dos dados. Seja (\\mathbb{T}, \\mathbb{C}) um espa\u00e7o mensur\u00e1vel tal que \\mathbb{C} cont\u00e9m todos os conjuntos unit\u00e1rios. Se T : \\mathbb{X} \\to \\mathbb{T} \u00e9 mensu\u00e1vel, ent\u00e3o \u00e9 uma estat\u00edstica. Seja \\mathbb{P}_0 uma familia param\u00e9trica de distribui\u00e7\u00f5es em (\\mathbb{X}, \\mathbb{B}) . Seja (\\Omega, \\tau) um espa\u00e7o dos par\u00e2metros e \\Theta: \\mathbb{P}_0 \\to \\Omega um par\u00e2metro. Seja T uma estat\u00edstica. Ela \u00e9 suficiente para \\Theta se para toda priori \\mu_{\\Theta} , existem vers\u00f5es da posteriori \\mu_{\\Theta|X} e \\mu_{\\Theta|T} tal que \\forall B \\in \\tau, \\mu_{\\Theta|X}(B|x) = \\mu_{\\Theta|T}(B|T(x)) , quase certamente convergente para [\\mu_X] onde \\mu_X \u00e9 distribui\u00e7\u00e3o marginal de X . Crit\u00e9rio de Fatoriza\u00e7\u00e3o Teorema atribu\u00eddo a Neyman-Fisher. X_1,...X_n amostra aleat\u00f3ria com pdf ou pmf f(x|\\theta) , onde \\theta \u00e9 desconhecido. Uma estat\u00edstica T = r(X) para \\theta \u00e9 suficiente se, e somente se, a distribui\u00e7\u00e3o conjunta f_n(x|\\theta) pode ser fatorada para todo valor x \\in \\mathbb{R}^n da seguinte forma: f_n(x|\\theta) = u(x)v[r(x), \\theta] Onde u e v s\u00e3o n\u00e3o negativas, u n\u00e3o depende de \\theta e v s\u00f3 depende dos dados atrav\u00e9s da estat\u00edstica. Isto \u00e9, n\u00e3o adianta voc\u00ea encontrar qualquer fun\u00e7\u00e3o de x , tem que encontrar a estat\u00edstica T em v . Estat\u00edsticas Conjuntas Suficientes Suponha que para cada \\theta , vetor, e cada valor das estat\u00edsticas (T_1, ..., T_k) = (t_1, ..., t_k) a distribui\u00e7\u00e3o conjunta condicional dos dados dadas as estat\u00edsticas n\u00e3o depende de \\theta . Veja que nesse caso, a diferen\u00e7a \u00e9 que condiciono em k estat\u00edsticas, k \\geq 1 . Crit\u00e9rio de Fatoriza\u00e7\u00e3o Sejam r_1, ..., r_k fun\u00e7\u00f5es de n vari\u00e1veis. A estat\u00edsticas T_i = r_i(X) s\u00e3o estat\u00edsticas suficientes conjuntas para \\theta se, e somene se, a pdf conjunta f_n(x|\\theta) pode ser fatorado como f_n(x|\\theta) = u(x)v[r_1(x), ..., r_k(x),\\theta], para todos os valores x \\in \\mathbb{R}^n e \\theta \\in \\Omega Obs.: Podemos mostrar que qualquer fun\u00e7\u00e3o injetiva de uma estat\u00edstica suficiente \u00e9 uma estat\u00edstica suficiente. Estat\u00edstica Suficiente M\u00ednima Estat\u00edstica de Ordem Considere uma amostra aleat\u00f3ria e a ordene. Diremos que a nova amostra, ordenada, \u00e9 uma estat\u00edstica de ordem. Observe que ela funciona como uma matrix de \"shifts\" que opera trocando as linhas do vetor de lugar. Por isso ela \u00e9 uma fun\u00e7\u00e3o. Essa estat\u00edstica \u00e9 sufciente conjunta para \\theta . O interessante que podemos ver isso dado que o produt\u00f3rio n\u00e3o importa a ordem. Estat\u00edstica Suficiente M\u00ednima \u00c9 uma estat\u00edstica T suficiente e, al\u00e9m disso, \u00e9 fun\u00e7\u00e3o de todas as outras estat\u00edsticas suficientes. MLE e Estat\u00edstica Suficiente Seja T uma estat\u00edstica suficiente para \\theta . Ent\u00e3o o estimador de m\u00e1xima verossimilhan\u00e7a \\hat{\\theta} depende das observa\u00e7\u00f5es somente atrav\u00e9s da estat\u00edstica T . Al\u00e9m disso, se \\hat{\\theta} \u00e9 suficiente, ent\u00e3o \u00e9 m\u00ednimo. Estat\u00edsticas Suficientes e Estimador de Bayes T = r(X) estat\u00edstica suficiente para \\theta . Ent\u00e3o todo estimador de Bayes \\hat{\\theta} depende nas observa\u00e7\u00f5es X_1, ..., X_n apenas atrav\u00e9s da estat\u00edstica T . Al\u00e9m do mais, se for suficiente, ser\u00e1 suficiente m\u00ednimo. Defini\u00e7\u00f5es Adicionais Considere uma amostra aleat\u00f3ria X_1,...,X_n Estat\u00edstica Completa Seja t = T(X) estat\u00edstica. Se E[g(T(X))|\\theta] = 0, \\forall \\theta \\implies P[g(T(X)) = 0] = 1, ent\u00e3o ela \u00e9 dita completa. Estat\u00edstica Ancillary Suponha que queremos estimar \\theta e f_n(x|\\theta) seja a pdf conjunta. Seja A(X) uma estat\u00edstica. Se a sua distribui\u00e7\u00e3o n\u00e3o depende de \\theta , ent\u00e3o ser\u00e1 uma estat\u00edstica ancillary (auxiliar?) Por exemplo, se X_1, X_2 \\sim N(\\mu, \\sigma^2) e \\mu \u00e9 desconhecido, temos que X_1 - X_2 \\sim N(0, 2\\sigma^2) \u00e9 uma estat\u00edstica auxiliar. Melhorando um Estimador Suponha que temos uma amostra aleat\u00f3ria X = (X_1, ..., X_n) cuja pdf \u00e9 f(x|\\theta) e \\theta \\in \\Omega desconhecido, tal que queremos estimar h(\\theta) para alguma fun\u00e7\u00e3o h . Seja Z = g(X_1, ..., X_n) . E_{\\theta}(Z) = \\int_{-\\infty}^{\\infty}...\\int_{-\\infty}^{\\infty} g(x)f_n(x|\\theta)dx_1, ..., dx_n Para cada estimado \\delta(X) e para todo valor de \\theta , definimos o MSE (Erro M\u00e9dio Quadr\u00e1tico) R(\\theta, \\delta) = E_{\\theta}\\{[\\delta(X) - h(\\theta)]^2\\} Quando n\u00e3o atribu\u00edmos uma priori para \\theta , ent\u00e3o queremos encontrar um estimador para que o MSE seja pequeno para v\u00e1rios valores de \\theta . Seja T uma estat\u00edstica suficiente conhecida. Definimos \\delta_0(T) = E_{\\theta}\\{\\delta(X)|T\\} \\overset{1}{=} E\\{\\delta(X)|T\\} (1) Agora, por que podemos chamar \\delta_0 de estimador se depende de \\theta ? Como T \u00e9 uma estat\u00edstica suficiente, a distribui\u00e7\u00e3o condicionada em T e em \\theta da amostra X_1, ..., X_n n\u00e3o depende de \\theta !!! Em particular o valor esperado do estimador \\delta(T) . Logo, como esse valor esperado n\u00e3o depende de \\theta , podemos dizer sim que ele \u00e9 um estimador. Teorema Rao - Blackwell Teorema 7.9.1 do livro. Seja \\delta(X) um estimador e T uma estat\u00edstica suficiente para \\theta . O estimador \\delta_0(T) definido acima, para todo valor \\theta \\in \\Omega \u00e9: R(\\theta, \\delta_0) \\leq R(\\theta, \\delta), isto \u00e9, \u00e9 um estimador com menor erro quadr\u00e1tico m\u00e9dio (MSE). Em particular se R(\\theta, \\delta) < \\infty , a desigualdade se torna estrita, a menos que \\delta(X) seja um afun\u00e7\u00e3o de T , isto \u00e9, se \\delta(X) n\u00e3o for fun\u00e7\u00e3o de T , ent\u00e3o a desigualdade ser\u00e1 estrita. Por desigualdade estrita entenda < . Obs.: Chamamos o processo de melhorar um estimador com esse teorema de \"Rao-Blackwelliation\". Obs.2: Podemos generalizar um pouco mais. Para isso, pesquise sobre Conjuntos Convexos e sobre Fun\u00e7\u00f5es Convexas . Em um conjunto convexo, se a nossa fun\u00e7\u00e3o de perda n\u00e3o for o MSE, mas for uma fun\u00e7\u00e3o convexa, o teorema tamb\u00e9m valer\u00e1. Uma suposi\u00e7\u00e3o interessante que o Livro n\u00e3o imp\u00f5e \u00e9 que E[||\\delta(X)||) < \\infty . Inadmissibilidade Suponha que R(\\theta, \\delta) \u00e9 MSE. O estimador \\delta \u00e9 inadimiss\u00edvel se existe outro estimador \\delta_0 tal que R(\\theta, \\delta_0) \\leq R(\\theta, \\delta) para todo valor de \\theta e existe a desigualdade estrita em, pelo menos um valor de \\theta . Dizemos nesse caso que \\delta_0 domina o estimador \\delta . Um estimador \\delta_0 \u00e9 admiss\u00edvel se n\u00e3o existe outro estimador que o domine.","title":"Estat\u00edsticas Suficientes"},{"location":"infestatistica/SufficientStatistics/#estatisticas-suficientes","text":"A ideia por tra\u015b da estat\u00edstica \u00e9, como o nome diz, ser suficiente. Uma estat\u00edstica \u00e9 uma fun\u00e7\u00e3o das vari\u00e1veis aleat\u00f3rias, como, por exemplo, T = r(X_1, ..., X_n) . M\u00e9dia amostral, vari\u00e2ncia amostral, valor m\u00e1ximo, s\u00e3o todos exemplos. Imagine que temos um problema como o seguinte: Vamos imaginar que um estat\u00edstico d\u00e1 um trabalho para seu estagi\u00e1rio para organizar os dados de forma mais eficiente poss\u00edvel, enquanto ele pensa no modelo. O estagi\u00e1rio de forma muito ing\u00eanua cria uma lista em seu Jupyter Notebook e salva o notebook com os dados na sua lista. Depois ele salva num arquivo .txt e vai para casa tranquilo que o trabalho acabou mais cedo. Ser\u00e1 que era necess\u00e1rio ter salvo todos os dados? O estat\u00edstico no dia seuinte diz que n\u00e3o! E manda o estagi\u00e1rio estudar novamente estat\u00edstica. Ele disse para estudar Estat\u00edstica Suficientes .","title":"Estat\u00edsticas Suficientes"},{"location":"infestatistica/SufficientStatistics/#definicao-unidimensional","text":"Seja X_1, ..., X_n uma amostra aleat\u00f3ria de distribui\u00e7\u00e3o indexada pelo par\u00e2metro \\theta . Suponha que para todo valor que \\theta assume e para todo valor que T assume (vamos chamar de t = r(x_1, ..., x_n) , nesse caso j\u00e1 observamos o processo e calculamos t ), a distribui\u00e7\u00e3o conjunta condicional de X_1, ...., X_n dado T=t e \\theta , isto \u00e9, dado que voc\u00ea observou uma estat\u00edstica (a m\u00e9dia de temperaturas, por exemplo) depende apenas de t , mas n\u00e3o de \\theta . Isso significa que a distribui\u00e7\u00e3o \u00e9 constante para todos os valores de \\theta . Chamaremos essa estat\u00edstica T de suficiente para \\theta . Obs.: Para quem estudou fun\u00e7\u00f5es mensur\u00e1veis, podemos definir estat\u00edstica como fun\u00e7\u00e3o mensur\u00e1vel dos dados. Seja (\\mathbb{T}, \\mathbb{C}) um espa\u00e7o mensur\u00e1vel tal que \\mathbb{C} cont\u00e9m todos os conjuntos unit\u00e1rios. Se T : \\mathbb{X} \\to \\mathbb{T} \u00e9 mensu\u00e1vel, ent\u00e3o \u00e9 uma estat\u00edstica. Seja \\mathbb{P}_0 uma familia param\u00e9trica de distribui\u00e7\u00f5es em (\\mathbb{X}, \\mathbb{B}) . Seja (\\Omega, \\tau) um espa\u00e7o dos par\u00e2metros e \\Theta: \\mathbb{P}_0 \\to \\Omega um par\u00e2metro. Seja T uma estat\u00edstica. Ela \u00e9 suficiente para \\Theta se para toda priori \\mu_{\\Theta} , existem vers\u00f5es da posteriori \\mu_{\\Theta|X} e \\mu_{\\Theta|T} tal que \\forall B \\in \\tau, \\mu_{\\Theta|X}(B|x) = \\mu_{\\Theta|T}(B|T(x)) , quase certamente convergente para [\\mu_X] onde \\mu_X \u00e9 distribui\u00e7\u00e3o marginal de X .","title":"Defini\u00e7\u00e3o Unidimensional"},{"location":"infestatistica/SufficientStatistics/#criterio-de-fatorizacao","text":"Teorema atribu\u00eddo a Neyman-Fisher. X_1,...X_n amostra aleat\u00f3ria com pdf ou pmf f(x|\\theta) , onde \\theta \u00e9 desconhecido. Uma estat\u00edstica T = r(X) para \\theta \u00e9 suficiente se, e somente se, a distribui\u00e7\u00e3o conjunta f_n(x|\\theta) pode ser fatorada para todo valor x \\in \\mathbb{R}^n da seguinte forma: f_n(x|\\theta) = u(x)v[r(x), \\theta] Onde u e v s\u00e3o n\u00e3o negativas, u n\u00e3o depende de \\theta e v s\u00f3 depende dos dados atrav\u00e9s da estat\u00edstica. Isto \u00e9, n\u00e3o adianta voc\u00ea encontrar qualquer fun\u00e7\u00e3o de x , tem que encontrar a estat\u00edstica T em v .","title":"Crit\u00e9rio de Fatoriza\u00e7\u00e3o"},{"location":"infestatistica/SufficientStatistics/#estatisticas-conjuntas-suficientes","text":"Suponha que para cada \\theta , vetor, e cada valor das estat\u00edsticas (T_1, ..., T_k) = (t_1, ..., t_k) a distribui\u00e7\u00e3o conjunta condicional dos dados dadas as estat\u00edsticas n\u00e3o depende de \\theta . Veja que nesse caso, a diferen\u00e7a \u00e9 que condiciono em k estat\u00edsticas, k \\geq 1 .","title":"Estat\u00edsticas Conjuntas Suficientes"},{"location":"infestatistica/SufficientStatistics/#criterio-de-fatorizacao_1","text":"Sejam r_1, ..., r_k fun\u00e7\u00f5es de n vari\u00e1veis. A estat\u00edsticas T_i = r_i(X) s\u00e3o estat\u00edsticas suficientes conjuntas para \\theta se, e somene se, a pdf conjunta f_n(x|\\theta) pode ser fatorado como f_n(x|\\theta) = u(x)v[r_1(x), ..., r_k(x),\\theta], para todos os valores x \\in \\mathbb{R}^n e \\theta \\in \\Omega Obs.: Podemos mostrar que qualquer fun\u00e7\u00e3o injetiva de uma estat\u00edstica suficiente \u00e9 uma estat\u00edstica suficiente.","title":"Crit\u00e9rio de Fatoriza\u00e7\u00e3o"},{"location":"infestatistica/SufficientStatistics/#estatistica-suficiente-minima","text":"","title":"Estat\u00edstica Suficiente M\u00ednima"},{"location":"infestatistica/SufficientStatistics/#estatistica-de-ordem","text":"Considere uma amostra aleat\u00f3ria e a ordene. Diremos que a nova amostra, ordenada, \u00e9 uma estat\u00edstica de ordem. Observe que ela funciona como uma matrix de \"shifts\" que opera trocando as linhas do vetor de lugar. Por isso ela \u00e9 uma fun\u00e7\u00e3o. Essa estat\u00edstica \u00e9 sufciente conjunta para \\theta . O interessante que podemos ver isso dado que o produt\u00f3rio n\u00e3o importa a ordem.","title":"Estat\u00edstica de Ordem"},{"location":"infestatistica/SufficientStatistics/#estatistica-suficiente-minima_1","text":"\u00c9 uma estat\u00edstica T suficiente e, al\u00e9m disso, \u00e9 fun\u00e7\u00e3o de todas as outras estat\u00edsticas suficientes.","title":"Estat\u00edstica Suficiente M\u00ednima"},{"location":"infestatistica/SufficientStatistics/#mle-e-estatistica-suficiente","text":"Seja T uma estat\u00edstica suficiente para \\theta . Ent\u00e3o o estimador de m\u00e1xima verossimilhan\u00e7a \\hat{\\theta} depende das observa\u00e7\u00f5es somente atrav\u00e9s da estat\u00edstica T . Al\u00e9m disso, se \\hat{\\theta} \u00e9 suficiente, ent\u00e3o \u00e9 m\u00ednimo.","title":"MLE e Estat\u00edstica Suficiente"},{"location":"infestatistica/SufficientStatistics/#estatisticas-suficientes-e-estimador-de-bayes","text":"T = r(X) estat\u00edstica suficiente para \\theta . Ent\u00e3o todo estimador de Bayes \\hat{\\theta} depende nas observa\u00e7\u00f5es X_1, ..., X_n apenas atrav\u00e9s da estat\u00edstica T . Al\u00e9m do mais, se for suficiente, ser\u00e1 suficiente m\u00ednimo.","title":"Estat\u00edsticas Suficientes e Estimador de Bayes"},{"location":"infestatistica/SufficientStatistics/#definicoes-adicionais","text":"Considere uma amostra aleat\u00f3ria X_1,...,X_n","title":"Defini\u00e7\u00f5es Adicionais"},{"location":"infestatistica/SufficientStatistics/#estatistica-completa","text":"Seja t = T(X) estat\u00edstica. Se E[g(T(X))|\\theta] = 0, \\forall \\theta \\implies P[g(T(X)) = 0] = 1, ent\u00e3o ela \u00e9 dita completa.","title":"Estat\u00edstica Completa"},{"location":"infestatistica/SufficientStatistics/#estatistica-ancillary","text":"Suponha que queremos estimar \\theta e f_n(x|\\theta) seja a pdf conjunta. Seja A(X) uma estat\u00edstica. Se a sua distribui\u00e7\u00e3o n\u00e3o depende de \\theta , ent\u00e3o ser\u00e1 uma estat\u00edstica ancillary (auxiliar?) Por exemplo, se X_1, X_2 \\sim N(\\mu, \\sigma^2) e \\mu \u00e9 desconhecido, temos que X_1 - X_2 \\sim N(0, 2\\sigma^2) \u00e9 uma estat\u00edstica auxiliar.","title":"Estat\u00edstica Ancillary"},{"location":"infestatistica/SufficientStatistics/#melhorando-um-estimador","text":"Suponha que temos uma amostra aleat\u00f3ria X = (X_1, ..., X_n) cuja pdf \u00e9 f(x|\\theta) e \\theta \\in \\Omega desconhecido, tal que queremos estimar h(\\theta) para alguma fun\u00e7\u00e3o h . Seja Z = g(X_1, ..., X_n) . E_{\\theta}(Z) = \\int_{-\\infty}^{\\infty}...\\int_{-\\infty}^{\\infty} g(x)f_n(x|\\theta)dx_1, ..., dx_n Para cada estimado \\delta(X) e para todo valor de \\theta , definimos o MSE (Erro M\u00e9dio Quadr\u00e1tico) R(\\theta, \\delta) = E_{\\theta}\\{[\\delta(X) - h(\\theta)]^2\\} Quando n\u00e3o atribu\u00edmos uma priori para \\theta , ent\u00e3o queremos encontrar um estimador para que o MSE seja pequeno para v\u00e1rios valores de \\theta . Seja T uma estat\u00edstica suficiente conhecida. Definimos \\delta_0(T) = E_{\\theta}\\{\\delta(X)|T\\} \\overset{1}{=} E\\{\\delta(X)|T\\} (1) Agora, por que podemos chamar \\delta_0 de estimador se depende de \\theta ? Como T \u00e9 uma estat\u00edstica suficiente, a distribui\u00e7\u00e3o condicionada em T e em \\theta da amostra X_1, ..., X_n n\u00e3o depende de \\theta !!! Em particular o valor esperado do estimador \\delta(T) . Logo, como esse valor esperado n\u00e3o depende de \\theta , podemos dizer sim que ele \u00e9 um estimador.","title":"Melhorando um Estimador"},{"location":"infestatistica/SufficientStatistics/#teorema-rao-blackwell","text":"Teorema 7.9.1 do livro. Seja \\delta(X) um estimador e T uma estat\u00edstica suficiente para \\theta . O estimador \\delta_0(T) definido acima, para todo valor \\theta \\in \\Omega \u00e9: R(\\theta, \\delta_0) \\leq R(\\theta, \\delta), isto \u00e9, \u00e9 um estimador com menor erro quadr\u00e1tico m\u00e9dio (MSE). Em particular se R(\\theta, \\delta) < \\infty , a desigualdade se torna estrita, a menos que \\delta(X) seja um afun\u00e7\u00e3o de T , isto \u00e9, se \\delta(X) n\u00e3o for fun\u00e7\u00e3o de T , ent\u00e3o a desigualdade ser\u00e1 estrita. Por desigualdade estrita entenda < . Obs.: Chamamos o processo de melhorar um estimador com esse teorema de \"Rao-Blackwelliation\". Obs.2: Podemos generalizar um pouco mais. Para isso, pesquise sobre Conjuntos Convexos e sobre Fun\u00e7\u00f5es Convexas . Em um conjunto convexo, se a nossa fun\u00e7\u00e3o de perda n\u00e3o for o MSE, mas for uma fun\u00e7\u00e3o convexa, o teorema tamb\u00e9m valer\u00e1. Uma suposi\u00e7\u00e3o interessante que o Livro n\u00e3o imp\u00f5e \u00e9 que E[||\\delta(X)||) < \\infty .","title":"Teorema Rao - Blackwell"},{"location":"infestatistica/SufficientStatistics/#inadmissibilidade","text":"Suponha que R(\\theta, \\delta) \u00e9 MSE. O estimador \\delta \u00e9 inadimiss\u00edvel se existe outro estimador \\delta_0 tal que R(\\theta, \\delta_0) \\leq R(\\theta, \\delta) para todo valor de \\theta e existe a desigualdade estrita em, pelo menos um valor de \\theta . Dizemos nesse caso que \\delta_0 domina o estimador \\delta . Um estimador \\delta_0 \u00e9 admiss\u00edvel se n\u00e3o existe outro estimador que o domine.","title":"Inadmissibilidade"},{"location":"infestatistica/info/","text":"Informa\u00e7\u00f5es Gerais Monitoria de Infer\u00eancia Estat\u00edstica correspondente ao per\u00edodo de 2020.2. Monitorias gravadas Notebooks : Voc\u00ea pode rodar esses notebooks tamb\u00e9m! Github Professor T\u00f3picos Nessa se\u00e7\u00e3o, os t\u00f3picos discutidos nas aulas s\u00e3o apresentados conjuntamente a exemplos. Todos os textos foram desenvolvidos em Jupyter Notebook , com a Linguagem de Programa\u00e7\u00e3o Python. Conceitos Introdut\u00f3rios Grandes Amostras Introdu\u00e7\u00e3o Priori e Posteriori Distribui\u00e7\u00e3o Amostral Estimadores Distribui\u00e7\u00f5es Conjugada e Estimador de Bayes Estimador de M\u00e1xima Verossimilhan\u00e7a Estat\u00edsticas Suficientes Intervalos de Confian\u00e7a An\u00e1lise Bayesiana da Normal Estimadores n\u00e3o enviesados Informa\u00e7\u00e3o de Fisher Testes de Hip\u00f3tese Teste de Hip\u00f3teses Teste de Hip\u00f3teses II Exemplos B\u00e1sicos Testes de Hip\u00f3tese Testes Uniformemente mais Poderosos Modelos Lineares Modelos Lineares Simples Exerc\u00edcios Os exerc\u00edcios resolvidos dispon\u00edveis est\u00e3o lincados ao t\u00edtulo da se\u00e7\u00e3o. A1 Se\u00e7\u00e3o T\u00edtulo Exerc\u00edcios 7.2 Priori e Posteriori 2,3,10 7.3 Fam\u00edlias Conjugadas 2,17,19,21 7.4 Estimador de Bayes 2,4,7,11,14 7.5 Estimador de M\u00e1xima Verossimilhan\u00e7a 1,4,9,10 7.6 Propriedades EMV 3,5,11,20,22,23 7.7 Estat\u00edstica Suficiente 4,7,13,16 7.8 Estat\u00edstica Suficiente Conjunta 3,8,12,16 7.9 Melhorando um Estimador 2,3,6,9,10 8.7 Estimadores n\u00e3o enviesados 4,6,11,13 8.8 Informa\u00e7\u00e3o de Fisher 5,7,10 8.1 Estimadores de Distribui\u00e7\u00f5es Amostrais 1,2,3,9 8.2 Distribui\u00e7\u00e3o Chi-Quadrada 4,7,10,13 - Exerc\u00edcios de Revis\u00e3o A1 - A2 Se\u00e7\u00e3o T\u00edtulo Exerc\u00edcios 8.3 Distribui\u00e7\u00e3o conjunta da m\u00e9dia e vari\u00e2ncia amostral 8 8.4 A distribui\u00e7\u00e3o t Derivar a distribui\u00e7\u00e3o 8.5 Intervalos de Confian\u00e7a 1,4,5,6 9.1 Problemas de Teste de Hip\u00f3teses 3,8,13,19,21 9.5 Teste t 4,5,8 9.6 Comparando as m\u00e9dias de Normais - 9.7 Teste F Derivar a distribui\u00e7\u00e3o e teste de raz\u00e3o de verossimilhan\u00e7as 11.1 M\u00e9todo de M\u00ednimos Quadrados 3 11.2 Regress\u00e3o 2,3,6,19 11.3 Infer\u00eancia Estat\u00edstica sobre Regress\u00e3o Linear Simples - - Exerc\u00edcios de Revis\u00e3o A2 - Resumos Probability and Statistics (Morris H. DeGroot) Cap\u00edtulo 7 Cap\u00edtulo 8 - parte 1 Cap\u00edtulo 8 - parte 2 Cap\u00edtulo 9 - Defini\u00e7\u00f5es Cap\u00edtulo 9 - Testes Documentos Adicionais Theory of Statistical Estimation (Ronald Fisher) : problema da estima\u00e7\u00e3o \u00e9 abordado e as t\u00e9cnicas apresentadas por Ronald Fisher. Ele se debru\u00e7a sobre estat\u00edsticas eficientes e suficientes. Mathematical Foudations Statistics (Ronald Fisher) : refer\u00eancia em estat\u00edstica com principais conceitos da mat\u00e9ria. Digital TextBook Statlect Bayesian x Frequentist Inference","title":"Estat\u00edstica"},{"location":"infestatistica/info/#informacoes-gerais","text":"Monitoria de Infer\u00eancia Estat\u00edstica correspondente ao per\u00edodo de 2020.2. Monitorias gravadas Notebooks : Voc\u00ea pode rodar esses notebooks tamb\u00e9m! Github Professor","title":"Informa\u00e7\u00f5es Gerais"},{"location":"infestatistica/info/#topicos","text":"Nessa se\u00e7\u00e3o, os t\u00f3picos discutidos nas aulas s\u00e3o apresentados conjuntamente a exemplos. Todos os textos foram desenvolvidos em Jupyter Notebook , com a Linguagem de Programa\u00e7\u00e3o Python. Conceitos Introdut\u00f3rios Grandes Amostras Introdu\u00e7\u00e3o Priori e Posteriori Distribui\u00e7\u00e3o Amostral Estimadores Distribui\u00e7\u00f5es Conjugada e Estimador de Bayes Estimador de M\u00e1xima Verossimilhan\u00e7a Estat\u00edsticas Suficientes Intervalos de Confian\u00e7a An\u00e1lise Bayesiana da Normal Estimadores n\u00e3o enviesados Informa\u00e7\u00e3o de Fisher Testes de Hip\u00f3tese Teste de Hip\u00f3teses Teste de Hip\u00f3teses II Exemplos B\u00e1sicos Testes de Hip\u00f3tese Testes Uniformemente mais Poderosos Modelos Lineares Modelos Lineares Simples","title":"T\u00f3picos"},{"location":"infestatistica/info/#exercicios","text":"Os exerc\u00edcios resolvidos dispon\u00edveis est\u00e3o lincados ao t\u00edtulo da se\u00e7\u00e3o.","title":"Exerc\u00edcios"},{"location":"infestatistica/info/#a1","text":"Se\u00e7\u00e3o T\u00edtulo Exerc\u00edcios 7.2 Priori e Posteriori 2,3,10 7.3 Fam\u00edlias Conjugadas 2,17,19,21 7.4 Estimador de Bayes 2,4,7,11,14 7.5 Estimador de M\u00e1xima Verossimilhan\u00e7a 1,4,9,10 7.6 Propriedades EMV 3,5,11,20,22,23 7.7 Estat\u00edstica Suficiente 4,7,13,16 7.8 Estat\u00edstica Suficiente Conjunta 3,8,12,16 7.9 Melhorando um Estimador 2,3,6,9,10 8.7 Estimadores n\u00e3o enviesados 4,6,11,13 8.8 Informa\u00e7\u00e3o de Fisher 5,7,10 8.1 Estimadores de Distribui\u00e7\u00f5es Amostrais 1,2,3,9 8.2 Distribui\u00e7\u00e3o Chi-Quadrada 4,7,10,13 - Exerc\u00edcios de Revis\u00e3o A1 -","title":"A1"},{"location":"infestatistica/info/#a2","text":"Se\u00e7\u00e3o T\u00edtulo Exerc\u00edcios 8.3 Distribui\u00e7\u00e3o conjunta da m\u00e9dia e vari\u00e2ncia amostral 8 8.4 A distribui\u00e7\u00e3o t Derivar a distribui\u00e7\u00e3o 8.5 Intervalos de Confian\u00e7a 1,4,5,6 9.1 Problemas de Teste de Hip\u00f3teses 3,8,13,19,21 9.5 Teste t 4,5,8 9.6 Comparando as m\u00e9dias de Normais - 9.7 Teste F Derivar a distribui\u00e7\u00e3o e teste de raz\u00e3o de verossimilhan\u00e7as 11.1 M\u00e9todo de M\u00ednimos Quadrados 3 11.2 Regress\u00e3o 2,3,6,19 11.3 Infer\u00eancia Estat\u00edstica sobre Regress\u00e3o Linear Simples - - Exerc\u00edcios de Revis\u00e3o A2 -","title":"A2"},{"location":"infestatistica/info/#resumos","text":"Probability and Statistics (Morris H. DeGroot) Cap\u00edtulo 7 Cap\u00edtulo 8 - parte 1 Cap\u00edtulo 8 - parte 2 Cap\u00edtulo 9 - Defini\u00e7\u00f5es Cap\u00edtulo 9 - Testes","title":"Resumos"},{"location":"infestatistica/info/#documentos-adicionais","text":"Theory of Statistical Estimation (Ronald Fisher) : problema da estima\u00e7\u00e3o \u00e9 abordado e as t\u00e9cnicas apresentadas por Ronald Fisher. Ele se debru\u00e7a sobre estat\u00edsticas eficientes e suficientes. Mathematical Foudations Statistics (Ronald Fisher) : refer\u00eancia em estat\u00edstica com principais conceitos da mat\u00e9ria. Digital TextBook Statlect Bayesian x Frequentist Inference","title":"Documentos Adicionais"},{"location":"infestatistica/BayesianAnalysisNormal/BayesianAnalysisNormal/","text":"An\u00e1lise Bayesiana de amostras da distribui\u00e7\u00e3o Normal Refer\u00eancias [1] Andrew Gelman. Bayesian Data Analysis. Parte I, Se\u00e7\u00e3o 3.3: essa refer\u00eancia usa a vari\u00e2ncia ao inv\u00e9s da precis\u00e3o. [2] Kevin P. Murphy. Conjugate Bayesian analysis of the Gaussian distribution: N\u00e3o muito simples, mas possui todas as contas. Precis\u00e3o da distribui\u00e7\u00e3o normal Definimos \\tau := 1/\\sigma^2 como a precis\u00e3o da distribui\u00e7\u00e3o normal. A fun\u00e7\u00e3o da densidade de probabilidade da distribui\u00e7\u00e3o normal f(x|\\mu,\\tau) \u00e9, -\\infty < x < \\infty : f(x|\\mu,\\tau) = \\left(\\frac{\\tau}{2\\pi}\\right)^{1/2}\\exp\\left[-\\frac{1}{2}\\tau(x-\\mu)^2\\right] Teorema Suponha que X_1,...,X_n \\overset{iid}{\\sim} N_2(\\mu, \\tau) , desconhecidos. Suponha que \\mu|\\tau \\sim N_2(\\mu_0, \\lambda_0 \\tau) \\tau \\sim \\text{Gamma}(\\alpha_0, \\beta_0) Ent\u00e3o a distribui\u00e7\u00e3o conjunta de \\mu e \\tau a posteriori \u00e9 dada por: \\mu|\\tau, X_1,...,X_n \\sim N_2(\\mu_1, \\lambda_1\\tau) \\tau|X_1,...,X_n \\sim \\text{Gamma}(\\alpha_1, \\beta_1), onde Par\u00e2metro Valor a posteriori do par\u00e2metro \\mu_1 \\frac{\\lambda_0 \\mu_0 + n\\bar{x}_n}{\\lambda_0 + n} \\lambda_1 \\lambda_0 + n \\alpha_1 \\alpha_0 + n/2 \\beta_1 \\beta_0 + s_n^2/2 + \\frac{n\\lambda_0(\\bar{x}_n - \\mu_0)^2}{2(\\lambda_0 + n)} Fam\u00edlia Normal-Gamma Sejam \\mu e \\tau vari\u00e1veis aleat\u00f3rias. Suponha que a distribui\u00e7\u00e3o condicional de \\mu dado \\tau \u00e9 normal com m\u00e9dia \\mu_0 e precis\u00e3o \\lambda_0 \\tau e que a distribui\u00e7\u00e3o marginal de \\tau seja gamma com par\u00e2metros \\alpha_0, \\beta_0 . Ent\u00e3o, falamos que a distribui\u00e7\u00e3o conjunta de \\mu e \\tau \u00e9 a distribui\u00e7\u00e3o normal-gamma com hiperpar\u00e2meteros \\mu_0, \\lambda_0, \\alpha_0 e \\beta_0 . A distribui\u00e7\u00e3o marginal da m\u00e9dia \\mu Suponha que a distribui\u00e7\u00e3o a priori \\mu e \\tau seja normal-gamma com hiperpar\u00e2metros \\mu_0, \\lambda_0, \\alpha_0 e \\beta_0 . Ent\u00e3o \\left(\\frac{\\lambda_0\\alpha_0}{\\beta_0}\\right)^{1/2}(\\mu - \\mu_0) tem a distribui\u00e7\u00e3o t com 2\\alpha_0 graus de liberdade. Nesse caso, se \\alpha_0 > 1/2 , E(\\mu) = \\mu_0 . Se \\alpha_0 > 1 , Var(\\mu) \\frac{\\beta_0}{\\lambda_0(\\alpha_0 - 1)} Obs.: As condi\u00e7\u00f5es sobre \\alpha_0 , vem da exist\u00eancia do k momento somente se o grau de liberdade da distribui\u00e7\u00e3o t \u00e9 maior do que k , isto \u00e9, 2\\alpha_0 > k . Compara\u00e7\u00e3o com Intervalos de Confian\u00e7a Podemos construir intervalos de confian\u00e7a para \\mu no mundo Bayesiano, pois ela \u00e9 uma vari\u00e1vel aleat\u00f3ria. Nesse caso, podemos fazer da seguinte maneira: \\begin{split} &P(-c < \\left(\\frac{\\lambda_0\\alpha_0}{\\beta_0}\\right)^{1/2}(\\mu - \\mu_0) < c) = \\gamma \\\\ &P(\\mu_0 - c\\left(\\frac{\\beta_0}{\\lambda_0\\alpha_0}\\right)^{1/2} < \\mu < \\mu_0 + c\\left(\\frac{\\beta_0}{\\lambda_0\\alpha_0}\\right)^{1/2}) &= \\gamma \\end{split} Ou seja, ganhamos um intervalo de confian\u00e7a de gra\u00e7a! Implementa\u00e7\u00e3o Quando temos dados, X_1, ..., X_n \\sim N(\\mu, \\sigma^2) , podemos fazer \\hat{\\mu} = \\bar{X}_n que \u00e9 uma pontual (s\u00f3 para um valor de \\mu ). Mas isso nem sempre \u00e9 o melhor, e \u00e0s vezes nem t\u00e3o prazeroso. Por isso precisamos trazer Bayes para nossa an\u00e1lise. import numpy as np from scipy.stats import norm , gamma , t import pandas as pd import matplotlib.pyplot as plt import seaborn as sns sns . set () Nesse exemplos, vamos estudar o exemplo Casas de Enfermagem em Novo M\u00e9xico , tema estudado por Howard L. Smith, Neill F. Piland e Nancy Fisher . Nesse trabalho explora os desafios da enfermagem na \u00e1rea rural, para que se mantenha a viabilidade. Vamos utilizar o banco de dados deles, que incluem as seguintes informa\u00e7\u00f5es: BED: n\u00fameros de camas na casa. MCDAYS: dias anuais de interna\u00e7\u00e3o m\u00e9dica (centenas) TDAYS: total anual de pacientes dias (centenas) PCREV: receita anual total de atendimento ao paciente (centenas) NSAL: sal\u00e1rio das enfermeiras anual (centenas) FEXP: despesas anuais com instala\u00e7\u00f5es (centenas) RURAL: 1, se rural, 0, se urbano Os dados se encontram nesse site . table = [] with open ( '../data/nursinghome.txt' , 'r' ) as f : line = f . readline () table . append ( line . split ()) line = f . readline () while line != '' : table . append ([ int ( i ) for i in line . split ()]) line = f . readline () nurse_df = pd . DataFrame ( data = table [ 1 :], columns = table [ 0 ], dtype = np . int ) Vamos considerar nesse exemplo a coluna MCDAYS, restrita \u00e0s casas urbanas, que denotaremos por X . Antes de observarmos os dados, vamos modelar o valor de X para cada casa como uma vari\u00e1vel aleat\u00f3ria normal com m\u00e9dia \\mu e precis\u00e3o \\tau . urban_nurse_df = nurse_df [ nurse_df . RURAL == 0 ] plt . bar ( x = range ( len ( urban_nurse_df )), height = urban_nurse_df . MCDAYS ) plt . title ( 'MCDAYS por casa urbana de enfermeiras' ) plt . show () Para calcular nossa priori, dever\u00edamos conversar com especialistas. Como n\u00e3o \u00e9 o caso, vamos usar as informa\u00e7\u00f5es de camas. Temos: print ( 'M\u00e9dia: {:.1f} , Desvio-Padr\u00e3o: {:.1f} ' . format ( np . mean ( urban_nurse_df . BED ), np . std ( urban_nurse_df . BED ))) M\u00e9dia: 111.4, Desvio-Padr\u00e3o: 42.3 Podemos supor, a priori, que a taxa de ocupa\u00e7\u00e3o \u00e9 de 50%. Logo, em um ano, podemos obter que os dias anuais de interna\u00e7\u00e3o m\u00e9dica s\u00e3o: media = 0.5 * 365 * np . mean ( urban_nurse_df . BED ) / 100 # unidade em centenas std = 0.5 * 365 * np . std ( urban_nurse_df . BED ) / 100 Agora precisamos mapear esses valores para os hiperpar\u00e2metros a priori \\alpha_0, \\beta_0, \\lambda_0, \\mu_0 . Vamos dibidir a vari\u00e2ncia obtida acima (usando o n\u00famero de camas), como incerteza, e dividiremos igualmente essa incerteza sobre a m\u00e9dia e a precis\u00e3o, isto \u00e9, Var(\\mu) = std^2/2 E(\\tau) = 1/(std^2/2) Escolhemos \\alpha_0 = 2 (arbitr\u00e1rio, mas prefer\u00edvel a ser pequeno, porque esse par\u00e2metro tem a interpreta\u00e7\u00e3o de ser o conhecimento sobre o valor. Logo E[\\tau] = \\alpha_0/\\beta_0 \\implies \\beta_0 = \\alpha_0/E[\\tau] E[\\mu] = \\mu_0 Var(\\mu) = \\frac{\\beta_0}{\\lambda_0(\\alpha_0 - 1)} \\implies \\lambda_0 = \\frac{\\beta_0}{Var(\\mu)(\\alpha_0 - 1)} alpha0 = 2 beta0 = alpha * std ** 2 / 2 mu0 = media lambda0 = beta * 2 / ( std ** 2 * ( alpha - 1 )) Agora conseguimos expressar nosso conhecimento a priori para construir o primeiro intervalo de confian\u00e7a. Vamos usar a express\u00e3o constru\u00edda anteriormente. Primeiro, vamos ver, numericamente, a distribui\u00e7\u00e3o de \\mu def draw_samples ( alpha0 , beta0 , lambda0 , mu0 , seed = 10000 ): r = np . random . RandomState ( seed ) tau = r . gamma ( shape = alpha0 , scale = 1 / beta0 , size = 100000 ) mu = r . normal ( mu0 , scale = np . sqrt ( 1 / ( lambda0 * tau ))) return mu , tau mu_samples , tau_samples = draw_samples ( alpha0 , beta0 , lambda0 , mu0 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) sns . histplot ( mu_samples , ax = ax [ 0 ]) sns . histplot ( tau_samples , ax = ax [ 1 ]) ax [ 0 ] . set_title ( r '$\\mu$ a priori' ) ax [ 1 ] . set_title ( r '$\\tau$ a priori' ) ax [ 0 ] . set_xlim (( 0 , 500 )) plt . show () ci_mu = ( np . quantile ( mu_samples , 0.025 ), np . quantile ( mu_samples , 0.975 )) print ( ci_mu ) (95.85458277508882, 309.0974651995673) Agora, vamos observar os dados e calcular nossa posteriori! Vamos atualizar como demonstrado no Teorema. Par\u00e2metro Valor a posteriori do par\u00e2metro \\mu_1 \\frac{\\lambda_0 \\mu_0 + n\\bar{x}_n}{\\lambda_0 + n} \\lambda_1 \\lambda_0 + n \\alpha_1 \\alpha_0 + n/2 \\beta_1 \\beta_0 + s_n^2/2 + \\frac{n\\lambda_0(\\bar{x}_n - \\mu_0)^2}{2(\\lambda_0 + n)} n = len ( urban_nurse_df ) mom1 = np . mean ( urban_nurse_df . MCDAYS ) mom2 = np . var ( urban_nurse_df . MCDAYS ) * n mu1 = ( lambda0 * mu0 + n * mom1 ) / ( lambda0 + n ) lambda1 = lambda0 + n alpha1 = alpha0 + n / 2 beta1 = beta0 + mom2 / 2 + ( n * lambda0 * ( mom1 - mu0 ) ** 2 ) / ( 2 * ( lambda0 + n )) mu_samples_post , tau_samples_post = draw_samples ( alpha1 , beta1 , lambda1 , mu1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) sns . histplot ( mu_samples_post , ax = ax [ 0 ]) sns . histplot ( tau_samples_post , ax = ax [ 1 ]) ax [ 0 ] . set_title ( r '$\\mu$ a posteriori' ) ax [ 1 ] . set_title ( r '$\\tau$ a posteriori' ) ax [ 0 ] . set_xlim (( 100 , 300 )) plt . show () ci_mu = ( np . quantile ( mu_samples_post , 0.025 ), np . quantile ( mu_samples_post , 0.975 )) print ( ci_mu ) (152.61993663355443, 215.58303707725233) Observe como o intervalo de confian\u00e7a est\u00e1 bem menor! Ou seja, os dados aumentaram nossa certeza sobre o par\u00e2metro. Vamos comparar os valores. fig , ax = plt . subplots ( figsize = ( 10 , 5 )) sns . histplot ( mu_samples_post , ax = ax , kde = True , stat = 'density' , label = 'Posteriori' ) sns . histplot ( mu_samples , ax = ax , kde = True , stat = 'density' , label = 'Priori' , color = 'red' ) ax . set_title ( r '$\\mu$' ) ax . set_xlim (( - 200 , 400 )) ax . legend () plt . show () Por fim, vamos comparar o estimador de Bayes com o de M\u00e1xima Verossimilhan\u00e7a: mle = mom1 eb = np . mean ( mu_samples_post ) print ( 'MLE: {} ' . format ( mle )) print ( 'Bayes Estimator: {} ' . format ( eb )) MLE: 182.16666666666666 Bayes Estimator: 184.21760983937554","title":"An\u00e1lise Bayesiana de amostras da distribui\u00e7\u00e3o Normal"},{"location":"infestatistica/BayesianAnalysisNormal/BayesianAnalysisNormal/#analise-bayesiana-de-amostras-da-distribuicao-normal","text":"","title":"An\u00e1lise Bayesiana de amostras da distribui\u00e7\u00e3o Normal"},{"location":"infestatistica/BayesianAnalysisNormal/BayesianAnalysisNormal/#referencias","text":"[1] Andrew Gelman. Bayesian Data Analysis. Parte I, Se\u00e7\u00e3o 3.3: essa refer\u00eancia usa a vari\u00e2ncia ao inv\u00e9s da precis\u00e3o. [2] Kevin P. Murphy. Conjugate Bayesian analysis of the Gaussian distribution: N\u00e3o muito simples, mas possui todas as contas.","title":"Refer\u00eancias"},{"location":"infestatistica/BayesianAnalysisNormal/BayesianAnalysisNormal/#precisao-da-distribuicao-normal","text":"Definimos \\tau := 1/\\sigma^2 como a precis\u00e3o da distribui\u00e7\u00e3o normal. A fun\u00e7\u00e3o da densidade de probabilidade da distribui\u00e7\u00e3o normal f(x|\\mu,\\tau) \u00e9, -\\infty < x < \\infty : f(x|\\mu,\\tau) = \\left(\\frac{\\tau}{2\\pi}\\right)^{1/2}\\exp\\left[-\\frac{1}{2}\\tau(x-\\mu)^2\\right]","title":"Precis\u00e3o da distribui\u00e7\u00e3o normal"},{"location":"infestatistica/BayesianAnalysisNormal/BayesianAnalysisNormal/#teorema","text":"Suponha que X_1,...,X_n \\overset{iid}{\\sim} N_2(\\mu, \\tau) , desconhecidos. Suponha que \\mu|\\tau \\sim N_2(\\mu_0, \\lambda_0 \\tau) \\tau \\sim \\text{Gamma}(\\alpha_0, \\beta_0) Ent\u00e3o a distribui\u00e7\u00e3o conjunta de \\mu e \\tau a posteriori \u00e9 dada por: \\mu|\\tau, X_1,...,X_n \\sim N_2(\\mu_1, \\lambda_1\\tau) \\tau|X_1,...,X_n \\sim \\text{Gamma}(\\alpha_1, \\beta_1), onde Par\u00e2metro Valor a posteriori do par\u00e2metro \\mu_1 \\frac{\\lambda_0 \\mu_0 + n\\bar{x}_n}{\\lambda_0 + n} \\lambda_1 \\lambda_0 + n \\alpha_1 \\alpha_0 + n/2 \\beta_1 \\beta_0 + s_n^2/2 + \\frac{n\\lambda_0(\\bar{x}_n - \\mu_0)^2}{2(\\lambda_0 + n)}","title":"Teorema"},{"location":"infestatistica/BayesianAnalysisNormal/BayesianAnalysisNormal/#familia-normal-gamma","text":"Sejam \\mu e \\tau vari\u00e1veis aleat\u00f3rias. Suponha que a distribui\u00e7\u00e3o condicional de \\mu dado \\tau \u00e9 normal com m\u00e9dia \\mu_0 e precis\u00e3o \\lambda_0 \\tau e que a distribui\u00e7\u00e3o marginal de \\tau seja gamma com par\u00e2metros \\alpha_0, \\beta_0 . Ent\u00e3o, falamos que a distribui\u00e7\u00e3o conjunta de \\mu e \\tau \u00e9 a distribui\u00e7\u00e3o normal-gamma com hiperpar\u00e2meteros \\mu_0, \\lambda_0, \\alpha_0 e \\beta_0 .","title":"Fam\u00edlia Normal-Gamma"},{"location":"infestatistica/BayesianAnalysisNormal/BayesianAnalysisNormal/#a-distribuicao-marginal-da-media-mu","text":"Suponha que a distribui\u00e7\u00e3o a priori \\mu e \\tau seja normal-gamma com hiperpar\u00e2metros \\mu_0, \\lambda_0, \\alpha_0 e \\beta_0 . Ent\u00e3o \\left(\\frac{\\lambda_0\\alpha_0}{\\beta_0}\\right)^{1/2}(\\mu - \\mu_0) tem a distribui\u00e7\u00e3o t com 2\\alpha_0 graus de liberdade. Nesse caso, se \\alpha_0 > 1/2 , E(\\mu) = \\mu_0 . Se \\alpha_0 > 1 , Var(\\mu) \\frac{\\beta_0}{\\lambda_0(\\alpha_0 - 1)} Obs.: As condi\u00e7\u00f5es sobre \\alpha_0 , vem da exist\u00eancia do k momento somente se o grau de liberdade da distribui\u00e7\u00e3o t \u00e9 maior do que k , isto \u00e9, 2\\alpha_0 > k .","title":"A distribui\u00e7\u00e3o marginal da m\u00e9dia \\mu"},{"location":"infestatistica/BayesianAnalysisNormal/BayesianAnalysisNormal/#comparacao-com-intervalos-de-confianca","text":"Podemos construir intervalos de confian\u00e7a para \\mu no mundo Bayesiano, pois ela \u00e9 uma vari\u00e1vel aleat\u00f3ria. Nesse caso, podemos fazer da seguinte maneira: \\begin{split} &P(-c < \\left(\\frac{\\lambda_0\\alpha_0}{\\beta_0}\\right)^{1/2}(\\mu - \\mu_0) < c) = \\gamma \\\\ &P(\\mu_0 - c\\left(\\frac{\\beta_0}{\\lambda_0\\alpha_0}\\right)^{1/2} < \\mu < \\mu_0 + c\\left(\\frac{\\beta_0}{\\lambda_0\\alpha_0}\\right)^{1/2}) &= \\gamma \\end{split} Ou seja, ganhamos um intervalo de confian\u00e7a de gra\u00e7a!","title":"Compara\u00e7\u00e3o com Intervalos de Confian\u00e7a"},{"location":"infestatistica/BayesianAnalysisNormal/BayesianAnalysisNormal/#implementacao","text":"Quando temos dados, X_1, ..., X_n \\sim N(\\mu, \\sigma^2) , podemos fazer \\hat{\\mu} = \\bar{X}_n que \u00e9 uma pontual (s\u00f3 para um valor de \\mu ). Mas isso nem sempre \u00e9 o melhor, e \u00e0s vezes nem t\u00e3o prazeroso. Por isso precisamos trazer Bayes para nossa an\u00e1lise. import numpy as np from scipy.stats import norm , gamma , t import pandas as pd import matplotlib.pyplot as plt import seaborn as sns sns . set () Nesse exemplos, vamos estudar o exemplo Casas de Enfermagem em Novo M\u00e9xico , tema estudado por Howard L. Smith, Neill F. Piland e Nancy Fisher . Nesse trabalho explora os desafios da enfermagem na \u00e1rea rural, para que se mantenha a viabilidade. Vamos utilizar o banco de dados deles, que incluem as seguintes informa\u00e7\u00f5es: BED: n\u00fameros de camas na casa. MCDAYS: dias anuais de interna\u00e7\u00e3o m\u00e9dica (centenas) TDAYS: total anual de pacientes dias (centenas) PCREV: receita anual total de atendimento ao paciente (centenas) NSAL: sal\u00e1rio das enfermeiras anual (centenas) FEXP: despesas anuais com instala\u00e7\u00f5es (centenas) RURAL: 1, se rural, 0, se urbano Os dados se encontram nesse site . table = [] with open ( '../data/nursinghome.txt' , 'r' ) as f : line = f . readline () table . append ( line . split ()) line = f . readline () while line != '' : table . append ([ int ( i ) for i in line . split ()]) line = f . readline () nurse_df = pd . DataFrame ( data = table [ 1 :], columns = table [ 0 ], dtype = np . int ) Vamos considerar nesse exemplo a coluna MCDAYS, restrita \u00e0s casas urbanas, que denotaremos por X . Antes de observarmos os dados, vamos modelar o valor de X para cada casa como uma vari\u00e1vel aleat\u00f3ria normal com m\u00e9dia \\mu e precis\u00e3o \\tau . urban_nurse_df = nurse_df [ nurse_df . RURAL == 0 ] plt . bar ( x = range ( len ( urban_nurse_df )), height = urban_nurse_df . MCDAYS ) plt . title ( 'MCDAYS por casa urbana de enfermeiras' ) plt . show () Para calcular nossa priori, dever\u00edamos conversar com especialistas. Como n\u00e3o \u00e9 o caso, vamos usar as informa\u00e7\u00f5es de camas. Temos: print ( 'M\u00e9dia: {:.1f} , Desvio-Padr\u00e3o: {:.1f} ' . format ( np . mean ( urban_nurse_df . BED ), np . std ( urban_nurse_df . BED ))) M\u00e9dia: 111.4, Desvio-Padr\u00e3o: 42.3 Podemos supor, a priori, que a taxa de ocupa\u00e7\u00e3o \u00e9 de 50%. Logo, em um ano, podemos obter que os dias anuais de interna\u00e7\u00e3o m\u00e9dica s\u00e3o: media = 0.5 * 365 * np . mean ( urban_nurse_df . BED ) / 100 # unidade em centenas std = 0.5 * 365 * np . std ( urban_nurse_df . BED ) / 100 Agora precisamos mapear esses valores para os hiperpar\u00e2metros a priori \\alpha_0, \\beta_0, \\lambda_0, \\mu_0 . Vamos dibidir a vari\u00e2ncia obtida acima (usando o n\u00famero de camas), como incerteza, e dividiremos igualmente essa incerteza sobre a m\u00e9dia e a precis\u00e3o, isto \u00e9, Var(\\mu) = std^2/2 E(\\tau) = 1/(std^2/2) Escolhemos \\alpha_0 = 2 (arbitr\u00e1rio, mas prefer\u00edvel a ser pequeno, porque esse par\u00e2metro tem a interpreta\u00e7\u00e3o de ser o conhecimento sobre o valor. Logo E[\\tau] = \\alpha_0/\\beta_0 \\implies \\beta_0 = \\alpha_0/E[\\tau] E[\\mu] = \\mu_0 Var(\\mu) = \\frac{\\beta_0}{\\lambda_0(\\alpha_0 - 1)} \\implies \\lambda_0 = \\frac{\\beta_0}{Var(\\mu)(\\alpha_0 - 1)} alpha0 = 2 beta0 = alpha * std ** 2 / 2 mu0 = media lambda0 = beta * 2 / ( std ** 2 * ( alpha - 1 )) Agora conseguimos expressar nosso conhecimento a priori para construir o primeiro intervalo de confian\u00e7a. Vamos usar a express\u00e3o constru\u00edda anteriormente. Primeiro, vamos ver, numericamente, a distribui\u00e7\u00e3o de \\mu def draw_samples ( alpha0 , beta0 , lambda0 , mu0 , seed = 10000 ): r = np . random . RandomState ( seed ) tau = r . gamma ( shape = alpha0 , scale = 1 / beta0 , size = 100000 ) mu = r . normal ( mu0 , scale = np . sqrt ( 1 / ( lambda0 * tau ))) return mu , tau mu_samples , tau_samples = draw_samples ( alpha0 , beta0 , lambda0 , mu0 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) sns . histplot ( mu_samples , ax = ax [ 0 ]) sns . histplot ( tau_samples , ax = ax [ 1 ]) ax [ 0 ] . set_title ( r '$\\mu$ a priori' ) ax [ 1 ] . set_title ( r '$\\tau$ a priori' ) ax [ 0 ] . set_xlim (( 0 , 500 )) plt . show () ci_mu = ( np . quantile ( mu_samples , 0.025 ), np . quantile ( mu_samples , 0.975 )) print ( ci_mu ) (95.85458277508882, 309.0974651995673) Agora, vamos observar os dados e calcular nossa posteriori! Vamos atualizar como demonstrado no Teorema. Par\u00e2metro Valor a posteriori do par\u00e2metro \\mu_1 \\frac{\\lambda_0 \\mu_0 + n\\bar{x}_n}{\\lambda_0 + n} \\lambda_1 \\lambda_0 + n \\alpha_1 \\alpha_0 + n/2 \\beta_1 \\beta_0 + s_n^2/2 + \\frac{n\\lambda_0(\\bar{x}_n - \\mu_0)^2}{2(\\lambda_0 + n)} n = len ( urban_nurse_df ) mom1 = np . mean ( urban_nurse_df . MCDAYS ) mom2 = np . var ( urban_nurse_df . MCDAYS ) * n mu1 = ( lambda0 * mu0 + n * mom1 ) / ( lambda0 + n ) lambda1 = lambda0 + n alpha1 = alpha0 + n / 2 beta1 = beta0 + mom2 / 2 + ( n * lambda0 * ( mom1 - mu0 ) ** 2 ) / ( 2 * ( lambda0 + n )) mu_samples_post , tau_samples_post = draw_samples ( alpha1 , beta1 , lambda1 , mu1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) sns . histplot ( mu_samples_post , ax = ax [ 0 ]) sns . histplot ( tau_samples_post , ax = ax [ 1 ]) ax [ 0 ] . set_title ( r '$\\mu$ a posteriori' ) ax [ 1 ] . set_title ( r '$\\tau$ a posteriori' ) ax [ 0 ] . set_xlim (( 100 , 300 )) plt . show () ci_mu = ( np . quantile ( mu_samples_post , 0.025 ), np . quantile ( mu_samples_post , 0.975 )) print ( ci_mu ) (152.61993663355443, 215.58303707725233) Observe como o intervalo de confian\u00e7a est\u00e1 bem menor! Ou seja, os dados aumentaram nossa certeza sobre o par\u00e2metro. Vamos comparar os valores. fig , ax = plt . subplots ( figsize = ( 10 , 5 )) sns . histplot ( mu_samples_post , ax = ax , kde = True , stat = 'density' , label = 'Posteriori' ) sns . histplot ( mu_samples , ax = ax , kde = True , stat = 'density' , label = 'Priori' , color = 'red' ) ax . set_title ( r '$\\mu$' ) ax . set_xlim (( - 200 , 400 )) ax . legend () plt . show () Por fim, vamos comparar o estimador de Bayes com o de M\u00e1xima Verossimilhan\u00e7a: mle = mom1 eb = np . mean ( mu_samples_post ) print ( 'MLE: {} ' . format ( mle )) print ( 'Bayes Estimator: {} ' . format ( eb )) MLE: 182.16666666666666 Bayes Estimator: 184.21760983937554","title":"Implementa\u00e7\u00e3o"},{"location":"infestatistica/ConfidenceIntervals/ConfidenceIntervals/","text":"Intervalos de Confian\u00e7a Esse tema procura responder qu\u00e3o confian\u00e7a dever\u00edamos por em um estimador. \u00c9 claro que essa pergunta tem que ser um pouco melhor descrita matematicamente. A ideia \u00e9 frequentista e tem a ideia a seguinte forma: O intervalo [a,b] , uma realiza\u00e7\u00e3o de [A,B] , tem 95% de confian\u00e7a se em 95% do tempo, o par\u00e2metro procurado est\u00e1 entre a e b . Veja que a ideia \u00e9 basicamente frequentista, dado que a interpreta\u00e7\u00e3o est\u00e1 ligada \u00e0 frequ\u00eancia quando o n\u00famero de experimentos tende para infinito. (Cuidado: N\u00e3o vamos falar da probabilidade do par\u00e2metro estar em [a,b] , isso n\u00e3o faz sentido, pois \\theta n\u00e3o \u00e9 uma vari\u00e1vel aleat\u00f3ria, e sim um valor fixo). Defini\u00e7\u00e3o Seja X_1, ..., X_n \\overset{iid}{\\sim} F(\\theta) . Sejam A \\leq B duas estat\u00edsticas que possuem a propriedade, para todo \\theta , P(A < g(\\theta) < B) \\geq \\gamma Chamamos (A,B) de intevalo de confian\u00e7a para g(\\theta) com coeficiente \\gamma . O intervalo \u00e9 chamado de exato se ao inv\u00e9s da desigualdade, tivermos uma igualdade. Ap\u00f3s observarmos os valores de X_1, ..., X_n e computarmos A = a e B = b , o intervalo (a,b) \u00e9 chamado de valor observado do intervalo de confian\u00e7a. Intervalo de Confian\u00e7a para a m\u00e9dia de N(\\mu, \\sigma^2) Seja X_1, ..., X_n \\sim N(\\mu, \\sigma^2) . Para cada 0 < \\gamma < 1 , o intervalo (A,B) \u00e9 intervalo de confian\u00e7a exato para \\mu com coeficiente \\gamma , em que: A = \\bar{X}_n - T_{n-1}^{-1}\\left(\\frac{1 + \\gamma}{2}\\right)\\frac{\\sigma '}{n^{1/2}} B = \\bar{X}_n + T_{n-1}^{-1}\\left(\\frac{1 + \\gamma}{2}\\right)\\frac{\\sigma '}{n^{1/2}} onde T_{n-1} denota a cdf da distribui\u00e7\u00e3o t com n-1 graus de liberdade. O interessante \u00e9 que isso \u00e9 implica\u00e7\u00e3o direta da distribui\u00e7\u00e3o de U = \\frac{n^{1/2}(\\bar{X}_n - \\mu)}{\\sigma '} que inferimos no cap\u00edtulo 8.4, nesse caso, simplemente fizemos a transforma\u00e7\u00e3o: \\gamma = P(-c < U < c) = P(A < \\mu < B) e c \u00e9 escolhido de acordo com \\gamma . Implementa\u00e7\u00e3o Vamos rever a informa\u00e7\u00e3o sobre caf\u00e9 que usamos cap\u00edtulos antes para ver como isso acontece na pr\u00e1tica. Considere dados sobre pesos de beb\u00eas logo ao nascer. bwt: peso do beb\u00ea ao nascer. gestation: dura\u00e7\u00e3o em dias da gesta\u00e7\u00e3o. parity: primeiro filho ou n\u00e3o. age: idade da m\u00e3e. height: altura da m\u00e3e em polegadas. weight: peso da m\u00e3e em pounds. smoke: se a m\u00e3e \u00e9 fumante ou n\u00e3o. # Importando bibliotecas import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from scipy.stats import t birth_df = pd . read_csv ( \"http://people.reed.edu/~jones/141/Bwt.dat\" ) birth_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bwt gestation parity age height weight smoke 0 120 284 0 27 62 100 0 1 113 282 0 33 64 135 0 2 128 279 0 28 64 115 1 3 108 282 0 23 67 125 1 4 136 286 0 25 62 93 0 sns . histplot ( data = birth_df . bwt , kde = True ) plt . title ( 'Histograma dos pesos dos beb\u00eas' ) plt . show () birth_df [ birth_df . smoke == 0 ] . bwt . hist ( density = True , label = 'N\u00e3o fumante' ) birth_df [ birth_df . smoke == 1 ] . bwt . hist ( density = True , label = 'Fumante' , alpha = 0.6 ) plt . xlabel ( 'Peso' ) plt . legend () plt . show () Sabemos que essa \u00e9 uma extra\u00e7\u00e3o de uma popula\u00e7\u00e3o maior. Para conseguirmos mais amostras, vamos usar um procedimento chamado bootstrap . A ideia desse procedimento \u00e9 criar um novas amostras a partir de uma amostra inicial, usando replace = True como diferencial. Vou fazer esse procedimento diversar vezes e ir calculando a m\u00e9dia amostral. Como a m\u00e9dia amostral \u00e9 uma vari\u00e1vel aleat\u00f3ria, vamos obter um histograma das realiza\u00e7\u00f5es. Vamos supor que o peso W_i da crian\u00e7a i vem de uma distribui\u00e7\u00e3o com par\u00e2metros \\mu e \\sigma^2 desconhecidos. Nesse caso, \\bar{W}_i vir\u00e1 de uma distribui\u00e7\u00e3o normal com par\u00e2metros \\mu e \\sigma^2/n . ite = 10000 n = 200 bootstrap_means = np . zeros ( ite ) for i in range ( ite ): bootstrap_sample = birth_df . sample ( n = n , replace = True , random_state = i ) bootstrap_means [ i ] = bootstrap_sample . bwt . mean () sns . histplot ( bootstrap_means , kde = True ) plt . title ( \"M\u00e9dias das amostras\" ) plt . xlabel ( 'Peso' ) plt . show () Vamos calcular o nosso intervalo de confian\u00e7a com \\gamma = 0.95 . Temos que: gamma = 0.95 A = lambda x : np . mean ( x ) - t . ppf ( q = ( 1 + gamma ) / 2 , df = len ( x ) - 1 ) * np . std ( x , ddof = 1 ) / len ( x ) ** ( 1 / 2 ) B = lambda x : np . mean ( x ) + t . ppf ( q = ( 1 + gamma ) / 2 , df = len ( x ) - 1 ) * np . std ( x , ddof = 1 ) / len ( x ) ** ( 1 / 2 ) ite = 100 n = 500 bootstrap_intervals = np . zeros (( ite , 2 )) for i in range ( ite ): bootstrap_sample = birth_df . sample ( n = n , replace = True , random_state = i ) bootstrap_intervals [ i , 0 ] = A ( bootstrap_sample . bwt ) bootstrap_intervals [ i , 1 ] = B ( bootstrap_sample . bwt ) out_values = np . where (( bootstrap_intervals [:, 0 ] > 119.5 ) | ( bootstrap_intervals [:, 1 ] < 119.5 )) plt . figure ( figsize = ( 6 , 10 )) plt . scatter ( bootstrap_intervals [:, 0 ], np . arange ( 0 , ite ), color = 'red' , label = 'a' ) plt . scatter ( bootstrap_intervals [:, 1 ], np . arange ( 0 , ite ), color = 'green' , label = 'b' ) plt . scatter ( bootstrap_intervals [ out_values [ 0 ], 0 ], out_values [ 0 ], color = 'black' , label = 'Fora' ) plt . scatter ( bootstrap_intervals [ out_values [ 0 ], 1 ], out_values [ 0 ], color = 'black' ) plt . vlines ( 119.5 , ymin = 0 , ymax = ite , linestyle = '--' , alpha = 0.6 , label = 'M\u00e9dia real' ) plt . legend () plt . show () Interpreta\u00e7\u00e3o Estamos fazer uma afirma\u00e7\u00e3o probabil\u00edstica sobre o intervalo (A,B) antes de observar os dados. Ap\u00f3s observarmos os dados, n\u00e3o podemos interpretar (a,b) como um intervalo em que temos 95% de confian\u00e7a de g(\\theta) estar no intervalo. Antes de observarmos as amostras, temos a confian\u00e7a de que 95% dos intervalos conter\u00e3o \\mu . Sem simetria Construimos anteriormente um intervalo sim\u00e9trico, onde a estat\u00edstica U acima mencionada estaria entre -c e c com probabilidade \\gamma . Mas podemos desenvolver intervalos n\u00e3o sim\u00e9tricos tamb\u00e9m. Uma forma que podemos fazer isso \u00e9 escolhendo \\gamma_1 e \\gamma_2 , tal que \\gamma_2 - \\gamma_1 = \\gamma . Assim: P\\left(T_{n-1}^{-1}(\\gamma_1) < U < T_{n-1}^{-1}(\\gamma_2)\\right) = \\gamma Talv\u00e9m vc esteja se perguntando: porque escolher \\gamma_1, \\gamma_2 dessa forma? Bom: \\begin{split} \\gamma &= P\\left(T_{n-1}^{-1}(\\gamma_1) < U < T_{n-1}^{-1}(\\gamma_2)\\right) \\\\ &= P\\left(U < T_{n-1}^{-1}(\\gamma_2)\\right) - P\\left(U \\leq T_{n-1}^{-1}(\\gamma_1)\\right) \\\\ &= \\gamma_2 - \\gamma_1 \\end{split} Intervalos de Confian\u00e7a Unilateral Defini\u00e7\u00e3o Seja X_1, ..., X_n \\overset{idd}{\\sim} F(\\theta) . Sejam A e B duas estat\u00edsticas tais que: P(A < g(\\theta)) \\geq \\gamma P(B > g(\\theta)) \\geq \\gamma Ent\u00e3o (A, \\infty) e (-\\infty, B) s\u00e3o chamados de intervalos de confia\u00e7a unilaterais para g(\\theta) de coeficiente \\gamma ou percentil 100\\gamma . No caso de A 100\\gamma porcento abaixo e no caso de B a cima. Se vale a igualdade, dizemos que o intervalor \u00e9 exato. Intervalo unilateral para a m\u00e9dia de N(\\mu,\\sigma^2) Nas mesma condi\u00e7\u00f5es do teorema anterior, mas as estat\u00edsticas para baixo e para cima com coeficiente \\gamma para \\mu s\u00e3o: A = \\bar{X}_n - T_{n-1}^{-1}\\left(\\gamma\\right)\\frac{\\sigma '}{n^{1/2}} B = \\bar{X}_n + T_{n-1}^{-1}\\left(\\gamma\\right)\\frac{\\sigma '}{n^{1/2}} Pivotal Seja X_1, ..., X_n \\overset{idd}{\\sim} F(\\theta) . Seja V(\\vec{X},\\theta) uma vari\u00e1vel aleat\u00f3ria cuja distribui\u00e7\u00e3o \u00e9 a mesma para \\theta . Chamamos V de quantidade pivotal . Teorema Seja X_1, ..., X_n \\overset{idd}{\\sim} F(\\theta) . Suponha que Exista V pivotal. A cdf G de V \u00e9 cont\u00ednua. Exista fun\u00e7\u00e3o r tal r(V(X,\\theta), X) = g(\\theta) , ou seja, \u00e9 uma esp\u00e9cie de \"inversa\". r(v,x) (3) \u00e9 uma fun\u00e7\u00e3o estritamente crescente em v para todo x . Ent\u00e3o A = r(G^{-1}(\\gamma_1), X) B = r(G^{-1}(\\gamma_2), X) s\u00e3o os pontos extremos do intervalo de confian\u00e7a exato para g(\\theta) de coeficiente \\gamma = \\gamma_2 - \\gamma_1 . Se r \u00e9 estritamente decrescente, invertemos A e B . Obs.: Ainda podemos usar o Teorema Central do Limite para obter intervalos de confian\u00e7a assint\u00f3ticos. Exemplo com Regress\u00e3o Linear O dataset que utilizei anteriormente n\u00e3o \u00e9 muito bom para esse exemplo, mas eu vou usar, de qualquer forma, para entendermos o processo e como pode nos ajudar o intervalo de confian\u00e7a. Em uma Regress\u00e3o Linear, queremos dizer aferir uma rela\u00e7\u00e3o linear entre duas vari\u00e1veis, isto \u00e9, queremos dizer que uma vari\u00e1vel pode ser obtida pela outra atrav\u00e9s de uma reta, mais um erro aleat\u00f3rio. Suponha que queremos estimar Y o peso da crian\u00e7a ao nascer, sabendo a informa\u00e7\u00e3o do tempo de gesta\u00e7\u00e3o X e que Y = aX + b + E, onde E \\sim N(0,\\sigma^2) . Nesse caso, estamos dizendo que Y|X \\sim N(aX + b, \\sigma^2) . Queremos estimar a e b de forma que tenhamos o melhor ajuste poss\u00edvel. Esse tema em espec\u00edfico n\u00e3o me interessa. Entretanto, podemos dizer que queremos estimar aX + b , a m\u00e9dia de uma normal, mas que muda para cada X = x observado. sns . lmplot ( x = 'gestation' , y = 'bwt' , data = birth_df , height = 5 , ci = 95 ) <seaborn.axisgrid.FacetGrid at 0x7fc517f55c18> O resultado n\u00e3o foi muito bom (na verdade eu j\u00e1 imaginava isso). Mas o interessante \u00e9 tentar refletir o que essas bandas significam? Por que os pontos n\u00e3o est\u00e3o nela? Esper\u00e1vamos que estiv\u00e9sse? E por que ela diminui a vari\u00e2ncia com o n\u00famero de pontos? Essas perguntas v\u00e3o ser devidamente respondidas no pr\u00f3ximo curso de Estat\u00edstica! Mas eu j\u00e1 vou adiantando que esse intervalo de confian\u00e7a \u00e9 para a m\u00e9dia estimada.","title":"Intervalos de Confian\u00e7a"},{"location":"infestatistica/ConfidenceIntervals/ConfidenceIntervals/#intervalos-de-confianca","text":"Esse tema procura responder qu\u00e3o confian\u00e7a dever\u00edamos por em um estimador. \u00c9 claro que essa pergunta tem que ser um pouco melhor descrita matematicamente. A ideia \u00e9 frequentista e tem a ideia a seguinte forma: O intervalo [a,b] , uma realiza\u00e7\u00e3o de [A,B] , tem 95% de confian\u00e7a se em 95% do tempo, o par\u00e2metro procurado est\u00e1 entre a e b . Veja que a ideia \u00e9 basicamente frequentista, dado que a interpreta\u00e7\u00e3o est\u00e1 ligada \u00e0 frequ\u00eancia quando o n\u00famero de experimentos tende para infinito. (Cuidado: N\u00e3o vamos falar da probabilidade do par\u00e2metro estar em [a,b] , isso n\u00e3o faz sentido, pois \\theta n\u00e3o \u00e9 uma vari\u00e1vel aleat\u00f3ria, e sim um valor fixo).","title":"Intervalos de Confian\u00e7a"},{"location":"infestatistica/ConfidenceIntervals/ConfidenceIntervals/#definicao","text":"Seja X_1, ..., X_n \\overset{iid}{\\sim} F(\\theta) . Sejam A \\leq B duas estat\u00edsticas que possuem a propriedade, para todo \\theta , P(A < g(\\theta) < B) \\geq \\gamma Chamamos (A,B) de intevalo de confian\u00e7a para g(\\theta) com coeficiente \\gamma . O intervalo \u00e9 chamado de exato se ao inv\u00e9s da desigualdade, tivermos uma igualdade. Ap\u00f3s observarmos os valores de X_1, ..., X_n e computarmos A = a e B = b , o intervalo (a,b) \u00e9 chamado de valor observado do intervalo de confian\u00e7a.","title":"Defini\u00e7\u00e3o"},{"location":"infestatistica/ConfidenceIntervals/ConfidenceIntervals/#intervalo-de-confianca-para-a-media-de-nmu-sigma2","text":"Seja X_1, ..., X_n \\sim N(\\mu, \\sigma^2) . Para cada 0 < \\gamma < 1 , o intervalo (A,B) \u00e9 intervalo de confian\u00e7a exato para \\mu com coeficiente \\gamma , em que: A = \\bar{X}_n - T_{n-1}^{-1}\\left(\\frac{1 + \\gamma}{2}\\right)\\frac{\\sigma '}{n^{1/2}} B = \\bar{X}_n + T_{n-1}^{-1}\\left(\\frac{1 + \\gamma}{2}\\right)\\frac{\\sigma '}{n^{1/2}} onde T_{n-1} denota a cdf da distribui\u00e7\u00e3o t com n-1 graus de liberdade. O interessante \u00e9 que isso \u00e9 implica\u00e7\u00e3o direta da distribui\u00e7\u00e3o de U = \\frac{n^{1/2}(\\bar{X}_n - \\mu)}{\\sigma '} que inferimos no cap\u00edtulo 8.4, nesse caso, simplemente fizemos a transforma\u00e7\u00e3o: \\gamma = P(-c < U < c) = P(A < \\mu < B) e c \u00e9 escolhido de acordo com \\gamma .","title":"Intervalo de Confian\u00e7a para a m\u00e9dia de N(\\mu, \\sigma^2)"},{"location":"infestatistica/ConfidenceIntervals/ConfidenceIntervals/#implementacao","text":"Vamos rever a informa\u00e7\u00e3o sobre caf\u00e9 que usamos cap\u00edtulos antes para ver como isso acontece na pr\u00e1tica. Considere dados sobre pesos de beb\u00eas logo ao nascer. bwt: peso do beb\u00ea ao nascer. gestation: dura\u00e7\u00e3o em dias da gesta\u00e7\u00e3o. parity: primeiro filho ou n\u00e3o. age: idade da m\u00e3e. height: altura da m\u00e3e em polegadas. weight: peso da m\u00e3e em pounds. smoke: se a m\u00e3e \u00e9 fumante ou n\u00e3o. # Importando bibliotecas import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from scipy.stats import t birth_df = pd . read_csv ( \"http://people.reed.edu/~jones/141/Bwt.dat\" ) birth_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bwt gestation parity age height weight smoke 0 120 284 0 27 62 100 0 1 113 282 0 33 64 135 0 2 128 279 0 28 64 115 1 3 108 282 0 23 67 125 1 4 136 286 0 25 62 93 0 sns . histplot ( data = birth_df . bwt , kde = True ) plt . title ( 'Histograma dos pesos dos beb\u00eas' ) plt . show () birth_df [ birth_df . smoke == 0 ] . bwt . hist ( density = True , label = 'N\u00e3o fumante' ) birth_df [ birth_df . smoke == 1 ] . bwt . hist ( density = True , label = 'Fumante' , alpha = 0.6 ) plt . xlabel ( 'Peso' ) plt . legend () plt . show () Sabemos que essa \u00e9 uma extra\u00e7\u00e3o de uma popula\u00e7\u00e3o maior. Para conseguirmos mais amostras, vamos usar um procedimento chamado bootstrap . A ideia desse procedimento \u00e9 criar um novas amostras a partir de uma amostra inicial, usando replace = True como diferencial. Vou fazer esse procedimento diversar vezes e ir calculando a m\u00e9dia amostral. Como a m\u00e9dia amostral \u00e9 uma vari\u00e1vel aleat\u00f3ria, vamos obter um histograma das realiza\u00e7\u00f5es. Vamos supor que o peso W_i da crian\u00e7a i vem de uma distribui\u00e7\u00e3o com par\u00e2metros \\mu e \\sigma^2 desconhecidos. Nesse caso, \\bar{W}_i vir\u00e1 de uma distribui\u00e7\u00e3o normal com par\u00e2metros \\mu e \\sigma^2/n . ite = 10000 n = 200 bootstrap_means = np . zeros ( ite ) for i in range ( ite ): bootstrap_sample = birth_df . sample ( n = n , replace = True , random_state = i ) bootstrap_means [ i ] = bootstrap_sample . bwt . mean () sns . histplot ( bootstrap_means , kde = True ) plt . title ( \"M\u00e9dias das amostras\" ) plt . xlabel ( 'Peso' ) plt . show () Vamos calcular o nosso intervalo de confian\u00e7a com \\gamma = 0.95 . Temos que: gamma = 0.95 A = lambda x : np . mean ( x ) - t . ppf ( q = ( 1 + gamma ) / 2 , df = len ( x ) - 1 ) * np . std ( x , ddof = 1 ) / len ( x ) ** ( 1 / 2 ) B = lambda x : np . mean ( x ) + t . ppf ( q = ( 1 + gamma ) / 2 , df = len ( x ) - 1 ) * np . std ( x , ddof = 1 ) / len ( x ) ** ( 1 / 2 ) ite = 100 n = 500 bootstrap_intervals = np . zeros (( ite , 2 )) for i in range ( ite ): bootstrap_sample = birth_df . sample ( n = n , replace = True , random_state = i ) bootstrap_intervals [ i , 0 ] = A ( bootstrap_sample . bwt ) bootstrap_intervals [ i , 1 ] = B ( bootstrap_sample . bwt ) out_values = np . where (( bootstrap_intervals [:, 0 ] > 119.5 ) | ( bootstrap_intervals [:, 1 ] < 119.5 )) plt . figure ( figsize = ( 6 , 10 )) plt . scatter ( bootstrap_intervals [:, 0 ], np . arange ( 0 , ite ), color = 'red' , label = 'a' ) plt . scatter ( bootstrap_intervals [:, 1 ], np . arange ( 0 , ite ), color = 'green' , label = 'b' ) plt . scatter ( bootstrap_intervals [ out_values [ 0 ], 0 ], out_values [ 0 ], color = 'black' , label = 'Fora' ) plt . scatter ( bootstrap_intervals [ out_values [ 0 ], 1 ], out_values [ 0 ], color = 'black' ) plt . vlines ( 119.5 , ymin = 0 , ymax = ite , linestyle = '--' , alpha = 0.6 , label = 'M\u00e9dia real' ) plt . legend () plt . show ()","title":"Implementa\u00e7\u00e3o"},{"location":"infestatistica/ConfidenceIntervals/ConfidenceIntervals/#interpretacao","text":"Estamos fazer uma afirma\u00e7\u00e3o probabil\u00edstica sobre o intervalo (A,B) antes de observar os dados. Ap\u00f3s observarmos os dados, n\u00e3o podemos interpretar (a,b) como um intervalo em que temos 95% de confian\u00e7a de g(\\theta) estar no intervalo. Antes de observarmos as amostras, temos a confian\u00e7a de que 95% dos intervalos conter\u00e3o \\mu .","title":"Interpreta\u00e7\u00e3o"},{"location":"infestatistica/ConfidenceIntervals/ConfidenceIntervals/#sem-simetria","text":"Construimos anteriormente um intervalo sim\u00e9trico, onde a estat\u00edstica U acima mencionada estaria entre -c e c com probabilidade \\gamma . Mas podemos desenvolver intervalos n\u00e3o sim\u00e9tricos tamb\u00e9m. Uma forma que podemos fazer isso \u00e9 escolhendo \\gamma_1 e \\gamma_2 , tal que \\gamma_2 - \\gamma_1 = \\gamma . Assim: P\\left(T_{n-1}^{-1}(\\gamma_1) < U < T_{n-1}^{-1}(\\gamma_2)\\right) = \\gamma Talv\u00e9m vc esteja se perguntando: porque escolher \\gamma_1, \\gamma_2 dessa forma? Bom: \\begin{split} \\gamma &= P\\left(T_{n-1}^{-1}(\\gamma_1) < U < T_{n-1}^{-1}(\\gamma_2)\\right) \\\\ &= P\\left(U < T_{n-1}^{-1}(\\gamma_2)\\right) - P\\left(U \\leq T_{n-1}^{-1}(\\gamma_1)\\right) \\\\ &= \\gamma_2 - \\gamma_1 \\end{split}","title":"Sem simetria"},{"location":"infestatistica/ConfidenceIntervals/ConfidenceIntervals/#intervalos-de-confianca-unilateral","text":"","title":"Intervalos de Confian\u00e7a Unilateral"},{"location":"infestatistica/ConfidenceIntervals/ConfidenceIntervals/#definicao_1","text":"Seja X_1, ..., X_n \\overset{idd}{\\sim} F(\\theta) . Sejam A e B duas estat\u00edsticas tais que: P(A < g(\\theta)) \\geq \\gamma P(B > g(\\theta)) \\geq \\gamma Ent\u00e3o (A, \\infty) e (-\\infty, B) s\u00e3o chamados de intervalos de confia\u00e7a unilaterais para g(\\theta) de coeficiente \\gamma ou percentil 100\\gamma . No caso de A 100\\gamma porcento abaixo e no caso de B a cima. Se vale a igualdade, dizemos que o intervalor \u00e9 exato.","title":"Defini\u00e7\u00e3o"},{"location":"infestatistica/ConfidenceIntervals/ConfidenceIntervals/#intervalo-unilateral-para-a-media-de-nmusigma2","text":"Nas mesma condi\u00e7\u00f5es do teorema anterior, mas as estat\u00edsticas para baixo e para cima com coeficiente \\gamma para \\mu s\u00e3o: A = \\bar{X}_n - T_{n-1}^{-1}\\left(\\gamma\\right)\\frac{\\sigma '}{n^{1/2}} B = \\bar{X}_n + T_{n-1}^{-1}\\left(\\gamma\\right)\\frac{\\sigma '}{n^{1/2}}","title":"Intervalo unilateral para a m\u00e9dia de N(\\mu,\\sigma^2)"},{"location":"infestatistica/ConfidenceIntervals/ConfidenceIntervals/#pivotal","text":"Seja X_1, ..., X_n \\overset{idd}{\\sim} F(\\theta) . Seja V(\\vec{X},\\theta) uma vari\u00e1vel aleat\u00f3ria cuja distribui\u00e7\u00e3o \u00e9 a mesma para \\theta . Chamamos V de quantidade pivotal .","title":"Pivotal"},{"location":"infestatistica/ConfidenceIntervals/ConfidenceIntervals/#teorema","text":"Seja X_1, ..., X_n \\overset{idd}{\\sim} F(\\theta) . Suponha que Exista V pivotal. A cdf G de V \u00e9 cont\u00ednua. Exista fun\u00e7\u00e3o r tal r(V(X,\\theta), X) = g(\\theta) , ou seja, \u00e9 uma esp\u00e9cie de \"inversa\". r(v,x) (3) \u00e9 uma fun\u00e7\u00e3o estritamente crescente em v para todo x . Ent\u00e3o A = r(G^{-1}(\\gamma_1), X) B = r(G^{-1}(\\gamma_2), X) s\u00e3o os pontos extremos do intervalo de confian\u00e7a exato para g(\\theta) de coeficiente \\gamma = \\gamma_2 - \\gamma_1 . Se r \u00e9 estritamente decrescente, invertemos A e B . Obs.: Ainda podemos usar o Teorema Central do Limite para obter intervalos de confian\u00e7a assint\u00f3ticos.","title":"Teorema"},{"location":"infestatistica/ConfidenceIntervals/ConfidenceIntervals/#exemplo-com-regressao-linear","text":"O dataset que utilizei anteriormente n\u00e3o \u00e9 muito bom para esse exemplo, mas eu vou usar, de qualquer forma, para entendermos o processo e como pode nos ajudar o intervalo de confian\u00e7a. Em uma Regress\u00e3o Linear, queremos dizer aferir uma rela\u00e7\u00e3o linear entre duas vari\u00e1veis, isto \u00e9, queremos dizer que uma vari\u00e1vel pode ser obtida pela outra atrav\u00e9s de uma reta, mais um erro aleat\u00f3rio. Suponha que queremos estimar Y o peso da crian\u00e7a ao nascer, sabendo a informa\u00e7\u00e3o do tempo de gesta\u00e7\u00e3o X e que Y = aX + b + E, onde E \\sim N(0,\\sigma^2) . Nesse caso, estamos dizendo que Y|X \\sim N(aX + b, \\sigma^2) . Queremos estimar a e b de forma que tenhamos o melhor ajuste poss\u00edvel. Esse tema em espec\u00edfico n\u00e3o me interessa. Entretanto, podemos dizer que queremos estimar aX + b , a m\u00e9dia de uma normal, mas que muda para cada X = x observado. sns . lmplot ( x = 'gestation' , y = 'bwt' , data = birth_df , height = 5 , ci = 95 ) <seaborn.axisgrid.FacetGrid at 0x7fc517f55c18> O resultado n\u00e3o foi muito bom (na verdade eu j\u00e1 imaginava isso). Mas o interessante \u00e9 tentar refletir o que essas bandas significam? Por que os pontos n\u00e3o est\u00e3o nela? Esper\u00e1vamos que estiv\u00e9sse? E por que ela diminui a vari\u00e2ncia com o n\u00famero de pontos? Essas perguntas v\u00e3o ser devidamente respondidas no pr\u00f3ximo curso de Estat\u00edstica! Mas eu j\u00e1 vou adiantando que esse intervalo de confian\u00e7a \u00e9 para a m\u00e9dia estimada.","title":"Exemplo com Regress\u00e3o Linear"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/","text":"Distribui\u00e7\u00f5es de Prioris Conjugadas Se a distribui\u00e7\u00e3o a priori \u00e9 membro de uma fam\u00edlia e a distribui\u00e7\u00e3o a posteriori tamb\u00e9m pertence a mesma a fam\u00edlia, essa fam\u00edlia de distribui\u00e7\u00f5es \u00e9 chamada de fam\u00edlia conjugada . A principal consequ\u00eancia de usar prioris de uma fam\u00edlia conjugada \u00e9 que as contas ficam muito mais simples. Principais Fam\u00edlias Conjugadas Teorema Suponha que X_1, ..., X_n \\overset{iid}{\\sim} \\text{Bernoulli}(\\theta) , 0 < \\theta < 1 desconhecido. Suponha que \\theta \\sim \\text{Beta}(\\alpha, \\beta) . Ent\u00e3o, a distribui\u00e7\u00e3o a posteriori de \\theta \u00e9 a distribui\u00e7\u00e3o beta com par\u00e2metros \\alpha + \\sum_{i=1}^n x_i e \\beta + n - \\sum_{i=1}^n x_i . Teorema Suponha que X_1, ...., X_n \\overset{iid}{\\sim} \\text{Poisson}(\\theta) , onde \\theta \u00e9 desconhecido. Suponha tamb\u00e9m que a distribui\u00e7\u00e3o a priori de \\theta \u00e9 uma gamma com par\u00e2metros \\alpha e \\beta . Ent\u00e3o a distribui\u00e7\u00e3o a posteriori de \\theta sej\u00e1 gamma com par\u00e2metros \\alpha + \\sum_{i=1}^n x_i e \\beta + n . Teorema Suponha que X_1, ...., X_n \\overset{iid}{\\sim} \\text{Normal}(\\theta, \\sigma^2) , onde \\theta \u00e9 desconhecido e \\sigma \u00e9 conhecido. Suponha tamb\u00e9m que a distribui\u00e7\u00e3o a priori de \\theta \u00e9 uma normal com m\u00e9dia \\mu_0 e vari\u00e2ncia v_0^2 . Ent\u00e3o a distribui\u00e7\u00e3o a posteriori de \\theta sej\u00e1 normal com m\u00e9dia \\mu_1 e vari\u00e2ncia v_1^2 , onde: \\mu_1 = \\frac{\\sigma^2\\mu_0 + nv_0^2\\bar{x}_n}{\\sigma^2 + nv_0^2} v_1^2 = \\frac{\\sigma^2v_0^2}{\\sigma^2 + nv_0^2} Teorema Suponha que X_1, ...., X_n \\overset{iid}{\\sim} \\text{Exp}(\\theta) , onde \\theta \u00e9 desconhecido. Suponha tamb\u00e9m que a distribui\u00e7\u00e3o a priori de \\theta \u00e9 uma gamma com par\u00e2metros \\alpha e \\beta . Ent\u00e3o a distribui\u00e7\u00e3o a posteriori de \\theta sej\u00e1 gamma com par\u00e2metros \\alpha + n e \\beta + \\sum_{i=1}^n x_i . import numpy as np from scipy import stats import matplotlib.pyplot as plt from matplotlib import animation , cm from IPython.display import HTML % matplotlib inline Suponha que \\theta seja a probabilidade de um item ser defeituoso em uma s\u00e9rie de items. Suponha que nossa priori em \\theta \u00e9 uma distribui\u00e7\u00e3o beta com par\u00e2metros \\alpha e \\beta . S\u00e3o selecionados n items por vez para o teste. Sabemos que nossa posteriori ser\u00e1 uma Beta com os par\u00e2metros acima. Vejamos graficamente esse processo. theta_real = 0.1 n = 100 np . random . seed ( 10 ) XMIN = 0 XMAX = 1 YMIN = 0 YMAX = 5 alpha = [ 1 ] beta = [ 1 ] x = np . linspace ( 0.001 , 1 , 1000 ) # Definindo cores cmap = cm . autumn # Esta fun\u00e7\u00e3o permite plotar o backgroud def init (): line . set_data ([], []) return ( line ,) # Definindo o espa\u00e7o da imagem fig , ax = plt . subplots () # Definindo caracter\u00edsticas do background ax . set_xlim (( XMIN , XMAX )) ax . set_ylim (( YMIN , YMAX )) ax . set_title ( 'Evolu\u00e7\u00e3o da posteriori a cada itera\u00e7\u00e3o' ) ax . vlines ( theta_real , ymin = YMIN , ymax = YMAX , linestyle = '--' , color = 'grey' ) # Definindo plots vari\u00e1veis line , = ax . plot ([], [], lw = 2 ) line2 , _ = ax . plot ( XMIN , XMAX , YMIN , YMAX , linestyle = ':' ) def animate ( i , alpha , beta , x , n ): # Amostro da distribui\u00e7\u00e3o sample = np . random . binomial ( 1 , p = theta_real ) # Junto a lista que guarda os alphas e betas de cada itera\u00e7\u00e3o alpha . append ( alpha [ - 1 ] + sample ) beta . append ( beta [ - 1 ] + 1 - sample ) # Calculo a posteriori posteriori = stats . beta ( a = alpha [ - 1 ], b = beta [ - 1 ]) line . set_data ( x , posteriori . pdf ( x )) line . set_color ( cmap ( 1 - i / n )) line2 . set_data ([ posteriori . mean (), posteriori . mean ()], [ YMIN , YMAX ]) line2 . set_color ( cmap ( 1 - i / n )) return ( line , line2 ) anim = animation . FuncAnimation ( fig , animate , frames = n , init_func = init , interval = 100 , blit = True , fargs = ( alpha , beta , x , n ), repeat = False ) HTML ( anim . to_html5_video ()) Your browser does not support the video tag. Hiperpar\u00e2metros Seja \\Psi uma fam\u00edlia de distribui\u00e7\u00f5es poss\u00edveis sobre um espa\u00e7o de par\u00eamtros \\Omega . Suponha que independente da distribui\u00e7\u00e3o a priori dessa fam\u00edlia e n\u00e3o importando as observa\u00e7\u00f5es (quais s\u00e3o ou quantas s\u00e3o), a distribui\u00e7\u00e3o a posteriori seja da mesma fam\u00edlia. Chamamos \\Psi de fam\u00edlia conjunda de distribui\u00e7\u00f5es a priori. Os par\u00e2metros associados a essa fam\u00edlia s\u00e3o chamados de hiperpar\u00e2metros. Distribui\u00e7\u00f5es a Priori Impr\u00f3prias Seja \\xi uma fun\u00e7\u00e3o n\u00e3o negativa tal que \\Omega \u00e9 subconjunto de seu dom\u00ednio. Suponha que \\int \\xi(\\theta) d\\theta = \\infty . Se \\xi(\\theta) \u00e9 priori de \\theta , ela \u00e9 chamada de prioori impr\u00f3pria. Podemos gerar limites de distribui\u00e7\u00f5es, como, por exemplo, a distribui\u00e7\u00e3o uniforme [0,1] com intervalo sendo a reta, agora. Estimador de Bayes Estimador Seja X_1, ..., X_n dados observador cuja distribui\u00e7\u00e3o conjunta \u00e9 inndexada pelo par\u00e2metro \\theta . Um estimador do par\u00e2metro \\theta \u00e9 uma fun\u00e7\u00e3o real X_1, ..., X_n \\mapsto \\delta(X_1, ..., X_n) . Se X_i = x_i \u00e9 observado, \\delta(x_1,...,x_n) \u00e9 uma estimativa. Fun\u00e7\u00e3o de Perda \u00c9 uma fun\u00e7\u00e3o real L(\\theta, a) onde \\theta \\in \\Omega e a \\in \\mathbb{R} . Essa fun\u00e7\u00e3o procura indicar, para cada escoolha de \\theta , a perda do estat\u00edstico. Seja \\xi(\\theta) priori de \\theta . O valor esperado da perda \u00e9 dado por: E[L(\\theta, a)] = \\int_{\\Omega} L(\\theta, a)\\xi(\\theta) d\\theta, \\text{ a priori} E[L(\\theta, a)|x] = \\int_{\\Omega} L(\\theta, a)\\xi(\\theta|x) d\\theta, \\text{ a posteriori} Estimador de Bayes Seja L(\\theta, a) fun\u00e7\u00e3o de perda. Seja \\delta^*(x) o valor de a tal que E[L(\\theta, a)|x] \u00e9 minimizado. Ent\u00e3o \\delta^* \u00e9 o estimador de Bayes de \\theta . E[L(\\theta, \\delta^*(x))|x] = \\min_{a \\in \\mathbb{R}}E[L(\\theta, a)|x] Fun\u00e7\u00f5es de Perda: Exemplos Erro quadr\u00e1tico: L(\\theta, a) = (\\theta - a)^2 Queremos minimizar E[(\\theta - a)^2|x] \\delta^*(X) = E(\\theta| X) P\u00e1gina 260 (DeGroot) Erro absoluto: L(\\theta, a) = |\\theta - a| Queremos minimizar E[|\\theta - a||x] \\delta^*(X) = \\text{mediana (quartil 0.5)} P\u00e1gina 245 (DeGroot) Estimador Consistente Uma sequ\u00eancia de estimadores que converge em probabilidade para um valor desconhecido de um par\u00e2metro a ser estimafo \u00e9 chamado de sequ\u00eancia consistente de estimadores. Essa consist\u00eancia fala que em grandes amostras, o estimador estar\u00e1 pr\u00f3ximo o suficiente do valor desconhecido de \\theta . O estimador de Bayes, sob algumas condi\u00e7\u00f5es, forma uma sequ\u00eancia de estimadores consistentes. Limita\u00e7\u00f5es De acordo com a teoria Bayesiana , esse estimador \u00e9 o \u00fanico coerente que pode ser constru\u00eddo. \u00c9 importante que tenha-se definido uma fun\u00e7\u00e3o de perda e uma distribui\u00e7\u00e3o a priori para os par\u00e2metros. Quando \\theta \u00e9 um vetor, precisamos definit uma priori multivariada, mesmo que n\u00e3o queiramos estimar todos os par\u00e2metros. Exemplo 7.4.7 Quetelet reportou medidas do peito de 5732 homens militares. Os dados foram retirados desse site . import requests from bs4 import BeautifulSoup import pandas as pd Obtendo os dados direto do site Eu uso essas tr\u00eas bibliotecas, onde as duas primeiras s\u00e3o usadas para retirar informa\u00e7\u00e3o do site desejado. Veja que n\u00e3o coloco verifica\u00e7\u00e3o, pois o site tem esse problema. Depois eu coloco numa estrutura chamada DataFrame que \u00e9 basicamente uma tabela onde tem cada item nas linhas e cada caracter\u00edstica nas colunas. website = requests . get ( 'https://www.stat.cmu.edu/StatDat/Datafiles/MilitiamenChests.html' , verify = False ) soup = BeautifulSoup ( website . content ) data = soup . pre . text . strip () . split ( ' \\n ' ) chest_data = { 'Chest' : [], 'Count' : []} for item in data [ 1 :]: co , ch = item . split ( ' \\t ' ) chest_data [ 'Chest' ] . append ( int ( ch )) chest_data [ 'Count' ] . append ( int ( co )) chest_df = pd . DataFrame ( chest_data ) chest_df . head () /home/lucasmoschen/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings InsecureRequestWarning) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Chest Count 0 33 3 1 34 18 2 35 81 3 36 185 4 37 420 plt . bar ( chest_df [ 'Chest' ], chest_df [ 'Count' ]) mean = sum ( chest_df [ 'Chest' ] * chest_df [ 'Count' ]) / chest_df [ 'Count' ] . sum () plt . vlines ( mean , ymin = 0 , ymax = 1200 , color = 'black' , linestyle = '--' , label = 'M\u00e9dia= {:.2f} ' . format ( mean )) plt . title ( 'Histograma do tamanho do ppeito de militares escoseses' ) plt . xlabel ( 'Medida de peitorais' ) plt . legend () plt . show () Vamos modelar as medidas do peitoral, como uma amostra aleat\u00f3ria com distribui\u00e7\u00e3o normal com m\u00e9dia \\theta e vari\u00e2ncia \\sigma^2 , conhecido. Temos que a m\u00e9dia \u00e9 39.83 das amostras. Se \\theta \\sim N(\\mu_0, v_0^2) \u00e9 uma priori para \\theta , podemos calcular o estimador de Bayes a posteriori. Sabemos que a posteriori ser\u00e1 uma normal (conjugada) com m\u00e9dia e vari\u00e2ncia: \\mu_1 = \\frac{\\sigma^2 + 5732\\cdot v_0^2 \\cdot 39.83}{\\sigma^2 + 5732\\cdot v_0^2} v_1^2 = \\frac{\\sigma^2 v_0^2}{\\sigma^2 + 5732\\cdot v_0^2} O estimador de Bayes, segundo a perda quadr\u00e1tica, \u00e9 a m\u00e9dia a posteriori, portanto \\delta(x) = \\mu_1 Priori para \\theta \\mu_0 = 39.83 v_0^2 = 4","title":"Distribui\u00e7\u00f5es de Prioris Conjugadas"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#distribuicoes-de-prioris-conjugadas","text":"Se a distribui\u00e7\u00e3o a priori \u00e9 membro de uma fam\u00edlia e a distribui\u00e7\u00e3o a posteriori tamb\u00e9m pertence a mesma a fam\u00edlia, essa fam\u00edlia de distribui\u00e7\u00f5es \u00e9 chamada de fam\u00edlia conjugada . A principal consequ\u00eancia de usar prioris de uma fam\u00edlia conjugada \u00e9 que as contas ficam muito mais simples.","title":"Distribui\u00e7\u00f5es de Prioris Conjugadas"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#principais-familias-conjugadas","text":"","title":"Principais Fam\u00edlias Conjugadas"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#teorema","text":"Suponha que X_1, ..., X_n \\overset{iid}{\\sim} \\text{Bernoulli}(\\theta) , 0 < \\theta < 1 desconhecido. Suponha que \\theta \\sim \\text{Beta}(\\alpha, \\beta) . Ent\u00e3o, a distribui\u00e7\u00e3o a posteriori de \\theta \u00e9 a distribui\u00e7\u00e3o beta com par\u00e2metros \\alpha + \\sum_{i=1}^n x_i e \\beta + n - \\sum_{i=1}^n x_i .","title":"Teorema"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#teorema_1","text":"Suponha que X_1, ...., X_n \\overset{iid}{\\sim} \\text{Poisson}(\\theta) , onde \\theta \u00e9 desconhecido. Suponha tamb\u00e9m que a distribui\u00e7\u00e3o a priori de \\theta \u00e9 uma gamma com par\u00e2metros \\alpha e \\beta . Ent\u00e3o a distribui\u00e7\u00e3o a posteriori de \\theta sej\u00e1 gamma com par\u00e2metros \\alpha + \\sum_{i=1}^n x_i e \\beta + n .","title":"Teorema"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#teorema_2","text":"Suponha que X_1, ...., X_n \\overset{iid}{\\sim} \\text{Normal}(\\theta, \\sigma^2) , onde \\theta \u00e9 desconhecido e \\sigma \u00e9 conhecido. Suponha tamb\u00e9m que a distribui\u00e7\u00e3o a priori de \\theta \u00e9 uma normal com m\u00e9dia \\mu_0 e vari\u00e2ncia v_0^2 . Ent\u00e3o a distribui\u00e7\u00e3o a posteriori de \\theta sej\u00e1 normal com m\u00e9dia \\mu_1 e vari\u00e2ncia v_1^2 , onde: \\mu_1 = \\frac{\\sigma^2\\mu_0 + nv_0^2\\bar{x}_n}{\\sigma^2 + nv_0^2} v_1^2 = \\frac{\\sigma^2v_0^2}{\\sigma^2 + nv_0^2}","title":"Teorema"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#teorema_3","text":"Suponha que X_1, ...., X_n \\overset{iid}{\\sim} \\text{Exp}(\\theta) , onde \\theta \u00e9 desconhecido. Suponha tamb\u00e9m que a distribui\u00e7\u00e3o a priori de \\theta \u00e9 uma gamma com par\u00e2metros \\alpha e \\beta . Ent\u00e3o a distribui\u00e7\u00e3o a posteriori de \\theta sej\u00e1 gamma com par\u00e2metros \\alpha + n e \\beta + \\sum_{i=1}^n x_i . import numpy as np from scipy import stats import matplotlib.pyplot as plt from matplotlib import animation , cm from IPython.display import HTML % matplotlib inline Suponha que \\theta seja a probabilidade de um item ser defeituoso em uma s\u00e9rie de items. Suponha que nossa priori em \\theta \u00e9 uma distribui\u00e7\u00e3o beta com par\u00e2metros \\alpha e \\beta . S\u00e3o selecionados n items por vez para o teste. Sabemos que nossa posteriori ser\u00e1 uma Beta com os par\u00e2metros acima. Vejamos graficamente esse processo. theta_real = 0.1 n = 100 np . random . seed ( 10 ) XMIN = 0 XMAX = 1 YMIN = 0 YMAX = 5 alpha = [ 1 ] beta = [ 1 ] x = np . linspace ( 0.001 , 1 , 1000 ) # Definindo cores cmap = cm . autumn # Esta fun\u00e7\u00e3o permite plotar o backgroud def init (): line . set_data ([], []) return ( line ,) # Definindo o espa\u00e7o da imagem fig , ax = plt . subplots () # Definindo caracter\u00edsticas do background ax . set_xlim (( XMIN , XMAX )) ax . set_ylim (( YMIN , YMAX )) ax . set_title ( 'Evolu\u00e7\u00e3o da posteriori a cada itera\u00e7\u00e3o' ) ax . vlines ( theta_real , ymin = YMIN , ymax = YMAX , linestyle = '--' , color = 'grey' ) # Definindo plots vari\u00e1veis line , = ax . plot ([], [], lw = 2 ) line2 , _ = ax . plot ( XMIN , XMAX , YMIN , YMAX , linestyle = ':' ) def animate ( i , alpha , beta , x , n ): # Amostro da distribui\u00e7\u00e3o sample = np . random . binomial ( 1 , p = theta_real ) # Junto a lista que guarda os alphas e betas de cada itera\u00e7\u00e3o alpha . append ( alpha [ - 1 ] + sample ) beta . append ( beta [ - 1 ] + 1 - sample ) # Calculo a posteriori posteriori = stats . beta ( a = alpha [ - 1 ], b = beta [ - 1 ]) line . set_data ( x , posteriori . pdf ( x )) line . set_color ( cmap ( 1 - i / n )) line2 . set_data ([ posteriori . mean (), posteriori . mean ()], [ YMIN , YMAX ]) line2 . set_color ( cmap ( 1 - i / n )) return ( line , line2 ) anim = animation . FuncAnimation ( fig , animate , frames = n , init_func = init , interval = 100 , blit = True , fargs = ( alpha , beta , x , n ), repeat = False ) HTML ( anim . to_html5_video ()) Your browser does not support the video tag.","title":"Teorema"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#hiperparametros","text":"Seja \\Psi uma fam\u00edlia de distribui\u00e7\u00f5es poss\u00edveis sobre um espa\u00e7o de par\u00eamtros \\Omega . Suponha que independente da distribui\u00e7\u00e3o a priori dessa fam\u00edlia e n\u00e3o importando as observa\u00e7\u00f5es (quais s\u00e3o ou quantas s\u00e3o), a distribui\u00e7\u00e3o a posteriori seja da mesma fam\u00edlia. Chamamos \\Psi de fam\u00edlia conjunda de distribui\u00e7\u00f5es a priori. Os par\u00e2metros associados a essa fam\u00edlia s\u00e3o chamados de hiperpar\u00e2metros.","title":"Hiperpar\u00e2metros"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#distribuicoes-a-priori-improprias","text":"Seja \\xi uma fun\u00e7\u00e3o n\u00e3o negativa tal que \\Omega \u00e9 subconjunto de seu dom\u00ednio. Suponha que \\int \\xi(\\theta) d\\theta = \\infty . Se \\xi(\\theta) \u00e9 priori de \\theta , ela \u00e9 chamada de prioori impr\u00f3pria. Podemos gerar limites de distribui\u00e7\u00f5es, como, por exemplo, a distribui\u00e7\u00e3o uniforme [0,1] com intervalo sendo a reta, agora.","title":"Distribui\u00e7\u00f5es a Priori Impr\u00f3prias"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#estimador-de-bayes","text":"","title":"Estimador de Bayes"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#estimador","text":"Seja X_1, ..., X_n dados observador cuja distribui\u00e7\u00e3o conjunta \u00e9 inndexada pelo par\u00e2metro \\theta . Um estimador do par\u00e2metro \\theta \u00e9 uma fun\u00e7\u00e3o real X_1, ..., X_n \\mapsto \\delta(X_1, ..., X_n) . Se X_i = x_i \u00e9 observado, \\delta(x_1,...,x_n) \u00e9 uma estimativa.","title":"Estimador"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#funcao-de-perda","text":"\u00c9 uma fun\u00e7\u00e3o real L(\\theta, a) onde \\theta \\in \\Omega e a \\in \\mathbb{R} . Essa fun\u00e7\u00e3o procura indicar, para cada escoolha de \\theta , a perda do estat\u00edstico. Seja \\xi(\\theta) priori de \\theta . O valor esperado da perda \u00e9 dado por: E[L(\\theta, a)] = \\int_{\\Omega} L(\\theta, a)\\xi(\\theta) d\\theta, \\text{ a priori} E[L(\\theta, a)|x] = \\int_{\\Omega} L(\\theta, a)\\xi(\\theta|x) d\\theta, \\text{ a posteriori}","title":"Fun\u00e7\u00e3o de Perda"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#estimador-de-bayes_1","text":"Seja L(\\theta, a) fun\u00e7\u00e3o de perda. Seja \\delta^*(x) o valor de a tal que E[L(\\theta, a)|x] \u00e9 minimizado. Ent\u00e3o \\delta^* \u00e9 o estimador de Bayes de \\theta . E[L(\\theta, \\delta^*(x))|x] = \\min_{a \\in \\mathbb{R}}E[L(\\theta, a)|x]","title":"Estimador de Bayes"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#funcoes-de-perda-exemplos","text":"Erro quadr\u00e1tico: L(\\theta, a) = (\\theta - a)^2 Queremos minimizar E[(\\theta - a)^2|x] \\delta^*(X) = E(\\theta| X) P\u00e1gina 260 (DeGroot) Erro absoluto: L(\\theta, a) = |\\theta - a| Queremos minimizar E[|\\theta - a||x] \\delta^*(X) = \\text{mediana (quartil 0.5)} P\u00e1gina 245 (DeGroot)","title":"Fun\u00e7\u00f5es de Perda: Exemplos"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#estimador-consistente","text":"Uma sequ\u00eancia de estimadores que converge em probabilidade para um valor desconhecido de um par\u00e2metro a ser estimafo \u00e9 chamado de sequ\u00eancia consistente de estimadores. Essa consist\u00eancia fala que em grandes amostras, o estimador estar\u00e1 pr\u00f3ximo o suficiente do valor desconhecido de \\theta . O estimador de Bayes, sob algumas condi\u00e7\u00f5es, forma uma sequ\u00eancia de estimadores consistentes.","title":"Estimador Consistente"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#limitacoes","text":"De acordo com a teoria Bayesiana , esse estimador \u00e9 o \u00fanico coerente que pode ser constru\u00eddo. \u00c9 importante que tenha-se definido uma fun\u00e7\u00e3o de perda e uma distribui\u00e7\u00e3o a priori para os par\u00e2metros. Quando \\theta \u00e9 um vetor, precisamos definit uma priori multivariada, mesmo que n\u00e3o queiramos estimar todos os par\u00e2metros.","title":"Limita\u00e7\u00f5es"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#exemplo-747","text":"Quetelet reportou medidas do peito de 5732 homens militares. Os dados foram retirados desse site . import requests from bs4 import BeautifulSoup import pandas as pd","title":"Exemplo 7.4.7"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#obtendo-os-dados-direto-do-site","text":"Eu uso essas tr\u00eas bibliotecas, onde as duas primeiras s\u00e3o usadas para retirar informa\u00e7\u00e3o do site desejado. Veja que n\u00e3o coloco verifica\u00e7\u00e3o, pois o site tem esse problema. Depois eu coloco numa estrutura chamada DataFrame que \u00e9 basicamente uma tabela onde tem cada item nas linhas e cada caracter\u00edstica nas colunas. website = requests . get ( 'https://www.stat.cmu.edu/StatDat/Datafiles/MilitiamenChests.html' , verify = False ) soup = BeautifulSoup ( website . content ) data = soup . pre . text . strip () . split ( ' \\n ' ) chest_data = { 'Chest' : [], 'Count' : []} for item in data [ 1 :]: co , ch = item . split ( ' \\t ' ) chest_data [ 'Chest' ] . append ( int ( ch )) chest_data [ 'Count' ] . append ( int ( co )) chest_df = pd . DataFrame ( chest_data ) chest_df . head () /home/lucasmoschen/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings InsecureRequestWarning) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Chest Count 0 33 3 1 34 18 2 35 81 3 36 185 4 37 420 plt . bar ( chest_df [ 'Chest' ], chest_df [ 'Count' ]) mean = sum ( chest_df [ 'Chest' ] * chest_df [ 'Count' ]) / chest_df [ 'Count' ] . sum () plt . vlines ( mean , ymin = 0 , ymax = 1200 , color = 'black' , linestyle = '--' , label = 'M\u00e9dia= {:.2f} ' . format ( mean )) plt . title ( 'Histograma do tamanho do ppeito de militares escoseses' ) plt . xlabel ( 'Medida de peitorais' ) plt . legend () plt . show () Vamos modelar as medidas do peitoral, como uma amostra aleat\u00f3ria com distribui\u00e7\u00e3o normal com m\u00e9dia \\theta e vari\u00e2ncia \\sigma^2 , conhecido. Temos que a m\u00e9dia \u00e9 39.83 das amostras. Se \\theta \\sim N(\\mu_0, v_0^2) \u00e9 uma priori para \\theta , podemos calcular o estimador de Bayes a posteriori. Sabemos que a posteriori ser\u00e1 uma normal (conjugada) com m\u00e9dia e vari\u00e2ncia: \\mu_1 = \\frac{\\sigma^2 + 5732\\cdot v_0^2 \\cdot 39.83}{\\sigma^2 + 5732\\cdot v_0^2} v_1^2 = \\frac{\\sigma^2 v_0^2}{\\sigma^2 + 5732\\cdot v_0^2} O estimador de Bayes, segundo a perda quadr\u00e1tica, \u00e9 a m\u00e9dia a posteriori, portanto \\delta(x) = \\mu_1","title":"Obtendo os dados direto do site"},{"location":"infestatistica/ConjugateDistributions/ConjugateDistributions/#priori-para-theta","text":"\\mu_0 = 39.83 v_0^2 = 4","title":"Priori para \\theta"},{"location":"infestatistica/ExampleHypothesesTesting/ExampleHypothesesTesting/","text":"Simples Exemplos em Teste de Hip\u00f3teses Refer\u00eancia: Dr. Tirthajyoti Sarkar , Fremont, 94536 Vamos lembrar que nosso objetivo \u00e9 testar hip\u00f3teses cient\u00edficas com uma formula\u00e7\u00e3o estat\u00edstica, a fim de fazer alguma infer\u00eancia sobre os par\u00e2metros de algum modelo. Os testes que ser\u00e3o discutidos nesse notebook s\u00e3o: Propor\u00e7\u00e3o de uma popula\u00e7\u00e3o. Deferen\u00e7a entre propor\u00e7\u00e3o de popula\u00e7\u00f5es M\u00e9dia de uma popula\u00e7\u00e3o. Diferen\u00e7a enre m\u00e9dias de popula\u00e7\u00f5es. import statsmodels.api as sm import numpy as np import matplotlib.pyplot as plt import pandas as pd Propor\u00e7\u00e3o de uma Popula\u00e7\u00e3o Quest\u00e3o : Em anos anteriores, 52% dos pais acreditavam que a falta de sono era causada por eletr\u00f4nicos e as m\u00e9dias sociais em seus filhos e filhas adolescentes. E agora, como essa propor\u00e7\u00e3o se encontra? Popula\u00e7\u00e3o : Pais com filhos e filhas adolescentes de 13 a 18 anos. Par\u00e2metro de interesse: p H_0: p \\le 0.52 e H_1: p > 0.52 . Dados: Pesquisa entre 1018 pessoas: 56% acreditam agora. Teste Z para propor\u00e7\u00f5es Vamos usar um teste chamado teste Z, considerando que $X_1 , ..., X_n \\sim Bernoulli(p) Z = \\frac{\\bar{X}_n - \\pi_0}{\\sqrt{(\\pi_0(1 - \\pi_0)/n)}} Onde \\pi_0 \u00e9 o valor limiar, no caso 0.52. Observe que quando p = \\pi_0 , a estat\u00edstica Z \u00e9 aproximadamente normal padr\u00e3o quando n cresce. O que esse teste mensura? No denominador, temos o desvio padr\u00e3o de quando p = \\pi_0 . Portanto, estamos medindo a dist\u00e2ncia entre a m\u00e9dia amostral e o limiar em unidades de desvio padr\u00e3o. Nosso procedimento de teste ser\u00e1, portanto, se Z \\ge c , rejeitamos H_0 , mas com n\u00edvel de signific\u00e2ncia \\alpha_0 , isto \u00e9: P(Z \\ge c|p \\le \\pi_0) \\le \\alpha_0 Podemos conferir que \\begin{split} P(Z \\ge c) &= P\\left(\\frac{\\bar{X}_n - \\pi_0}{\\sqrt{(\\pi_0(1 - \\pi_0)/n)}} \\ge c\\right) \\\\ &= P\\left(\\frac{\\bar{X}_n - p}{\\sqrt{(p(1 - p)/n)}}\\frac{\\sqrt{(p(1 - p)/n)}}{\\sqrt{(\\pi_0(1 - \\pi_0)/n)}} + \\frac{p - \\pi_0}{\\sqrt{(\\pi_0(1 - \\pi_0)/n)}} \\ge c\\right) \\\\ &= P\\left(\\frac{\\bar{X}_n - p}{\\sqrt{(p(1 - p)/n)}} \\ge c\\frac{\\sqrt{\\pi_0(1 - \\pi_0)}}{\\sqrt{p(1 - p)}} - \\frac{p - \\pi_0}{\\sqrt{(p(1 - p)/n)}}\\right) \\\\ &\\approx 1 - \\Phi\\left(c\\frac{\\sqrt{\\pi_0(1 - \\pi_0)}}{\\sqrt{p(1 - p)}} - \\frac{p - \\pi_0}{\\sqrt{(p(1 - p)/n)}}\\right), \\text{ pelo Teo. Central do Limite} \\end{split} Assim podemos encontrar \\alpha_0 a partir do m\u00e1ximo que essa quantidade assume (Mas com uma obseeva\u00e7\u00e3o bem detalhada, quando n \u00e9 grande, p = 0.52 \u00e9 o maximizador) Mas como isso funciona na pr\u00e1tica, propriamente dito. Tenho que escolher um valor limiar para o p-valor, isto \u00e9, se p-valor for menor do que esse limiar, eu rejeito a hip\u00f3tese. Vou ficar esse limiar em 0.05, mas isso \u00e9 arbitr\u00e1rio, apesar da literatura costumar us\u00e1-lo. n = 1018 pnull = . 52 phat = . 56 ztest , pvalue = sm . stats . proportions_ztest ( count = phat * n , #n\u00famero de sucessos nobs = n , #n\u00famero de observa\u00e7\u00f5es value = pnull , #pi_0 alternative = 'larger' ) #hip\u00f3tese alternativa print ( 'O valor da estat\u00edstica de teste foi {} e o p-valor {} ' . format ( ztest , pvalue )) O valor da estat\u00edstica de teste foi 0.6392739759907055 e o p-valor 0.26132235751888716 Conclus\u00e3o do teste de hip\u00f3teses Como o p-valor foi menor do que nosso limiar, n\u00f3s temos evid\u00eancia para rejeitar a hip\u00f3tese nula que dizia que a prpor\u00e7\u00e3o teria permanecido ou at\u00e9 diminu\u00eddo. Isso n\u00e3o significa que aceitamos a hip\u00f3tese alternativa, apenas que temos evid\u00eancia para acreditar que a propor\u00e7\u00e3o seja maior do que 0.52. n = 1018 pnull = . 52 phat = . 53 ztest , pvalue = sm . stats . proportions_ztest ( count = phat * n , #n\u00famero de sucessos nobs = n , #n\u00famero de observa\u00e7\u00f5es value = pnull , #pi_0 alternative = 'larger' ) #hip\u00f3tese alternativa print ( 'O valor da estat\u00edstica de teste foi {} e o p-valor {} ' . format ( ztest , pvalue )) O valor da estat\u00edstica de teste foi 0.6392739759907055 e o p-valor 0.26132235751888716 Diferen\u00e7a entre propo\u00e7\u00f5es de duas popula\u00e7\u00f5es Quest\u00e3o : Existe diferen\u00e7a significativa entre pais ingleses e pais alem\u00e3es que reportaram que seus filhos e filhas tiveram aulas de nata\u00e7\u00e3o? Popula\u00e7\u00e3o : Pais com filhos e filhas ingleses e alem\u00e3es. Par\u00e2metro de interesse: p_{ingleses} = p_i e p_{alemaes} = p_a . H_0: p_i - p_a = 0 e H_1: p_i - p_a \\neq 0 . Dados: 247 pais ingleses responderam e dentre eles 36.8% reportaram que sim. Pais alem\u00e3es foram 308, 38.9% que disseram sim. Teste T para propor\u00e7\u00f5es Na verdade, poder\u00edamos usar o teste Z, com a mesma ideia, s\u00f3 que nesse caso, ter\u00edamos que tomar um pouco de cuidado com o denominador, dado que agora existem duas m\u00e9dias, ent\u00e3o nosso estimador para o desvio padr\u00e3o deve levar em conta esses dois fatores e n deve ser suficientemente grande para que n\u00e3o tenhamos problema. Para evitar isso, vamos usar o T teste. A estat\u00edstica de teste \u00e9 a seguinte, tratando como X e Y as duas amostras consideradas. t = \\frac{\\bar{X}_n - \\bar{Y}_m}{SE} Nesse caso SE \u00e9 o erro entre a diferen\u00e7a entre a m\u00e9dias: isso tem um pequeno problema quando m \\neq n . Ent\u00e3o fazer as contas no papel n\u00e3o \u00e9 trivial. Confira aqui as defini\u00e7\u00f5es de SE precisas. Como nossa inten\u00e7\u00e3o \u00e9 apenas usar esse teste, vamos mostrar como isso pode ser pr\u00e1tico. n = 247 pi = . 368 m = 308 pa = . 389 # Gerando as popula\u00e7\u00f5es england = np . random . binomial ( n = 1 , p = pi , size = n ) germany = np . random . binomial ( n = 1 , p = pa , size = m ) _ , p_value , _ = sm . stats . ttest_ind ( england , germany ) print ( 'O p-valor foi {} ' . format ( p_value )) O p-valor foi 0.2615435082780627 Conclus\u00e3o sobre o teste de hip\u00f3teses Dado que o p-valor \u00e9 maior do que nosso limiar, n\u00e3o podemos rejeitar a hip\u00f3tese nula. Nesse caso, a diferen\u00e7a das propor\u00e7\u00f5es nas popu\u00e7a\u00e7\u00f5es n\u00e3o foi nada mais do que meramente uma aleatoriedade. Mas o que acontece se essas propor\u00e7\u00f5es se mantiveram para mais pessoas? n = 5000 pi = . 37 m = 5000 pa = . 389 england = np . random . binomial ( n = 1 , p = pi , size = n ) germany = np . random . binomial ( n = 1 , p = pa , size = m ) _ , p_value , _ = sm . stats . ttest_ind ( england , germany ) print ( 'O p-valor foi {} ' . format ( p_value )) O p-valor foi 0.027721099791980015 Diferen\u00e7a entre m\u00e9dias de popula\u00e7\u00f5es Quest\u00e3o : Considerando os adultos nos dados da NHAMES , homens tem maior m\u00e9dia de \u00cdndice de Massa Corp\u00f3rea do que mulheres? Popula\u00e7\u00e3o : Adultos na base NHAMES. Par\u00e2metro de interesse: \\mu_{homens} = \\mu_h e \\mu_{mulheres} = \\mu_m . H_0: \\mu_1 = \\mu_2 e H_1: \\mu_1 \\neq \\mu_2 . Dados: 2976 mulheres adultas \\hat{\\mu}_m = 29.94 \\hat{\\sigma}_m = 7.75 2759 homens adultos \\hat{\\mu}_h = 28.78 \\hat{\\sigma}_h = 6.25 url = \"https://raw.githubusercontent.com/kshedden/statswpy/master/NHANES/merged/nhanes_2015_2016.csv\" da = pd . read_csv ( url ) da . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } SEQN ALQ101 ALQ110 ALQ130 SMQ020 RIAGENDR RIDAGEYR RIDRETH1 DMDCITZN DMDEDUC2 ... BPXSY2 BPXDI2 BMXWT BMXHT BMXBMI BMXLEG BMXARML BMXARMC BMXWAIST HIQ210 0 83732 1.0 NaN 1.0 1 1 62 3 1.0 5.0 ... 124.0 64.0 94.8 184.5 27.8 43.3 43.6 35.9 101.1 2.0 1 83733 1.0 NaN 6.0 1 1 53 3 2.0 3.0 ... 140.0 88.0 90.4 171.4 30.8 38.0 40.0 33.2 107.9 NaN 2 83734 1.0 NaN NaN 1 1 78 3 1.0 3.0 ... 132.0 44.0 83.4 170.1 28.8 35.6 37.0 31.0 116.5 2.0 3 83735 2.0 1.0 1.0 2 2 56 3 1.0 5.0 ... 134.0 68.0 109.8 160.9 42.4 38.5 37.7 38.3 110.1 2.0 4 83736 2.0 1.0 1.0 2 2 42 4 1.0 4.0 ... 114.0 54.0 55.2 164.9 20.3 37.4 36.0 27.2 80.4 2.0 5 rows \u00d7 28 columns females = da [ da [ \"RIAGENDR\" ] == 2 ] male = da [ da [ \"RIAGENDR\" ] == 1 ] n_m = len ( females ) mu_m = females [ \"BMXBMI\" ] . mean () sd_m = females [ \"BMXBMI\" ] . std () ( n_m , mu_m , sd_m ) (2976, 29.939945652173996, 7.75331880954568) n_h = len ( male ) mu_h = male [ \"BMXBMI\" ] . mean () sd_h = male [ \"BMXBMI\" ] . std () ( n_h , mu_h , sd_h ) (2759, 28.778072111846985, 6.252567616801485) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax [ 0 ] . set_title ( \"Histogram BMI para mulheres\" , fontsize = 16 ) ax [ 0 ] . hist ( females [ \"BMXBMI\" ] . dropna (), edgecolor = 'k' , color = 'darkred' , bins = 25 ) ax [ 1 ] . set_title ( \"Histogram BMI para homens\" , fontsize = 16 ) ax [ 1 ] . hist ( male [ \"BMXBMI\" ] . dropna (), edgecolor = 'k' , color = 'green' , bins = 25 ) plt . show () Vamos usar o t-test descrito em Welch . Temos que us\u00e1-lo porque n\u00e3o conhecemos a vari\u00e2ncia. Se conhecessemos, poder\u00edamos usar o teste normal mesmo e se soub\u00e9ssemos que s\u00e3o iguais, mas desconhecessemos, poder\u00edamos usar o ttest ques estudamos no cap\u00edtulo 9.5 . sm . stats . ttest_ind ( x1 = females [ \"BMXBMI\" ] . dropna (), x2 = male [ \"BMXBMI\" ] . dropna (), alternative = 'two-sided' , value = 0 ) # diferen\u00e7a na hip\u00f3tese nula (6.175593353138302, 7.050275578095374e-10, 5660.0) Conclus\u00e3o no teste de hip\u00f3teses Como o p-valor \u00e9 bem pequeno, n\u00f3s podemos rejeitar a hip\u00f3tese nula, o que significa que existe diferen\u00e7a estat\u00edstica entre as m\u00e9dias. Isso n\u00e3o responde se a o \u00edndice \u00e9 mais alto para homens, mas podemos fazer o teste unilateral e perceber que de fato isso de fato acontece segundo os dados. sm . stats . ttest_ind ( x1 = females [ \"BMXBMI\" ] . dropna (), x2 = male [ \"BMXBMI\" ] . dropna (), alternative = 'larger' , value = 0 ) # diferen\u00e7a na hip\u00f3tese nula (6.175593353138302, 3.525137789047687e-10, 5660.0)","title":"Simples Exemplos em Teste de Hip\u00f3teses"},{"location":"infestatistica/ExampleHypothesesTesting/ExampleHypothesesTesting/#simples-exemplos-em-teste-de-hipoteses","text":"","title":"Simples Exemplos em Teste de Hip\u00f3teses"},{"location":"infestatistica/ExampleHypothesesTesting/ExampleHypothesesTesting/#referencia-dr-tirthajyoti-sarkar-fremont-94536","text":"Vamos lembrar que nosso objetivo \u00e9 testar hip\u00f3teses cient\u00edficas com uma formula\u00e7\u00e3o estat\u00edstica, a fim de fazer alguma infer\u00eancia sobre os par\u00e2metros de algum modelo. Os testes que ser\u00e3o discutidos nesse notebook s\u00e3o: Propor\u00e7\u00e3o de uma popula\u00e7\u00e3o. Deferen\u00e7a entre propor\u00e7\u00e3o de popula\u00e7\u00f5es M\u00e9dia de uma popula\u00e7\u00e3o. Diferen\u00e7a enre m\u00e9dias de popula\u00e7\u00f5es. import statsmodels.api as sm import numpy as np import matplotlib.pyplot as plt import pandas as pd","title":"Refer\u00eancia:  Dr. Tirthajyoti Sarkar, Fremont, 94536"},{"location":"infestatistica/ExampleHypothesesTesting/ExampleHypothesesTesting/#proporcao-de-uma-populacao","text":"Quest\u00e3o : Em anos anteriores, 52% dos pais acreditavam que a falta de sono era causada por eletr\u00f4nicos e as m\u00e9dias sociais em seus filhos e filhas adolescentes. E agora, como essa propor\u00e7\u00e3o se encontra? Popula\u00e7\u00e3o : Pais com filhos e filhas adolescentes de 13 a 18 anos. Par\u00e2metro de interesse: p H_0: p \\le 0.52 e H_1: p > 0.52 . Dados: Pesquisa entre 1018 pessoas: 56% acreditam agora.","title":"Propor\u00e7\u00e3o de uma Popula\u00e7\u00e3o"},{"location":"infestatistica/ExampleHypothesesTesting/ExampleHypothesesTesting/#teste-z-para-proporcoes","text":"Vamos usar um teste chamado teste Z, considerando que $X_1 , ..., X_n \\sim Bernoulli(p) Z = \\frac{\\bar{X}_n - \\pi_0}{\\sqrt{(\\pi_0(1 - \\pi_0)/n)}} Onde \\pi_0 \u00e9 o valor limiar, no caso 0.52. Observe que quando p = \\pi_0 , a estat\u00edstica Z \u00e9 aproximadamente normal padr\u00e3o quando n cresce. O que esse teste mensura? No denominador, temos o desvio padr\u00e3o de quando p = \\pi_0 . Portanto, estamos medindo a dist\u00e2ncia entre a m\u00e9dia amostral e o limiar em unidades de desvio padr\u00e3o. Nosso procedimento de teste ser\u00e1, portanto, se Z \\ge c , rejeitamos H_0 , mas com n\u00edvel de signific\u00e2ncia \\alpha_0 , isto \u00e9: P(Z \\ge c|p \\le \\pi_0) \\le \\alpha_0 Podemos conferir que \\begin{split} P(Z \\ge c) &= P\\left(\\frac{\\bar{X}_n - \\pi_0}{\\sqrt{(\\pi_0(1 - \\pi_0)/n)}} \\ge c\\right) \\\\ &= P\\left(\\frac{\\bar{X}_n - p}{\\sqrt{(p(1 - p)/n)}}\\frac{\\sqrt{(p(1 - p)/n)}}{\\sqrt{(\\pi_0(1 - \\pi_0)/n)}} + \\frac{p - \\pi_0}{\\sqrt{(\\pi_0(1 - \\pi_0)/n)}} \\ge c\\right) \\\\ &= P\\left(\\frac{\\bar{X}_n - p}{\\sqrt{(p(1 - p)/n)}} \\ge c\\frac{\\sqrt{\\pi_0(1 - \\pi_0)}}{\\sqrt{p(1 - p)}} - \\frac{p - \\pi_0}{\\sqrt{(p(1 - p)/n)}}\\right) \\\\ &\\approx 1 - \\Phi\\left(c\\frac{\\sqrt{\\pi_0(1 - \\pi_0)}}{\\sqrt{p(1 - p)}} - \\frac{p - \\pi_0}{\\sqrt{(p(1 - p)/n)}}\\right), \\text{ pelo Teo. Central do Limite} \\end{split} Assim podemos encontrar \\alpha_0 a partir do m\u00e1ximo que essa quantidade assume (Mas com uma obseeva\u00e7\u00e3o bem detalhada, quando n \u00e9 grande, p = 0.52 \u00e9 o maximizador) Mas como isso funciona na pr\u00e1tica, propriamente dito. Tenho que escolher um valor limiar para o p-valor, isto \u00e9, se p-valor for menor do que esse limiar, eu rejeito a hip\u00f3tese. Vou ficar esse limiar em 0.05, mas isso \u00e9 arbitr\u00e1rio, apesar da literatura costumar us\u00e1-lo. n = 1018 pnull = . 52 phat = . 56 ztest , pvalue = sm . stats . proportions_ztest ( count = phat * n , #n\u00famero de sucessos nobs = n , #n\u00famero de observa\u00e7\u00f5es value = pnull , #pi_0 alternative = 'larger' ) #hip\u00f3tese alternativa print ( 'O valor da estat\u00edstica de teste foi {} e o p-valor {} ' . format ( ztest , pvalue )) O valor da estat\u00edstica de teste foi 0.6392739759907055 e o p-valor 0.26132235751888716","title":"Teste Z para propor\u00e7\u00f5es"},{"location":"infestatistica/ExampleHypothesesTesting/ExampleHypothesesTesting/#conclusao-do-teste-de-hipoteses","text":"Como o p-valor foi menor do que nosso limiar, n\u00f3s temos evid\u00eancia para rejeitar a hip\u00f3tese nula que dizia que a prpor\u00e7\u00e3o teria permanecido ou at\u00e9 diminu\u00eddo. Isso n\u00e3o significa que aceitamos a hip\u00f3tese alternativa, apenas que temos evid\u00eancia para acreditar que a propor\u00e7\u00e3o seja maior do que 0.52. n = 1018 pnull = . 52 phat = . 53 ztest , pvalue = sm . stats . proportions_ztest ( count = phat * n , #n\u00famero de sucessos nobs = n , #n\u00famero de observa\u00e7\u00f5es value = pnull , #pi_0 alternative = 'larger' ) #hip\u00f3tese alternativa print ( 'O valor da estat\u00edstica de teste foi {} e o p-valor {} ' . format ( ztest , pvalue )) O valor da estat\u00edstica de teste foi 0.6392739759907055 e o p-valor 0.26132235751888716","title":"Conclus\u00e3o do teste de hip\u00f3teses"},{"location":"infestatistica/ExampleHypothesesTesting/ExampleHypothesesTesting/#diferenca-entre-propocoes-de-duas-populacoes","text":"Quest\u00e3o : Existe diferen\u00e7a significativa entre pais ingleses e pais alem\u00e3es que reportaram que seus filhos e filhas tiveram aulas de nata\u00e7\u00e3o? Popula\u00e7\u00e3o : Pais com filhos e filhas ingleses e alem\u00e3es. Par\u00e2metro de interesse: p_{ingleses} = p_i e p_{alemaes} = p_a . H_0: p_i - p_a = 0 e H_1: p_i - p_a \\neq 0 . Dados: 247 pais ingleses responderam e dentre eles 36.8% reportaram que sim. Pais alem\u00e3es foram 308, 38.9% que disseram sim.","title":"Diferen\u00e7a entre propo\u00e7\u00f5es de duas popula\u00e7\u00f5es"},{"location":"infestatistica/ExampleHypothesesTesting/ExampleHypothesesTesting/#teste-t-para-proporcoes","text":"Na verdade, poder\u00edamos usar o teste Z, com a mesma ideia, s\u00f3 que nesse caso, ter\u00edamos que tomar um pouco de cuidado com o denominador, dado que agora existem duas m\u00e9dias, ent\u00e3o nosso estimador para o desvio padr\u00e3o deve levar em conta esses dois fatores e n deve ser suficientemente grande para que n\u00e3o tenhamos problema. Para evitar isso, vamos usar o T teste. A estat\u00edstica de teste \u00e9 a seguinte, tratando como X e Y as duas amostras consideradas. t = \\frac{\\bar{X}_n - \\bar{Y}_m}{SE} Nesse caso SE \u00e9 o erro entre a diferen\u00e7a entre a m\u00e9dias: isso tem um pequeno problema quando m \\neq n . Ent\u00e3o fazer as contas no papel n\u00e3o \u00e9 trivial. Confira aqui as defini\u00e7\u00f5es de SE precisas. Como nossa inten\u00e7\u00e3o \u00e9 apenas usar esse teste, vamos mostrar como isso pode ser pr\u00e1tico. n = 247 pi = . 368 m = 308 pa = . 389 # Gerando as popula\u00e7\u00f5es england = np . random . binomial ( n = 1 , p = pi , size = n ) germany = np . random . binomial ( n = 1 , p = pa , size = m ) _ , p_value , _ = sm . stats . ttest_ind ( england , germany ) print ( 'O p-valor foi {} ' . format ( p_value )) O p-valor foi 0.2615435082780627","title":"Teste T para propor\u00e7\u00f5es"},{"location":"infestatistica/ExampleHypothesesTesting/ExampleHypothesesTesting/#conclusao-sobre-o-teste-de-hipoteses","text":"Dado que o p-valor \u00e9 maior do que nosso limiar, n\u00e3o podemos rejeitar a hip\u00f3tese nula. Nesse caso, a diferen\u00e7a das propor\u00e7\u00f5es nas popu\u00e7a\u00e7\u00f5es n\u00e3o foi nada mais do que meramente uma aleatoriedade.","title":"Conclus\u00e3o sobre o teste de hip\u00f3teses"},{"location":"infestatistica/ExampleHypothesesTesting/ExampleHypothesesTesting/#mas-o-que-acontece-se-essas-proporcoes-se-mantiveram-para-mais-pessoas","text":"n = 5000 pi = . 37 m = 5000 pa = . 389 england = np . random . binomial ( n = 1 , p = pi , size = n ) germany = np . random . binomial ( n = 1 , p = pa , size = m ) _ , p_value , _ = sm . stats . ttest_ind ( england , germany ) print ( 'O p-valor foi {} ' . format ( p_value )) O p-valor foi 0.027721099791980015","title":"Mas o que acontece se essas propor\u00e7\u00f5es se mantiveram para mais pessoas?"},{"location":"infestatistica/ExampleHypothesesTesting/ExampleHypothesesTesting/#diferenca-entre-medias-de-populacoes","text":"Quest\u00e3o : Considerando os adultos nos dados da NHAMES , homens tem maior m\u00e9dia de \u00cdndice de Massa Corp\u00f3rea do que mulheres? Popula\u00e7\u00e3o : Adultos na base NHAMES. Par\u00e2metro de interesse: \\mu_{homens} = \\mu_h e \\mu_{mulheres} = \\mu_m . H_0: \\mu_1 = \\mu_2 e H_1: \\mu_1 \\neq \\mu_2 . Dados: 2976 mulheres adultas \\hat{\\mu}_m = 29.94 \\hat{\\sigma}_m = 7.75 2759 homens adultos \\hat{\\mu}_h = 28.78 \\hat{\\sigma}_h = 6.25 url = \"https://raw.githubusercontent.com/kshedden/statswpy/master/NHANES/merged/nhanes_2015_2016.csv\" da = pd . read_csv ( url ) da . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } SEQN ALQ101 ALQ110 ALQ130 SMQ020 RIAGENDR RIDAGEYR RIDRETH1 DMDCITZN DMDEDUC2 ... BPXSY2 BPXDI2 BMXWT BMXHT BMXBMI BMXLEG BMXARML BMXARMC BMXWAIST HIQ210 0 83732 1.0 NaN 1.0 1 1 62 3 1.0 5.0 ... 124.0 64.0 94.8 184.5 27.8 43.3 43.6 35.9 101.1 2.0 1 83733 1.0 NaN 6.0 1 1 53 3 2.0 3.0 ... 140.0 88.0 90.4 171.4 30.8 38.0 40.0 33.2 107.9 NaN 2 83734 1.0 NaN NaN 1 1 78 3 1.0 3.0 ... 132.0 44.0 83.4 170.1 28.8 35.6 37.0 31.0 116.5 2.0 3 83735 2.0 1.0 1.0 2 2 56 3 1.0 5.0 ... 134.0 68.0 109.8 160.9 42.4 38.5 37.7 38.3 110.1 2.0 4 83736 2.0 1.0 1.0 2 2 42 4 1.0 4.0 ... 114.0 54.0 55.2 164.9 20.3 37.4 36.0 27.2 80.4 2.0 5 rows \u00d7 28 columns females = da [ da [ \"RIAGENDR\" ] == 2 ] male = da [ da [ \"RIAGENDR\" ] == 1 ] n_m = len ( females ) mu_m = females [ \"BMXBMI\" ] . mean () sd_m = females [ \"BMXBMI\" ] . std () ( n_m , mu_m , sd_m ) (2976, 29.939945652173996, 7.75331880954568) n_h = len ( male ) mu_h = male [ \"BMXBMI\" ] . mean () sd_h = male [ \"BMXBMI\" ] . std () ( n_h , mu_h , sd_h ) (2759, 28.778072111846985, 6.252567616801485) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax [ 0 ] . set_title ( \"Histogram BMI para mulheres\" , fontsize = 16 ) ax [ 0 ] . hist ( females [ \"BMXBMI\" ] . dropna (), edgecolor = 'k' , color = 'darkred' , bins = 25 ) ax [ 1 ] . set_title ( \"Histogram BMI para homens\" , fontsize = 16 ) ax [ 1 ] . hist ( male [ \"BMXBMI\" ] . dropna (), edgecolor = 'k' , color = 'green' , bins = 25 ) plt . show ()","title":"Diferen\u00e7a entre m\u00e9dias de popula\u00e7\u00f5es"},{"location":"infestatistica/ExampleHypothesesTesting/ExampleHypothesesTesting/#vamos-usar-o-t-test-descrito-em-welch","text":"Temos que us\u00e1-lo porque n\u00e3o conhecemos a vari\u00e2ncia. Se conhecessemos, poder\u00edamos usar o teste normal mesmo e se soub\u00e9ssemos que s\u00e3o iguais, mas desconhecessemos, poder\u00edamos usar o ttest ques estudamos no cap\u00edtulo 9.5 . sm . stats . ttest_ind ( x1 = females [ \"BMXBMI\" ] . dropna (), x2 = male [ \"BMXBMI\" ] . dropna (), alternative = 'two-sided' , value = 0 ) # diferen\u00e7a na hip\u00f3tese nula (6.175593353138302, 7.050275578095374e-10, 5660.0)","title":"Vamos usar o t-test descrito em Welch."},{"location":"infestatistica/ExampleHypothesesTesting/ExampleHypothesesTesting/#conclusao-no-teste-de-hipoteses","text":"Como o p-valor \u00e9 bem pequeno, n\u00f3s podemos rejeitar a hip\u00f3tese nula, o que significa que existe diferen\u00e7a estat\u00edstica entre as m\u00e9dias. Isso n\u00e3o responde se a o \u00edndice \u00e9 mais alto para homens, mas podemos fazer o teste unilateral e perceber que de fato isso de fato acontece segundo os dados. sm . stats . ttest_ind ( x1 = females [ \"BMXBMI\" ] . dropna (), x2 = male [ \"BMXBMI\" ] . dropna (), alternative = 'larger' , value = 0 ) # diferen\u00e7a na hip\u00f3tese nula (6.175593353138302, 3.525137789047687e-10, 5660.0)","title":"Conclus\u00e3o no teste de hip\u00f3teses"},{"location":"infestatistica/FisherInformation/FisherInformation/","text":"Informa\u00e7\u00e3o de Fisher Seja X uma amostra aleat\u00f3ria cuja distribui\u00e7\u00e3o depende de \\theta e tem valores em (a,b) \\subset \\mathbb{R} . Seja f_n(x|\\theta) a pdf conjunta de X . Assuma que S = {x | f(x|\\theta) > 0} \u00e9 o mesmo para todo \\theta . E \\lambda_n(x|\\theta) = \\log f_n(x|\\theta) \u00e9 duas vezes diferenci\u00e1vel em \\theta . A informa\u00e7\u00e3o \u00e9: I_n(\\theta) = E_{\\theta}\\{[\\lambda_n '(X|\\theta)]^2\\} Agora assuma que duas derivadas de \\int_S f_n(x|\\theta)dx com respeito a \\theta podemos inverter a ordem de integra\u00e7\u00e3o e diferencia\u00e7\u00e3o . Ent\u00e3o: I_n(\\theta) = - E_{\\theta}[\\lambda_n ''(X|\\theta)] Teorema I_n(\\theta) = nI(\\theta) Obs.: Estamos tratando da informa\u00e7\u00e3o de Fisher para o caso unidimensional. Para o caso em que temos \\Omega \\subset \\mathbb{R}^k , a informa\u00e7\u00e3o de Fisher ser\u00e1 uma matriz de tamanho k \\times k onde I_{n,i,j} = Cov_{\\theta}\\left[\\frac{\\partial}{\\partial \\theta_i}\\lambda_n'(X|\\theta), \\frac{\\partial}{\\partial \\theta_j}\\lambda_n'(X|\\theta)\\right] import numpy as np from scipy.stats import norm from scipy.misc import derivative from scipy.optimize import curve_fit import matplotlib.pyplot as plt from seaborn import violinplot import inspect Exemplo Construtivo Vamos pensar num caso bem simples: amostra aleat\u00f3ria X_1, ..., X_n \\sim \\text{Normal}(\\mu, \\sigma^2) , onde o par\u00e2metro \\sigma^2 \u00e9 conhecido e \\mu n\u00e3o. De forma direta, poder\u00edamos perguntar qual a Informa\u00e7\u00e3o de Fisher (ou Informa\u00e7\u00e3o Diferencial) da amostra aleat\u00f3ria sobre o par\u00e2metro desconhecido \\mu . Vamos encontrar a distribui\u00e7\u00e3o conjunta: f(x|\\mu) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{1}{2}\\frac{(x - \\mu)^2}{\\sigma^2}\\right] \\begin{split} f_n(x|\\mu) &= \\prod_{i=1}^n f(x_i|\\mu) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i - \\mu)^2\\right] \\\\ &= \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i^2 - 2x_i\\mu + \\mu^2)\\right] \\\\ &= \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\left(\\sum_{i=1}^n x_i^2 - 2n\\bar{x}_n\\mu + n\\mu^2\\right)\\right] \\\\ &= \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n x_i^2\\right]\\exp\\left[-\\frac{1}{2\\sigma^2}\\left(- 2n\\bar{x}_n\\mu + n\\mu^2\\right)\\right] \\end{split} Vamos encontrar a verossimilhan\u00e7a: \u00e9 a distribui\u00e7\u00e3o conjunta como fun\u00e7\u00e3o do par\u00e2metro! f_n(x|\\mu) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n x_i^2\\right]\\exp\\left[-\\frac{1}{2\\sigma^2}\\left(- 2n\\bar{x}_n\\mu + n\\mu^2\\right)\\right] Vamos comparar para \\sigma = 1 e \\sigma = 5 loglikelihood = lambda mu , sigma , x : np . sum ( np . log ([ norm ( loc = mu , scale = sigma ) . pdf ( xi ) for xi in x ]), axis = 0 ) sigmas = [ 1 , 3 , 5 , 10 ] mu_true = 5 mu_range = np . linspace ( 0 , 10 , 1000 ) fig , ax = plt . subplots ( 2 , 2 , figsize = ( 16 , 10 )) fig . suptitle ( 'Comparando Log-verossimilhan\u00e7as da Distribui\u00e7\u00e3o Normal' ) def generate_curves ( sigma , ax , n = 20 , n_times = 50 ): for i in range ( n_times ): x = np . random . normal ( loc = mu_true , scale = sigma , size = n ) logvalues = loglikelihood ( mu_range , sigma , x ) ax . plot ( mu_range , logvalues , color = 'blue' , alpha = 0.2 ) ax . vlines ( mu_true , ymin = ax . get_ylim ()[ 0 ], ymax = ax . get_ylim ()[ 1 ], linestyle = '--' ) ax . set_title ( r '$\\sigma =$ {} ' . format ( sigma )) ax . set_xlabel ( r '$\\mu$' ) generate_curves ( sigmas [ 0 ], ax [ 0 ][ 0 ]) generate_curves ( sigmas [ 1 ], ax [ 0 ][ 1 ]) generate_curves ( sigmas [ 2 ], ax [ 1 ][ 0 ]) generate_curves ( sigmas [ 3 ], ax [ 1 ][ 1 ]) Vamos ver como se comporta derivada. Esse \u00e9 o score: \\lambda '_n(y|\\mu) = \\frac{1}{\\sigma^2}\\left(n\\bar{x}_n - \\mu\\right) score = lambda mu , sigma , x : derivative ( loglikelihood , mu , dx = 1e-5 , args = ( sigma , x )) fig , ax = plt . subplots ( 2 , 2 , figsize = ( 16 , 10 )) fig . suptitle ( 'Comparando Scores da Distribui\u00e7\u00e3o Normal' ) def generate_curves ( sigma , ax , n = 20 , n_times = 50 ): for i in range ( n_times ): x = np . random . normal ( loc = mu_true , scale = sigma , size = n ) scorevalues = score ( mu_range , sigma , x ) ax . plot ( mu_range , scorevalues , color = 'blue' , alpha = 0.2 ) ax . vlines ( mu_true , ymin = ax . get_ylim ()[ 0 ], ymax = ax . get_ylim ()[ 1 ], linestyle = '--' ) ax . set_title ( r '$\\sigma =$ {} ' . format ( sigma )) ax . set_xlabel ( r '$\\mu$' ) ax . set_ylim (( - 10 , 10 )) generate_curves ( sigmas [ 0 ], ax [ 0 ][ 0 ]) generate_curves ( sigmas [ 1 ], ax [ 0 ][ 1 ]) generate_curves ( sigmas [ 2 ], ax [ 1 ][ 0 ]) generate_curves ( sigmas [ 3 ], ax [ 1 ][ 1 ]) fig , ax = plt . subplots ( 2 , 2 , figsize = ( 16 , 10 )) fig . suptitle ( 'Comparando Histogramas dos Scores para mu' ) def generate_histograms ( mu , sigma , ax , n = 15 , n_times = 100 ): scorevalues = [] for i in range ( n_times ): x = np . random . normal ( loc = mu_true , scale = sigma , size = n ) scorevalues . append ( score ( mu , sigma , x )) violinplot ( scorevalues , ax = ax ) ax . set_title ( r '$\\sigma =$ {} ' . format ( sigma )) ax . set_xlabel ( 'score' ) generate_histograms ( 5 , sigmas [ 0 ], ax [ 0 ][ 0 ]) generate_histograms ( 5 , sigmas [ 1 ], ax [ 0 ][ 1 ]) generate_histograms ( 5 , sigmas [ 2 ], ax [ 1 ][ 0 ]) generate_histograms ( 5 , sigmas [ 3 ], ax [ 1 ][ 1 ]) A informa\u00e7\u00e3o de Fisher \u00e9 a Vari\u00e2ncia da fun\u00e7\u00e3o score em X , isto \u00e9: \\begin{split} I_n(\\mu) &= Var(\\lambda '_n(x|p)) = E[(\\lambda '_n(x|p))^2] - E[\\lambda '_n(x|p)]^2\\\\ &= \\frac{1}{\\sigma^4}Var\\left[n\\bar{x}_n - \\mu\\right] \\\\ &= \\frac{n^2}{\\sigma^4}Var(\\bar{x}_n) \\\\ &= \\frac{n^2\\sigma^2}{n\\sigma^4} \\\\ &= \\frac{n}{\\sigma^2} \\end{split} Desigualdade de Cram\u00e9r-Rao Seja X uma amostra aleat\u00f3ria com pdf f(x| \\theta) . Suponha as hip\u00f3teses acima acerca dessa distribui\u00e7\u00e3o. Seja T = r(X) com vari\u00e2ncia finita e m(\\theta) = E_{\\theta}(T) \u00e9 diferenci\u00e1vel. Assim: Var_{\\theta}(T) \\geq \\frac{[m'(\\theta)]^2}{nI(\\theta)} A igualdade vale se, e somente se, existem fun\u00e7\u00f5es u(\\theta) e v(\\theta) que podem depender em \\theta mas n\u00e3o de X tal que: T = u(\\theta)\\lambda_n'(X|\\theta) + v(\\theta) Se T for n\u00e3o enviesado m(\\theta) = \\theta \\implies m'(\\theta) = 1 Exemplo Num\u00e9rico do limite de Cram\u00e9r-Rao Refer\u00eancia Considere um sinal (como uma m\u00fasica) com tr\u00eas par\u00e2metros, amplitude, frequ\u00eancia e fase inicia. Saberemos o n\u00famero de amostras que sera 100Hz com n\u00edvel de ru\u00eddo de 0.1 s = lambda t , a , f , ph : a * np . sin ( 2 * np . pi * f * t + ph ) # fun\u00e7\u00e3o que representa o sinal p0 = [ 2 , 8 , 0 ] # Amplitude, frequ\u00eancia e fase inicial para testar noise = 0.1 T = np . linspace ( 0 , 1 , 100 ) #100 valores entre 0 e 1 igualmente espa\u00e7ados plt . plot ( T , s ( T , * p0 ), '.-k' ) plt . xlabel ( 'Tempo (s)' ) plt . title ( 'Sinal' ) plt . show () Vamos usar inspect para nos ajudar a pegar labels das fun\u00e7\u00f5es, isto \u00e9, os par\u00e2metros necess\u00e1rios das fun\u00e7\u00f5es. Essa biblioteca fornece v\u00e1rias fun\u00e7\u00f5es de ajuda desse tipo. D\u00ea uma olhada. parameters = str ( inspect . signature ( s )) . strip ( '()' ) . replace ( ' ' , '' ) . split ( ',' )[ 1 :] p0dict = dict ( zip ( parameters , p0 )) p0dict {'a': 2, 'f': 8, 'ph': 0} No caso geral, calcular a Matriz de Informa\u00e7\u00e3o de Fisher n\u00e3o \u00e9 trivial. Por isso, vamos calcular para o caso em que as medi\u00e7\u00f5es s\u00e3o de uma amostra com distribui\u00e7\u00e3o multivariada normal, isto \u00e9, \u00e9 uma distribui\u00e7\u00e3o normal, s\u00f3 que em mais dimens\u00f5es, em particular, 441 dimens\u00f5es (n\u00famero de pontos no tempo) Se calcularmos a informa\u00e7\u00e3o de Fisher, podemos ver que: \\mathcal{I}_{mn} = \\frac{1}{\\sigma^2} \\frac{\\partial \\mu^\\mathrm{T}}{\\partial \\theta_m} \\frac{\\partial \\mu}{\\partial \\theta_n} = \\frac{1}{\\sigma^2} \\sum_k \\frac{\\partial \\mu_k}{\\partial \\theta_m} \\frac{\\partial \\mu_k}{\\partial \\theta_n} onde \\theta = [a,f,ph]^T , \\mu = \\mu(\\theta) \u00e9 o vetor m\u00e9dia da normal multivariada e \\sigma^2 \u00e9 a vari\u00e2ncia de cada marginal da normal. N\u00e3o se assuste. Na multivariada, temos uma matriz para indicar as vari\u00e2ncias (ela se chama Matriz de Covari\u00e2ncias, na verdade). O que estou dizendo \u00e9 que ela \u00e9 \\sigma^2 vezes a identidade. \u00c9 bom conhecer essa distribui\u00e7\u00e3o! Por enquando acredite em mim! Ou no Wikipedia . Vou chamar D_{ik} = \\frac{\\partial \\mu_k}{\\partial \\theta_i} # Usamos ** para desempacotar elementos de um dicion\u00e1rio. string = \"a: {a} f: {f} ph: {ph} \" . format ( ** p0dict ) print ( string ) a: 2 f: 8 ph: 0 D = np . zeros (( len ( p0 ), len ( T ))) # para cada par\u00e2metro for i , parameter in enumerate ( parameters ): # para cada ponto no tempo for k , t in enumerate ( T ): func = lambda x : s ( t , ** dict ( p0dict , ** { parameter : x })) # Calculamos a derivada com respeito a x, que nesse caso \u00e9 o valor do parametro D [ i , k ] = derivative ( func , p0dict [ parameter ], dx = 1e-4 ) Veja que o tamanho de D \u00e9 o seguinte: D . shape (3, 100) plt . plot ( T , s ( T , * p0 ), '--k' , lw = 2 , label = 'Sinal' ) for Di , parameter in zip ( D , parameters ): # Estamos acessando Di = linha_i(D) plt . plot ( T , Di , '.-' , label = parameter ) plt . legend () plt . xlabel ( 'Tempo (s)' ) plt . show () O que D_{ik} indica? \u00c9 a derivada da k-\u00e9sima m\u00e9dia com respeito ao i-\u00e9simo par\u00e2metro. Logo indica o quanto o quando a amostra k afeta o par\u00e2metro i . Veja que quando temos picos no seno, teremos pico na amplitude,. Tamb\u00e9m vemos que a fase inicial n\u00e3o tem essa relev\u00e2ncia. Vemos tamb\u00e9m que o sinal se torna mais e mais sens\u00edvel \u00e0 frequ\u00eancia. Assim, podemos calular a informa\u00e7\u00e3o de fisher, usando einsum I = 1 / noise ** 2 * np . einsum ( 'mk,nk' , D , D ) print ( I ) [[ 4.95000000e+03 -5.64643569e+02 -3.43706036e-09] [-5.64643569e+02 2.68635205e+05 6.34601694e+04] [-3.43706036e-09 6.34601694e+04 2.01999999e+04]] Podemos calcular o limite de Cram\u00e9r-Rao para qualquer estimador n\u00e3o enviesado. Nesse caso, veja aqui para mais detalhes. Mas n\u00e3o se incomode com os detalhes, se preferir. iI = np . linalg . inv ( I ) print ( 'Cram\u00e9r-Rao Limite Inferior' ) for parameter , variance in zip ( parameters , iI . diagonal ()): print ( ' {} : {:.2g} ' . format ( parameter , np . sqrt ( variance ))) Cram\u00e9r-Rao Limite Inferior a: 0.014 f: 0.0038 ph: 0.014 Estimador Eficiente T \u00e9 um estimador eficiente de sua esperan\u00e7a m(\\theta) se, para todo \\theta , vale a igualdade em Cram\u00e9r-Rao. Mas nem sempre vale a igualdade, inclusive conhecemos uma consdi\u00e7\u00e3o necess\u00e1ria e suficiente para isso, que est\u00e1 logo acima. Estimadores n\u00e3o enviesados com vari\u00e2ncia m\u00ednima Suponha que T seja um estimador eficiente de sua esperan\u00e7a m(\\theta) e T_1 outro estimador n\u00e3o enviesado. Ent\u00e3o para todo valor \\theta \\in \\Omega , Var_{\\theta}(T) ser\u00e1 igual ao limite inferior de Cram\u00e9r-Rao e Var_{\\theta}(T_1) ser\u00e1 pelo menos maior ou igual. Portanto Var_{\\theta}(T) \\leq Var_{\\theta}(T_1), \\forall \\theta . Isto \u00e9, um estimado eficiente de m(\\theta) ter\u00e1 menor vari\u00e2ncia. Distribui\u00e7\u00e3o assint\u00f3tica de um estimador eficiente Assuma as hip\u00f3teses do teorema de Cram\u00e9r-Rao. Seja T um estimador eficiente para a sua m\u00e9dia m(\\theta) e m'(\\theta) \\neq 0 . Ent\u00e3o: \\frac{[nI(\\theta)]^{1/2}}{m'(\\theta)}[T - m(\\theta)] \\overset{d}{\\to} N(0,1) Distribui\u00e7\u00e3o assint\u00f3tica do MLE Suponha que obtemos \\hat{\\theta}_n resolvendo a equa\u00e7\u00e3o \\lambda_n'(x|\\theta) = 0 , isto \u00e9, maximizando a log-verossimilhan\u00e7a (MLE). E suponha que \\lambda_n'' e \\lambda_n''' existem e satisfazem certas condi\u00e7\u00f5es de regularidade. Ent\u00e3o [nI(\\theta)]^{1/2}(\\hat{\\theta}_n - \\theta) \\overset{d}{\\to} N(0,1) Como o MLE \u00e9 n\u00e3o enviesado, ent\u00e3o se ele for Eficiente, j\u00e1 sabemos que esse teorema \u00e9 verdade pelo anterior. (se ele \u00e9 n\u00e3o enviesado) Bayesiano Suponha que adotamos uma priori para \\theta com uma pdf diferenci\u00e1vel no intervalo. Sobre condi\u00e7\u00f5es de regularidade similares \u00e0quelas que garantem normalidade assint\u00f3tica para \\hat{\\theta}_n , pode-se mostrar que que a distribui\u00e7\u00e3o a posteriori de \\theta vai se aproximadamente uma normal com m\u00e9dia \\hat{\\theta}_n e vari\u00e2ncia 1/[nI(\\hat{\\theta}_n)] , onde \\hat{\\theta}_n \u00e9 o MLE.","title":"Informa\u00e7\u00e3o de Fisher"},{"location":"infestatistica/FisherInformation/FisherInformation/#informacao-de-fisher","text":"Seja X uma amostra aleat\u00f3ria cuja distribui\u00e7\u00e3o depende de \\theta e tem valores em (a,b) \\subset \\mathbb{R} . Seja f_n(x|\\theta) a pdf conjunta de X . Assuma que S = {x | f(x|\\theta) > 0} \u00e9 o mesmo para todo \\theta . E \\lambda_n(x|\\theta) = \\log f_n(x|\\theta) \u00e9 duas vezes diferenci\u00e1vel em \\theta . A informa\u00e7\u00e3o \u00e9: I_n(\\theta) = E_{\\theta}\\{[\\lambda_n '(X|\\theta)]^2\\} Agora assuma que duas derivadas de \\int_S f_n(x|\\theta)dx com respeito a \\theta podemos inverter a ordem de integra\u00e7\u00e3o e diferencia\u00e7\u00e3o . Ent\u00e3o: I_n(\\theta) = - E_{\\theta}[\\lambda_n ''(X|\\theta)]","title":"Informa\u00e7\u00e3o de Fisher"},{"location":"infestatistica/FisherInformation/FisherInformation/#teorema","text":"I_n(\\theta) = nI(\\theta) Obs.: Estamos tratando da informa\u00e7\u00e3o de Fisher para o caso unidimensional. Para o caso em que temos \\Omega \\subset \\mathbb{R}^k , a informa\u00e7\u00e3o de Fisher ser\u00e1 uma matriz de tamanho k \\times k onde I_{n,i,j} = Cov_{\\theta}\\left[\\frac{\\partial}{\\partial \\theta_i}\\lambda_n'(X|\\theta), \\frac{\\partial}{\\partial \\theta_j}\\lambda_n'(X|\\theta)\\right] import numpy as np from scipy.stats import norm from scipy.misc import derivative from scipy.optimize import curve_fit import matplotlib.pyplot as plt from seaborn import violinplot import inspect","title":"Teorema"},{"location":"infestatistica/FisherInformation/FisherInformation/#exemplo-construtivo","text":"Vamos pensar num caso bem simples: amostra aleat\u00f3ria X_1, ..., X_n \\sim \\text{Normal}(\\mu, \\sigma^2) , onde o par\u00e2metro \\sigma^2 \u00e9 conhecido e \\mu n\u00e3o. De forma direta, poder\u00edamos perguntar qual a Informa\u00e7\u00e3o de Fisher (ou Informa\u00e7\u00e3o Diferencial) da amostra aleat\u00f3ria sobre o par\u00e2metro desconhecido \\mu . Vamos encontrar a distribui\u00e7\u00e3o conjunta: f(x|\\mu) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{1}{2}\\frac{(x - \\mu)^2}{\\sigma^2}\\right] \\begin{split} f_n(x|\\mu) &= \\prod_{i=1}^n f(x_i|\\mu) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i - \\mu)^2\\right] \\\\ &= \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i^2 - 2x_i\\mu + \\mu^2)\\right] \\\\ &= \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\left(\\sum_{i=1}^n x_i^2 - 2n\\bar{x}_n\\mu + n\\mu^2\\right)\\right] \\\\ &= \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n x_i^2\\right]\\exp\\left[-\\frac{1}{2\\sigma^2}\\left(- 2n\\bar{x}_n\\mu + n\\mu^2\\right)\\right] \\end{split} Vamos encontrar a verossimilhan\u00e7a: \u00e9 a distribui\u00e7\u00e3o conjunta como fun\u00e7\u00e3o do par\u00e2metro! f_n(x|\\mu) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n x_i^2\\right]\\exp\\left[-\\frac{1}{2\\sigma^2}\\left(- 2n\\bar{x}_n\\mu + n\\mu^2\\right)\\right] Vamos comparar para \\sigma = 1 e \\sigma = 5 loglikelihood = lambda mu , sigma , x : np . sum ( np . log ([ norm ( loc = mu , scale = sigma ) . pdf ( xi ) for xi in x ]), axis = 0 ) sigmas = [ 1 , 3 , 5 , 10 ] mu_true = 5 mu_range = np . linspace ( 0 , 10 , 1000 ) fig , ax = plt . subplots ( 2 , 2 , figsize = ( 16 , 10 )) fig . suptitle ( 'Comparando Log-verossimilhan\u00e7as da Distribui\u00e7\u00e3o Normal' ) def generate_curves ( sigma , ax , n = 20 , n_times = 50 ): for i in range ( n_times ): x = np . random . normal ( loc = mu_true , scale = sigma , size = n ) logvalues = loglikelihood ( mu_range , sigma , x ) ax . plot ( mu_range , logvalues , color = 'blue' , alpha = 0.2 ) ax . vlines ( mu_true , ymin = ax . get_ylim ()[ 0 ], ymax = ax . get_ylim ()[ 1 ], linestyle = '--' ) ax . set_title ( r '$\\sigma =$ {} ' . format ( sigma )) ax . set_xlabel ( r '$\\mu$' ) generate_curves ( sigmas [ 0 ], ax [ 0 ][ 0 ]) generate_curves ( sigmas [ 1 ], ax [ 0 ][ 1 ]) generate_curves ( sigmas [ 2 ], ax [ 1 ][ 0 ]) generate_curves ( sigmas [ 3 ], ax [ 1 ][ 1 ]) Vamos ver como se comporta derivada. Esse \u00e9 o score: \\lambda '_n(y|\\mu) = \\frac{1}{\\sigma^2}\\left(n\\bar{x}_n - \\mu\\right) score = lambda mu , sigma , x : derivative ( loglikelihood , mu , dx = 1e-5 , args = ( sigma , x )) fig , ax = plt . subplots ( 2 , 2 , figsize = ( 16 , 10 )) fig . suptitle ( 'Comparando Scores da Distribui\u00e7\u00e3o Normal' ) def generate_curves ( sigma , ax , n = 20 , n_times = 50 ): for i in range ( n_times ): x = np . random . normal ( loc = mu_true , scale = sigma , size = n ) scorevalues = score ( mu_range , sigma , x ) ax . plot ( mu_range , scorevalues , color = 'blue' , alpha = 0.2 ) ax . vlines ( mu_true , ymin = ax . get_ylim ()[ 0 ], ymax = ax . get_ylim ()[ 1 ], linestyle = '--' ) ax . set_title ( r '$\\sigma =$ {} ' . format ( sigma )) ax . set_xlabel ( r '$\\mu$' ) ax . set_ylim (( - 10 , 10 )) generate_curves ( sigmas [ 0 ], ax [ 0 ][ 0 ]) generate_curves ( sigmas [ 1 ], ax [ 0 ][ 1 ]) generate_curves ( sigmas [ 2 ], ax [ 1 ][ 0 ]) generate_curves ( sigmas [ 3 ], ax [ 1 ][ 1 ]) fig , ax = plt . subplots ( 2 , 2 , figsize = ( 16 , 10 )) fig . suptitle ( 'Comparando Histogramas dos Scores para mu' ) def generate_histograms ( mu , sigma , ax , n = 15 , n_times = 100 ): scorevalues = [] for i in range ( n_times ): x = np . random . normal ( loc = mu_true , scale = sigma , size = n ) scorevalues . append ( score ( mu , sigma , x )) violinplot ( scorevalues , ax = ax ) ax . set_title ( r '$\\sigma =$ {} ' . format ( sigma )) ax . set_xlabel ( 'score' ) generate_histograms ( 5 , sigmas [ 0 ], ax [ 0 ][ 0 ]) generate_histograms ( 5 , sigmas [ 1 ], ax [ 0 ][ 1 ]) generate_histograms ( 5 , sigmas [ 2 ], ax [ 1 ][ 0 ]) generate_histograms ( 5 , sigmas [ 3 ], ax [ 1 ][ 1 ]) A informa\u00e7\u00e3o de Fisher \u00e9 a Vari\u00e2ncia da fun\u00e7\u00e3o score em X , isto \u00e9: \\begin{split} I_n(\\mu) &= Var(\\lambda '_n(x|p)) = E[(\\lambda '_n(x|p))^2] - E[\\lambda '_n(x|p)]^2\\\\ &= \\frac{1}{\\sigma^4}Var\\left[n\\bar{x}_n - \\mu\\right] \\\\ &= \\frac{n^2}{\\sigma^4}Var(\\bar{x}_n) \\\\ &= \\frac{n^2\\sigma^2}{n\\sigma^4} \\\\ &= \\frac{n}{\\sigma^2} \\end{split}","title":"Exemplo Construtivo"},{"location":"infestatistica/FisherInformation/FisherInformation/#desigualdade-de-cramer-rao","text":"Seja X uma amostra aleat\u00f3ria com pdf f(x| \\theta) . Suponha as hip\u00f3teses acima acerca dessa distribui\u00e7\u00e3o. Seja T = r(X) com vari\u00e2ncia finita e m(\\theta) = E_{\\theta}(T) \u00e9 diferenci\u00e1vel. Assim: Var_{\\theta}(T) \\geq \\frac{[m'(\\theta)]^2}{nI(\\theta)} A igualdade vale se, e somente se, existem fun\u00e7\u00f5es u(\\theta) e v(\\theta) que podem depender em \\theta mas n\u00e3o de X tal que: T = u(\\theta)\\lambda_n'(X|\\theta) + v(\\theta) Se T for n\u00e3o enviesado m(\\theta) = \\theta \\implies m'(\\theta) = 1","title":"Desigualdade de Cram\u00e9r-Rao"},{"location":"infestatistica/FisherInformation/FisherInformation/#exemplo-numerico-do-limite-de-cramer-rao","text":"Refer\u00eancia Considere um sinal (como uma m\u00fasica) com tr\u00eas par\u00e2metros, amplitude, frequ\u00eancia e fase inicia. Saberemos o n\u00famero de amostras que sera 100Hz com n\u00edvel de ru\u00eddo de 0.1 s = lambda t , a , f , ph : a * np . sin ( 2 * np . pi * f * t + ph ) # fun\u00e7\u00e3o que representa o sinal p0 = [ 2 , 8 , 0 ] # Amplitude, frequ\u00eancia e fase inicial para testar noise = 0.1 T = np . linspace ( 0 , 1 , 100 ) #100 valores entre 0 e 1 igualmente espa\u00e7ados plt . plot ( T , s ( T , * p0 ), '.-k' ) plt . xlabel ( 'Tempo (s)' ) plt . title ( 'Sinal' ) plt . show () Vamos usar inspect para nos ajudar a pegar labels das fun\u00e7\u00f5es, isto \u00e9, os par\u00e2metros necess\u00e1rios das fun\u00e7\u00f5es. Essa biblioteca fornece v\u00e1rias fun\u00e7\u00f5es de ajuda desse tipo. D\u00ea uma olhada. parameters = str ( inspect . signature ( s )) . strip ( '()' ) . replace ( ' ' , '' ) . split ( ',' )[ 1 :] p0dict = dict ( zip ( parameters , p0 )) p0dict {'a': 2, 'f': 8, 'ph': 0} No caso geral, calcular a Matriz de Informa\u00e7\u00e3o de Fisher n\u00e3o \u00e9 trivial. Por isso, vamos calcular para o caso em que as medi\u00e7\u00f5es s\u00e3o de uma amostra com distribui\u00e7\u00e3o multivariada normal, isto \u00e9, \u00e9 uma distribui\u00e7\u00e3o normal, s\u00f3 que em mais dimens\u00f5es, em particular, 441 dimens\u00f5es (n\u00famero de pontos no tempo) Se calcularmos a informa\u00e7\u00e3o de Fisher, podemos ver que: \\mathcal{I}_{mn} = \\frac{1}{\\sigma^2} \\frac{\\partial \\mu^\\mathrm{T}}{\\partial \\theta_m} \\frac{\\partial \\mu}{\\partial \\theta_n} = \\frac{1}{\\sigma^2} \\sum_k \\frac{\\partial \\mu_k}{\\partial \\theta_m} \\frac{\\partial \\mu_k}{\\partial \\theta_n} onde \\theta = [a,f,ph]^T , \\mu = \\mu(\\theta) \u00e9 o vetor m\u00e9dia da normal multivariada e \\sigma^2 \u00e9 a vari\u00e2ncia de cada marginal da normal. N\u00e3o se assuste. Na multivariada, temos uma matriz para indicar as vari\u00e2ncias (ela se chama Matriz de Covari\u00e2ncias, na verdade). O que estou dizendo \u00e9 que ela \u00e9 \\sigma^2 vezes a identidade. \u00c9 bom conhecer essa distribui\u00e7\u00e3o! Por enquando acredite em mim! Ou no Wikipedia . Vou chamar D_{ik} = \\frac{\\partial \\mu_k}{\\partial \\theta_i} # Usamos ** para desempacotar elementos de um dicion\u00e1rio. string = \"a: {a} f: {f} ph: {ph} \" . format ( ** p0dict ) print ( string ) a: 2 f: 8 ph: 0 D = np . zeros (( len ( p0 ), len ( T ))) # para cada par\u00e2metro for i , parameter in enumerate ( parameters ): # para cada ponto no tempo for k , t in enumerate ( T ): func = lambda x : s ( t , ** dict ( p0dict , ** { parameter : x })) # Calculamos a derivada com respeito a x, que nesse caso \u00e9 o valor do parametro D [ i , k ] = derivative ( func , p0dict [ parameter ], dx = 1e-4 ) Veja que o tamanho de D \u00e9 o seguinte: D . shape (3, 100) plt . plot ( T , s ( T , * p0 ), '--k' , lw = 2 , label = 'Sinal' ) for Di , parameter in zip ( D , parameters ): # Estamos acessando Di = linha_i(D) plt . plot ( T , Di , '.-' , label = parameter ) plt . legend () plt . xlabel ( 'Tempo (s)' ) plt . show () O que D_{ik} indica? \u00c9 a derivada da k-\u00e9sima m\u00e9dia com respeito ao i-\u00e9simo par\u00e2metro. Logo indica o quanto o quando a amostra k afeta o par\u00e2metro i . Veja que quando temos picos no seno, teremos pico na amplitude,. Tamb\u00e9m vemos que a fase inicial n\u00e3o tem essa relev\u00e2ncia. Vemos tamb\u00e9m que o sinal se torna mais e mais sens\u00edvel \u00e0 frequ\u00eancia. Assim, podemos calular a informa\u00e7\u00e3o de fisher, usando einsum I = 1 / noise ** 2 * np . einsum ( 'mk,nk' , D , D ) print ( I ) [[ 4.95000000e+03 -5.64643569e+02 -3.43706036e-09] [-5.64643569e+02 2.68635205e+05 6.34601694e+04] [-3.43706036e-09 6.34601694e+04 2.01999999e+04]] Podemos calcular o limite de Cram\u00e9r-Rao para qualquer estimador n\u00e3o enviesado. Nesse caso, veja aqui para mais detalhes. Mas n\u00e3o se incomode com os detalhes, se preferir. iI = np . linalg . inv ( I ) print ( 'Cram\u00e9r-Rao Limite Inferior' ) for parameter , variance in zip ( parameters , iI . diagonal ()): print ( ' {} : {:.2g} ' . format ( parameter , np . sqrt ( variance ))) Cram\u00e9r-Rao Limite Inferior a: 0.014 f: 0.0038 ph: 0.014","title":"Exemplo Num\u00e9rico do limite de Cram\u00e9r-Rao"},{"location":"infestatistica/FisherInformation/FisherInformation/#estimador-eficiente","text":"T \u00e9 um estimador eficiente de sua esperan\u00e7a m(\\theta) se, para todo \\theta , vale a igualdade em Cram\u00e9r-Rao. Mas nem sempre vale a igualdade, inclusive conhecemos uma consdi\u00e7\u00e3o necess\u00e1ria e suficiente para isso, que est\u00e1 logo acima.","title":"Estimador Eficiente"},{"location":"infestatistica/FisherInformation/FisherInformation/#estimadores-nao-enviesados-com-variancia-minima","text":"Suponha que T seja um estimador eficiente de sua esperan\u00e7a m(\\theta) e T_1 outro estimador n\u00e3o enviesado. Ent\u00e3o para todo valor \\theta \\in \\Omega , Var_{\\theta}(T) ser\u00e1 igual ao limite inferior de Cram\u00e9r-Rao e Var_{\\theta}(T_1) ser\u00e1 pelo menos maior ou igual. Portanto Var_{\\theta}(T) \\leq Var_{\\theta}(T_1), \\forall \\theta . Isto \u00e9, um estimado eficiente de m(\\theta) ter\u00e1 menor vari\u00e2ncia.","title":"Estimadores n\u00e3o enviesados com vari\u00e2ncia m\u00ednima"},{"location":"infestatistica/FisherInformation/FisherInformation/#distribuicao-assintotica-de-um-estimador-eficiente","text":"Assuma as hip\u00f3teses do teorema de Cram\u00e9r-Rao. Seja T um estimador eficiente para a sua m\u00e9dia m(\\theta) e m'(\\theta) \\neq 0 . Ent\u00e3o: \\frac{[nI(\\theta)]^{1/2}}{m'(\\theta)}[T - m(\\theta)] \\overset{d}{\\to} N(0,1)","title":"Distribui\u00e7\u00e3o assint\u00f3tica de um estimador eficiente"},{"location":"infestatistica/FisherInformation/FisherInformation/#distribuicao-assintotica-do-mle","text":"Suponha que obtemos \\hat{\\theta}_n resolvendo a equa\u00e7\u00e3o \\lambda_n'(x|\\theta) = 0 , isto \u00e9, maximizando a log-verossimilhan\u00e7a (MLE). E suponha que \\lambda_n'' e \\lambda_n''' existem e satisfazem certas condi\u00e7\u00f5es de regularidade. Ent\u00e3o [nI(\\theta)]^{1/2}(\\hat{\\theta}_n - \\theta) \\overset{d}{\\to} N(0,1) Como o MLE \u00e9 n\u00e3o enviesado, ent\u00e3o se ele for Eficiente, j\u00e1 sabemos que esse teorema \u00e9 verdade pelo anterior. (se ele \u00e9 n\u00e3o enviesado)","title":"Distribui\u00e7\u00e3o assint\u00f3tica do MLE"},{"location":"infestatistica/FisherInformation/FisherInformation/#bayesiano","text":"Suponha que adotamos uma priori para \\theta com uma pdf diferenci\u00e1vel no intervalo. Sobre condi\u00e7\u00f5es de regularidade similares \u00e0quelas que garantem normalidade assint\u00f3tica para \\hat{\\theta}_n , pode-se mostrar que que a distribui\u00e7\u00e3o a posteriori de \\theta vai se aproximadamente uma normal com m\u00e9dia \\hat{\\theta}_n e vari\u00e2ncia 1/[nI(\\hat{\\theta}_n)] , onde \\hat{\\theta}_n \u00e9 o MLE.","title":"Bayesiano"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/","text":"Grandes Amostras Desigualdade de Markov P(X \\geq t) \\leq \\frac{E[X^n]}{t^n}, ~dado~que~P(X \\geq 0) = 1, t > 0 Desigualdade de Chebyshev Seja X uma vari\u00e1vel aleat\u00f3ria em que o segundo momento \u00e9 finito. Ent\u00e3o, \\forall t > 0 . P(|X - E[X]| \\geq t) \\leq \\frac{Var[X]}{t^2} Propriedades Importantes X_1, ..., X_n amostra aleat\u00f3ria (por defini\u00e7\u00e3o mesma distribui\u00e7\u00e3o e independentes), com m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 . Ent\u00e3o E[\\bar{X_n}] = \\mu e Var[\\bar{X_n}] = \\sigma^2/n . Importando bibliotecas import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns . set () Exemplo 6.2.2 Um engenheiro ambiental acredita que existam dois contaminantes na \u00e1gua: ars\u00eanico e chumbo. Suponha que ambas s\u00e3o vari\u00e1veis aleat\u00f3rias independentes X e Y , medidas na mesma unidade. O engenheiro est\u00e1 interessado em saber a propor\u00e7\u00e3o de contamina\u00e7\u00e3o por chumbo em m\u00e9dia, isto \u00e9, E[R] = E[Y/(X + Y)] . Como nem sempre conhecemos a distribui\u00e7\u00e3o de R, podemos aproximar o valor esperado atrav\u00e9s de uma m\u00e9dia amostral de R , atrav\u00e9s de observa\u00e7\u00f5es (X_1, Y_1), ..., (X_n, Y_n) . Usando a desigualdade de Chebyshev, (tente ver que Var[R] \\leq 1 ). P(|\\bar{R_n} - E[R]| \\geq \\epsilon) \\leq \\frac{1}{n\\epsilon^2} # Usando apenas Chebyshev: epsilon = 0.0005 prob = 0.95 # probabilidade m\u00ednima de que a diferen\u00e7a entre a # m\u00e9dia amostral e o valor esperado seja menor do que epsilon def get_number_simulations ( epsilon , prob ): assert prob >= 0 assert prob <= 1 # Queremos que P <= 1 - prob-> 1/(n*eps**2) <= 1 - prob min_n = 1 / (( 1 - prob ) * ( epsilon ** 2 )) min_n = np . ceil ( min_n ) print ( '----------------------------------------------------------------------' ) print ( 'In order to have the sample mean at least {} close with probability {} : ' . format ( epsilon , prob )) print ( 'The minimum number of simulations are: {} ' . format ( int ( min_n ))) print ( '----------------------------------------------------------------------' ) return min_n def get_epsilon ( prob , n ): assert prob >= 0 assert prob <= 1 # Queremos encontrar epsilon para que 1/(n*eps**2) = 1 - prob eps = np . sqrt ( 1 / ( n * ( 1 - prob ))) return eps _ = get_number_simulations ( epsilon , prob ) ---------------------------------------------------------------------- In order to have the sample mean at least 0.0005 close with probability 0.95: The minimum number of simulations are: 80000000 ---------------------------------------------------------------------- # Testando com distribui\u00e7\u00e3o uniforme (X e Y tem distribui\u00e7\u00f5es uniformes) # Nesse caso, podemos provar que E[R] = 0.5 probs = [ 0.6 , 0.75 , 0.9 , 0.95 , 0.99 ] n_range = np . array ([ j * 10 ** i for i in [ 2 , 3 , 4 , 5 , 6 , 7 ] for j in [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ]]) E_R = [] for n in n_range : X = np . random . uniform ( 0 , 1 , size = int ( n )) #np = numpy Y = np . random . uniform ( 0 , 1 , size = int ( n )) R = Y / ( X + Y ) E_R . append ( np . mean ( R )) # E[R] = 0.5 chebyshev_interval = np . empty ( shape = ( len ( probs ), len ( n_range ))) for i , prob in enumerate ( probs ): chebyshev_interval [ i , :] = get_epsilon ( prob , n_range ) # Plotando fig , ax = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) for i in [ 0 , 1 ]: ax [ i ] . plot ( n_range , E_R , color = 'darkred' ) ax [ i ] . hlines ( 0.5 , xmin = min ( n_range ), xmax = max ( n_range ), linestyle = '--' , alpha = 0.4 , color = 'black' ) ax [ i ] . set_xscale ( 'log' ) colors = [ 'black' , 'red' , 'green' , 'blue' , 'pink' ] for i in range ( len ( probs )): ax [ 1 ] . fill_between ( x = n_range , y1 = 0.5 + chebyshev_interval [ i ,:], y2 = 0.5 - chebyshev_interval [ i ,:], color = colors [ i ], alpha = 0.3 + 0.5 * ( len ( probs ) - i ) / ( len ( probs )), label = probs [ i ]) ax [ 1 ] . legend () ax [ 0 ] . set_title ( 'Different mean samples' , fontsize = 15 ) ax [ 1 ] . set_title ( 'Chebyshev bound with prob' , fontsize = 15 ) plt . show () Lei dos Grandes N\u00fameros Converg\u00eancia em Probabilidade \\forall \\epsilon > 0, \\lim_{n\\to\\infty} P[|Z_n - b| < \\epsilon] = 1 \\iff Z_n \\overset{p}{\\to} b Converg\u00eancia quase certa (Implica a anterior) P[\\lim_{n_\\to\\infty} Z_n = b] = 1 Vers\u00e3o Fraca X_1, \\dots, X_n amostra aleat\u00f3ria de uma distribui\u00e7\u00e3o com m\u00e9dia \\mu e vari\u00e2ncia vinita. Se \\bar{X}_n \u00e9 a m\u00e9dia amostral. Ent\u00e3o \\bar{X}_n \\overset{p}{\\to} \\mu . Vers\u00e3o Forte P[\\lim_{n\\to\\infty} \\bar{X}_n = \\mu] = 1 Histogramas S\u00e3o usados para aproximar uma fun\u00e7\u00e3o de densidade de probabilidade de forma discreta. Seja X_1, X_2, \\dots vari\u00e1veis aleat\u00f3rias iid. Seja c_1 < c_2 constantes. Seja Y_i uma indicadora para c_1\\leq X_i < c_2 . Ent\u00e3o \\bar{Y}_n (propor\u00e7\u00e3o de valores X_1, ..., X_n no intervalo [c_1, c_2) e \\bar{Y}_n \\overset{p}{\\to} P[c_1 \\leq X_1 < c_2] . # Exemplo 6.2.4 lamda = 0.5 # N\u00e3o posso usar lambda beta = 1 / lamda # Numpy usa esse par\u00e2metro t = np . arange ( 0.0001 , 15 , 0.01 ) X_true = lamda * np . exp ( - lamda * t ) fig , ax = plt . subplots ( figsize = ( 14 , 5 )) for n in [ 1 , 10 , 100 , 1000 , 10000 ]: X = np . random . exponential ( scale = beta , size = n ) sns . distplot ( X , ax = ax , kde = False , norm_hist = True , label = 'n=' + str ( n )) # area = 1 sns . lineplot ( t , X_true , ax = ax , lw = 3 ) ax . set_title ( 'Histograma da Distribui\u00e7\u00e3o Exponencial' ) ax . legend () plt . show () Teorema Central do Limite Se as vari\u00e1veis aleat\u00f3rias X_1, ..., X_n formam uma amostra aleat\u00f3ria de tamanho n para uma dada distribui\u00e7\u00e3o de m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 finita, ent\u00e3o para cada n\u00famero x , lim_{n\\to\\infty} P[\\frac{\\bar{X}_n - \\mu}{\\sigma/n^{1/2}} \\leq x] = \\Phi(x), onde \\Phi \u00e9 a fun\u00e7\u00e3o de densidade acumulada da distribui\u00e7\u00e3o normal!!! coffee_df = pd . read_csv ( '../data/CoffeeAndCode.csv' ) display ( coffee_df . head ()) display ( coffee_df . shape ) display ( coffee_df . describe ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CodingHours CoffeeCupsPerDay CoffeeTime CodingWithoutCoffee CoffeeType CoffeeSolveBugs Gender Country AgeRange 0 8 2 Before coding Yes Caff\u00e8 latte Sometimes Female Lebanon 18 to 29 1 3 2 Before coding Yes Americano Yes Female Lebanon 30 to 39 2 5 3 While coding No Nescafe Yes Female Lebanon 18 to 29 3 8 2 Before coding No Nescafe Yes Male Lebanon NaN 4 10 3 While coding Sometimes Turkish No Male Lebanon 18 to 29 (100, 9) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CodingHours CoffeeCupsPerDay count 100.000000 100.000000 mean 6.410000 2.890000 std 2.644205 1.613673 min 1.000000 1.000000 25% 4.000000 2.000000 50% 7.000000 2.500000 75% 8.000000 4.000000 max 10.000000 8.000000 # Plotting fig , ax = plt . subplots ( figsize = ( 7 , 5 )) sns . distplot ( coffee_df . CoffeeCupsPerDay , ax = ax ) ax . vlines ( coffee_df . CoffeeCupsPerDay . mean (), ymin = 0 , ymax = 1 , linestyle = '--' , color = 'black' , label = 'black' ) ax . annotate ( 'M\u00e9dia:' + str ( coffee_df . CoffeeCupsPerDay . mean ()), ( coffee_df . CoffeeCupsPerDay . mean () + 0.5 , 0.4 )) ax . set_title ( 'Histrograma de copos de caf\u00e9 bebidos por programadores' ) ax . set_ylabel ( 'Frequ\u00eancia' ) ax . set_ylim (( 0 , 0.45 )) plt . show () # Generating sample means samples = [ 10 , 50 , 150 , 300 , 500 , 1000 ] n_experiments = 500 experiments_coffe_cups = np . empty (( n_experiments , len ( samples ))) for j , sample_size in enumerate ( samples ): sample = coffee_df . CoffeeCupsPerDay . sample ( n = sample_size * n_experiments , replace = True ) matrix = np . array ( sample ) . reshape (( n_experiments , sample_size )) experiments_coffe_cups [:, j ] = matrix . mean ( axis = 1 ) experiments_coffe_cups_df = pd . DataFrame ( experiments_coffe_cups , columns = samples ) fig , ax = plt . subplots ( 2 , 3 , figsize = ( 20 , 10 )) for index , column in enumerate ( experiments_coffe_cups_df . columns ): i = int ( index / 3 ) j = index % 3 sns . distplot ( experiments_coffe_cups_df [ column ], ax = ax [ i ][ j ]) ax [ i ][ j ] . set_title ( 'M\u00e9dia amostral com {} amostras' . format ( column )) ax [ i ][ j ] . set_ylabel ( 'Frequ\u00eancia' ) fig . suptitle ( 'Histogramas com diferentes n\u00fameros de amostras' ) plt . show () M\u00e9todo Delta Seja Y_1, Y_2, \\dots uma sequ\u00eancia de v.a. e F uma fun\u00e7\u00e3o de densidade acumulada cont\u00ednua. Sejam \\theta \\in \\mathbb{R} e \\{a_n\\}_{n\\in\\mathbb{N}} que tende ao \\infty . Suponha que a_n(Y_n - \\theta) converge para F . Seja \\alpha uma fun\u00e7\u00e3o com derivada cont\u00ednua, tal que \\alpha '(\\theta) \\neq 0 . Ent\u00e3o a_n[\\alpha(Y_n) - \\alpha(\\theta)]/\\alpha '(\\theta) converge para a distribui\u00e7\u00e3o F . Teorema de Slutsky \\begin{align*} {X}^{(n)}& \\overset{d}{\\to} X\\quad \\text{ e }\\quad ({X}^{(n)}-{Y}^{(n)})\\overset{p}{\\to} 0 \\quad \\text{implica} \\quad {Y}^{(n)}\\overset{d}{\\to} X,\\\\ {X}^{(n)}& \\overset{p}{\\to} X\\quad \\text{ e }\\quad ({X}^{(n)}-{Y}^{(n)})\\overset{p}{\\to} 0 \\quad \\text{implica} \\quad {Y}^{(n)}\\overset{p}{\\to} X,\\\\ {X}^{(n)}& \\overset{as}{\\to} X\\quad \\text{ e }\\quad ({X}^{(n)}-{Y}^{(n)})\\overset{as}{\\to} 0 \\quad \\text{implica} \\quad {Y}^{(n)}\\overset{as}{\\to} X. \\end{align*} Corol\u00e1rio Se f \u00e9 uma fun\u00e7\u00e3o cont\u00ednua: {X}^{(n)}\\overset{d}{\\to} X \\quad \\text{ e }\\quad {Y}^{(n)}\\overset{p}{\\to} c\\quad \\text{implica}\\quad f ({X}^{(n)},{Y}^{(n)}) \\overset{d}{\\to} f(X,c). Aproxima\u00e7\u00e3o de Taylor e M\u00e9todo Delta Refer\u00eancia de Probabilidade","title":"Grandes Amostras"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#grandes-amostras","text":"","title":"Grandes Amostras"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#desigualdade-de-markov","text":"P(X \\geq t) \\leq \\frac{E[X^n]}{t^n}, ~dado~que~P(X \\geq 0) = 1, t > 0","title":"Desigualdade de Markov"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#desigualdade-de-chebyshev","text":"Seja X uma vari\u00e1vel aleat\u00f3ria em que o segundo momento \u00e9 finito. Ent\u00e3o, \\forall t > 0 . P(|X - E[X]| \\geq t) \\leq \\frac{Var[X]}{t^2}","title":"Desigualdade de Chebyshev"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#propriedades-importantes","text":"X_1, ..., X_n amostra aleat\u00f3ria (por defini\u00e7\u00e3o mesma distribui\u00e7\u00e3o e independentes), com m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 . Ent\u00e3o E[\\bar{X_n}] = \\mu e Var[\\bar{X_n}] = \\sigma^2/n .","title":"Propriedades Importantes"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#importando-bibliotecas","text":"import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns . set ()","title":"Importando bibliotecas"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#exemplo-622","text":"Um engenheiro ambiental acredita que existam dois contaminantes na \u00e1gua: ars\u00eanico e chumbo. Suponha que ambas s\u00e3o vari\u00e1veis aleat\u00f3rias independentes X e Y , medidas na mesma unidade. O engenheiro est\u00e1 interessado em saber a propor\u00e7\u00e3o de contamina\u00e7\u00e3o por chumbo em m\u00e9dia, isto \u00e9, E[R] = E[Y/(X + Y)] . Como nem sempre conhecemos a distribui\u00e7\u00e3o de R, podemos aproximar o valor esperado atrav\u00e9s de uma m\u00e9dia amostral de R , atrav\u00e9s de observa\u00e7\u00f5es (X_1, Y_1), ..., (X_n, Y_n) . Usando a desigualdade de Chebyshev, (tente ver que Var[R] \\leq 1 ). P(|\\bar{R_n} - E[R]| \\geq \\epsilon) \\leq \\frac{1}{n\\epsilon^2} # Usando apenas Chebyshev: epsilon = 0.0005 prob = 0.95 # probabilidade m\u00ednima de que a diferen\u00e7a entre a # m\u00e9dia amostral e o valor esperado seja menor do que epsilon def get_number_simulations ( epsilon , prob ): assert prob >= 0 assert prob <= 1 # Queremos que P <= 1 - prob-> 1/(n*eps**2) <= 1 - prob min_n = 1 / (( 1 - prob ) * ( epsilon ** 2 )) min_n = np . ceil ( min_n ) print ( '----------------------------------------------------------------------' ) print ( 'In order to have the sample mean at least {} close with probability {} : ' . format ( epsilon , prob )) print ( 'The minimum number of simulations are: {} ' . format ( int ( min_n ))) print ( '----------------------------------------------------------------------' ) return min_n def get_epsilon ( prob , n ): assert prob >= 0 assert prob <= 1 # Queremos encontrar epsilon para que 1/(n*eps**2) = 1 - prob eps = np . sqrt ( 1 / ( n * ( 1 - prob ))) return eps _ = get_number_simulations ( epsilon , prob ) ---------------------------------------------------------------------- In order to have the sample mean at least 0.0005 close with probability 0.95: The minimum number of simulations are: 80000000 ---------------------------------------------------------------------- # Testando com distribui\u00e7\u00e3o uniforme (X e Y tem distribui\u00e7\u00f5es uniformes) # Nesse caso, podemos provar que E[R] = 0.5 probs = [ 0.6 , 0.75 , 0.9 , 0.95 , 0.99 ] n_range = np . array ([ j * 10 ** i for i in [ 2 , 3 , 4 , 5 , 6 , 7 ] for j in [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ]]) E_R = [] for n in n_range : X = np . random . uniform ( 0 , 1 , size = int ( n )) #np = numpy Y = np . random . uniform ( 0 , 1 , size = int ( n )) R = Y / ( X + Y ) E_R . append ( np . mean ( R )) # E[R] = 0.5 chebyshev_interval = np . empty ( shape = ( len ( probs ), len ( n_range ))) for i , prob in enumerate ( probs ): chebyshev_interval [ i , :] = get_epsilon ( prob , n_range ) # Plotando fig , ax = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) for i in [ 0 , 1 ]: ax [ i ] . plot ( n_range , E_R , color = 'darkred' ) ax [ i ] . hlines ( 0.5 , xmin = min ( n_range ), xmax = max ( n_range ), linestyle = '--' , alpha = 0.4 , color = 'black' ) ax [ i ] . set_xscale ( 'log' ) colors = [ 'black' , 'red' , 'green' , 'blue' , 'pink' ] for i in range ( len ( probs )): ax [ 1 ] . fill_between ( x = n_range , y1 = 0.5 + chebyshev_interval [ i ,:], y2 = 0.5 - chebyshev_interval [ i ,:], color = colors [ i ], alpha = 0.3 + 0.5 * ( len ( probs ) - i ) / ( len ( probs )), label = probs [ i ]) ax [ 1 ] . legend () ax [ 0 ] . set_title ( 'Different mean samples' , fontsize = 15 ) ax [ 1 ] . set_title ( 'Chebyshev bound with prob' , fontsize = 15 ) plt . show ()","title":"Exemplo 6.2.2"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#lei-dos-grandes-numeros","text":"","title":"Lei dos Grandes N\u00fameros"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#convergencia-em-probabilidade","text":"\\forall \\epsilon > 0, \\lim_{n\\to\\infty} P[|Z_n - b| < \\epsilon] = 1 \\iff Z_n \\overset{p}{\\to} b","title":"Converg\u00eancia em Probabilidade"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#convergencia-quase-certa-implica-a-anterior","text":"P[\\lim_{n_\\to\\infty} Z_n = b] = 1","title":"Converg\u00eancia quase certa (Implica a anterior)"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#versao-fraca","text":"X_1, \\dots, X_n amostra aleat\u00f3ria de uma distribui\u00e7\u00e3o com m\u00e9dia \\mu e vari\u00e2ncia vinita. Se \\bar{X}_n \u00e9 a m\u00e9dia amostral. Ent\u00e3o \\bar{X}_n \\overset{p}{\\to} \\mu .","title":"Vers\u00e3o Fraca"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#versao-forte","text":"P[\\lim_{n\\to\\infty} \\bar{X}_n = \\mu] = 1","title":"Vers\u00e3o Forte"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#histogramas","text":"S\u00e3o usados para aproximar uma fun\u00e7\u00e3o de densidade de probabilidade de forma discreta. Seja X_1, X_2, \\dots vari\u00e1veis aleat\u00f3rias iid. Seja c_1 < c_2 constantes. Seja Y_i uma indicadora para c_1\\leq X_i < c_2 . Ent\u00e3o \\bar{Y}_n (propor\u00e7\u00e3o de valores X_1, ..., X_n no intervalo [c_1, c_2) e \\bar{Y}_n \\overset{p}{\\to} P[c_1 \\leq X_1 < c_2] . # Exemplo 6.2.4 lamda = 0.5 # N\u00e3o posso usar lambda beta = 1 / lamda # Numpy usa esse par\u00e2metro t = np . arange ( 0.0001 , 15 , 0.01 ) X_true = lamda * np . exp ( - lamda * t ) fig , ax = plt . subplots ( figsize = ( 14 , 5 )) for n in [ 1 , 10 , 100 , 1000 , 10000 ]: X = np . random . exponential ( scale = beta , size = n ) sns . distplot ( X , ax = ax , kde = False , norm_hist = True , label = 'n=' + str ( n )) # area = 1 sns . lineplot ( t , X_true , ax = ax , lw = 3 ) ax . set_title ( 'Histograma da Distribui\u00e7\u00e3o Exponencial' ) ax . legend () plt . show ()","title":"Histogramas"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#teorema-central-do-limite","text":"Se as vari\u00e1veis aleat\u00f3rias X_1, ..., X_n formam uma amostra aleat\u00f3ria de tamanho n para uma dada distribui\u00e7\u00e3o de m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 finita, ent\u00e3o para cada n\u00famero x , lim_{n\\to\\infty} P[\\frac{\\bar{X}_n - \\mu}{\\sigma/n^{1/2}} \\leq x] = \\Phi(x), onde \\Phi \u00e9 a fun\u00e7\u00e3o de densidade acumulada da distribui\u00e7\u00e3o normal!!! coffee_df = pd . read_csv ( '../data/CoffeeAndCode.csv' ) display ( coffee_df . head ()) display ( coffee_df . shape ) display ( coffee_df . describe ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CodingHours CoffeeCupsPerDay CoffeeTime CodingWithoutCoffee CoffeeType CoffeeSolveBugs Gender Country AgeRange 0 8 2 Before coding Yes Caff\u00e8 latte Sometimes Female Lebanon 18 to 29 1 3 2 Before coding Yes Americano Yes Female Lebanon 30 to 39 2 5 3 While coding No Nescafe Yes Female Lebanon 18 to 29 3 8 2 Before coding No Nescafe Yes Male Lebanon NaN 4 10 3 While coding Sometimes Turkish No Male Lebanon 18 to 29 (100, 9) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CodingHours CoffeeCupsPerDay count 100.000000 100.000000 mean 6.410000 2.890000 std 2.644205 1.613673 min 1.000000 1.000000 25% 4.000000 2.000000 50% 7.000000 2.500000 75% 8.000000 4.000000 max 10.000000 8.000000 # Plotting fig , ax = plt . subplots ( figsize = ( 7 , 5 )) sns . distplot ( coffee_df . CoffeeCupsPerDay , ax = ax ) ax . vlines ( coffee_df . CoffeeCupsPerDay . mean (), ymin = 0 , ymax = 1 , linestyle = '--' , color = 'black' , label = 'black' ) ax . annotate ( 'M\u00e9dia:' + str ( coffee_df . CoffeeCupsPerDay . mean ()), ( coffee_df . CoffeeCupsPerDay . mean () + 0.5 , 0.4 )) ax . set_title ( 'Histrograma de copos de caf\u00e9 bebidos por programadores' ) ax . set_ylabel ( 'Frequ\u00eancia' ) ax . set_ylim (( 0 , 0.45 )) plt . show () # Generating sample means samples = [ 10 , 50 , 150 , 300 , 500 , 1000 ] n_experiments = 500 experiments_coffe_cups = np . empty (( n_experiments , len ( samples ))) for j , sample_size in enumerate ( samples ): sample = coffee_df . CoffeeCupsPerDay . sample ( n = sample_size * n_experiments , replace = True ) matrix = np . array ( sample ) . reshape (( n_experiments , sample_size )) experiments_coffe_cups [:, j ] = matrix . mean ( axis = 1 ) experiments_coffe_cups_df = pd . DataFrame ( experiments_coffe_cups , columns = samples ) fig , ax = plt . subplots ( 2 , 3 , figsize = ( 20 , 10 )) for index , column in enumerate ( experiments_coffe_cups_df . columns ): i = int ( index / 3 ) j = index % 3 sns . distplot ( experiments_coffe_cups_df [ column ], ax = ax [ i ][ j ]) ax [ i ][ j ] . set_title ( 'M\u00e9dia amostral com {} amostras' . format ( column )) ax [ i ][ j ] . set_ylabel ( 'Frequ\u00eancia' ) fig . suptitle ( 'Histogramas com diferentes n\u00fameros de amostras' ) plt . show ()","title":"Teorema Central do Limite"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#metodo-delta","text":"Seja Y_1, Y_2, \\dots uma sequ\u00eancia de v.a. e F uma fun\u00e7\u00e3o de densidade acumulada cont\u00ednua. Sejam \\theta \\in \\mathbb{R} e \\{a_n\\}_{n\\in\\mathbb{N}} que tende ao \\infty . Suponha que a_n(Y_n - \\theta) converge para F . Seja \\alpha uma fun\u00e7\u00e3o com derivada cont\u00ednua, tal que \\alpha '(\\theta) \\neq 0 . Ent\u00e3o a_n[\\alpha(Y_n) - \\alpha(\\theta)]/\\alpha '(\\theta) converge para a distribui\u00e7\u00e3o F .","title":"M\u00e9todo Delta"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#teorema-de-slutsky","text":"\\begin{align*} {X}^{(n)}& \\overset{d}{\\to} X\\quad \\text{ e }\\quad ({X}^{(n)}-{Y}^{(n)})\\overset{p}{\\to} 0 \\quad \\text{implica} \\quad {Y}^{(n)}\\overset{d}{\\to} X,\\\\ {X}^{(n)}& \\overset{p}{\\to} X\\quad \\text{ e }\\quad ({X}^{(n)}-{Y}^{(n)})\\overset{p}{\\to} 0 \\quad \\text{implica} \\quad {Y}^{(n)}\\overset{p}{\\to} X,\\\\ {X}^{(n)}& \\overset{as}{\\to} X\\quad \\text{ e }\\quad ({X}^{(n)}-{Y}^{(n)})\\overset{as}{\\to} 0 \\quad \\text{implica} \\quad {Y}^{(n)}\\overset{as}{\\to} X. \\end{align*}","title":"Teorema de Slutsky"},{"location":"infestatistica/LargeRandomSamples/LargeRandomSamples/#corolario","text":"Se f \u00e9 uma fun\u00e7\u00e3o cont\u00ednua: {X}^{(n)}\\overset{d}{\\to} X \\quad \\text{ e }\\quad {Y}^{(n)}\\overset{p}{\\to} c\\quad \\text{implica}\\quad f ({X}^{(n)},{Y}^{(n)}) \\overset{d}{\\to} f(X,c). Aproxima\u00e7\u00e3o de Taylor e M\u00e9todo Delta Refer\u00eancia de Probabilidade","title":"Corol\u00e1rio"},{"location":"infestatistica/LinearModel/LinearModel/","text":"Modelo Linear M\u00ednimos quadrados Sejam os pontos (x_1, y_1), ..., (x_n,y_n) . A reta que minimiza \\sum_{i=1}^n (y_i - (\\beta_0 + \\beta_1 x_i))^2 segundo \\beta_0 e \\beta_1 tem inclina\u00e7\u00e3o e intecepto \\hat{\\beta}_1 = \\dfrac{\\sum_{i=1}^n (y_i - \\bar{y})(x_i - \\bar{x})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x} A verifica\u00e7\u00e3o desse resultado \u00e9 direta ao derivar o funcional objetivo com respeito aos par\u00e2metros e igualando a 0. Al\u00e9m disso, observar que a Hessiana \u00e9 definida positiva, o que garante a exist\u00eancia do m\u00ednimo local (que nesse caso ser\u00e1 global). import numpy as np import matplotlib.pyplot as plt from scipy.stats import t from statsmodels.regression.linear_model import OLS plt . style . use ( 'ggplot' ) ro = np . random . RandomState ( 10000 ) Vamos gerar os n pontos e calcular a reta que minimiza os m\u00ednimos quadrados. n = 20 x = ro . uniform ( 0 , 10 , size = n ) epsilon = ro . normal ( 0 , scale = 3 , size = n ) beta0 , beta1 = ro . uniform ( 0 , 10 , size = 2 ) y = beta0 + beta1 * x + epsilon plt . scatter ( x , y ) plt . title ( 'Pontos gerados' ) plt . show () Estimando os coeficientes beta1_hat = np . dot ( y - y . mean (), x - x . mean ()) / np . dot ( x - x . mean (), x - x . mean ()) beta0_hat = y . mean () - beta1_hat * x . mean () t = np . linspace ( 0 , 10 , 1000 ) plt . scatter ( x , y , label = r '$\\beta_0$ = {:.2f} , $\\beta_1$ = {:.2f} ' . format ( beta0 , beta1 )) plt . plot ( t , beta0_hat + beta1_hat * t , color = 'blue' , label = r '$\\beta_0$ = {:.2f} , $\\beta_1$ = {:.2f} ' . format ( beta0_hat , beta1_hat )) plt . legend () plt . title ( 'Reta m\u00ednimos quadrados' ) plt . show () Agora vamos usar numpy para a estima\u00e7\u00e3o. Observe que o resultado coincide com o calculado usando a f\u00f3rmula derivada. # adiciona coluna de 1 para lidar com o beta_0 xlinha = np . c_ [ np . ones ( n ), x ] sol , _ , _ , _ = np . linalg . lstsq ( xlinha , y , rcond = None ) print ( sol ) [3.40285283 3.52323316] V\u00e1rias vari\u00e1veis Nesse caso, queremos encontrar um hiperplano que minimiza Q = \\sum_{i=1}^n (y_i - (\\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_k x_{ik}))^2 Nesse caso teremos k+1 equa\u00e7\u00f5es para resolver. Regress\u00e3o Dizemos que X_1, \\dots, X_k s\u00e3o preditores e Y \u00e9 a resposta. A esperan\u00e7a condicional da resposta dados os preditores \u00e9 chamada de fun\u00e7\u00e3o regress\u00e3o. Assim a regress\u00e3o de Y sobre X_1, \\dots, X_k depende dos valores x_1, \\dots, x_k assumidos pelos preditores. Vamos assumir que estamos com uma regress\u00e3o linear e, portanto, E[Y|x_1,\\dots,x_k] = \\beta_0 + \\beta_1 x_1 + \\dots \\beta_k x_k onde os coeficientes \\beta_j s\u00e3o coeficientes de regress\u00e3o, que s\u00e3o desconhecidos (par\u00e2metros do modelo). Regress\u00e3o Linear Simples Nesse caso Y = \\beta_0 + \\beta_1 x + \\varepsilon para cada X = x e, em geral, \\varepsilon \\sim N(0, \\sigma^2) . Assumimos que Os preditores s\u00e3o conhecidos; A distribui\u00e7\u00e3o de Y condicionada em x_1, ..., x_n \u00e9 normal. A m\u00e9dia condicional \u00e9 linear com par\u00e2metros \\beta_0 e \\beta_1 . Homoscedasticidade, isto \u00e9, vari\u00e2ncia constante. Independ\u00eancia das respostas dadas as covari\u00e1veis. Teorema: Os estimadores de m\u00e1xima verossimilhan\u00e7a de \\beta_0 e \\beta_1 s\u00e3o os de m\u00ednimos quadrados e de \\sigma^2 \u00e9 \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)^2 Distribui\u00e7\u00e3o dos estimadores: Considere os estimadores de m\u00ednimos quadrados como fun\u00e7\u00e3o das vari\u00e1veis aleat\u00f3rias Y_1,...,Y_n dados os preditores x_1,...,x_n . \\hat{\\beta}_1 \\sim \\mathcal{N}(\\beta_1, \\sigma^2/s_x^2) \\hat{\\beta}_0 \\sim \\mathcal{N}\\left(\\beta_0, \\sigma^2\\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{s_x^2}\\right)\\right) tal que Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) = \\frac{\\bar{x}\\sigma^2}{s_x^2} . Em particular a distribui\u00e7\u00e3o conjunta desses par\u00e2metros \u00e9 uma normal bivariada com as m\u00e9dias, vai\u00e2ncias e covari\u00e2ncia especificadas acima. As distribui\u00e7\u00f5es s\u00e3o todas condicionadas em X_i = x_i . Al\u00e9m disso, se n \\ge 3, \\hat{\\sigma}^2 \u00e9 independente de (\\hat{\\beta}_0, \\hat{\\beta}_1) e n\\hat{\\sigma}^2/\\sigma^2 tem distribui\u00e7\u00e3o \\chi^2 com n-2 graus de liberdade. Infer\u00eancia sobre regress\u00e3o linear simples Teste de hip\u00f3teses sobre os coeficientes Defina S^2 = \\sum_{i=1}^n (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2 \\sigma ' = \\left(\\dfrac{S^2}{n-2}\\right)^{1/2} Sejam c_0, c_1 e \\bar{c} n\u00fameros especificados onde c_i \\neq 0 para algum i e suponha que queiramos testar H_0: c_0 \\beta_0 + c_1 \\beta_1 = \\bar{c} H_1: c_0 \\beta_0 + c_1 \\beta_1 \\neq \\bar{c} Para cada \\alpha_0 \\in (0,1) um teste de hip\u00f3teses com n\u00edvel \\alpha_0 \u00e9 rejeitar H_0 se |U| \\ge T_{n-2}^{-1}(1 - \\alpha_0/2) , onde U = \\left[\\frac{c_0^2}{n} + \\frac{(c_0\\bar{x} - c_1)^2}{s_x^2}\\right]^{-1/2}\\left(\\frac{c_0\\hat{\\beta}_0 + c_1\\hat{\\beta}_1 - \\bar{c}}{\\sigma '}\\right) Para ver isso, basta olhar a vari\u00e1vel W que tem distribui\u00e7\u00e3o normal padr\u00e3o W = \\left[\\frac{c_0^2}{n} + \\frac{(c_0\\bar{x} - c_1)^2}{s_x^2}\\right]^{-1/2}\\left(\\frac{c_0\\hat{\\beta}_0 + c_1\\hat{\\beta}_1 - \\bar{c}}{\\sigma}\\right) e que S^2/\\sigma^2 \\sim \\chi^2_{n-2} A mesma deriva\u00e7\u00e3o pode ser usada para testes unilaterais. A diferen\u00e7a \u00e9 que rejeitaremos quando U \\ge T_{n-2}^{-1}(1 - \\alpha_0) quando a hip\u00f3tese nula \u00e9 uma desiguldade do tipo menor igual ou U \\le -T_{n-2}^{-1}(1 - \\alpha_0) quando \u00e9 do tipo maior ou igual. Exemplo: Vamos testar a hip\u00f3tese de que \\beta_1 = 0 em rela\u00e7\u00e3o a curva anterior. S\u00f3 para lembrar: t = np . linspace ( 0 , 10 , 1000 ) plt . scatter ( x , y , label = r '$\\beta_0$ = {:.2f} , $\\beta_1$ = {:.2f} ' . format ( beta0 , beta1 )) plt . plot ( t , beta0_hat + beta1_hat * t , color = 'blue' , label = r '$\\beta_0$ = {:.2f} , $\\beta_1$ = {:.2f} ' . format ( beta0_hat , beta1_hat )) plt . legend () plt . title ( 'Reta m\u00ednimos quadrados' ) plt . show () Nesse caso temos c_0 = \\bar{c} = 0 e c_1 = 1 . Assim: # O denominador da divis\u00e3o \u00e9 n - ddof sx2 = np . var ( x , ddof = n - 1 ) S2 = sum (( y - beta0_hat - beta1_hat * x ) ** 2 ) sigma_prime = ( S2 / ( n - 2 )) ** ( 1 / 2 ) # Estat\u00edstica de teste U = np . sqrt ( sx2 ) * beta1_hat / sigma_prime Seja \\alpha_0 = 0.05 o n\u00edvel de signific\u00e2ncia alpha0 = 0.05 abs ( U ) >= t . ppf ( 1 - alpha0 / 2 , df = n - 2 ) True Como isso \u00e9 verdade, rejeitamos a hip\u00f3tese nula! Intervalos de confian\u00e7a Temos que c_0\\hat{\\beta}_0 + c_1\\hat{\\beta}_1 \\pm \\sigma '\\left[\\frac{c_0^2}{n} + \\frac{(c_0\\bar{x} - c_1)^2}{s_x^2}\\right]^{1/2}T_{n-2}^{-1}\\left(1 - \\frac{\\alpha_0}{2}\\right) \u00e9 un intervalo de confian\u00e7a 1 - \\alpha_0 para c_0\\beta_0 + c_1\\beta_1 . Para encontrar esse intervalo basta encontrar o conjunto de todos os valores \\bar{c} para que a hip\u00f3tese nula seja rejeitada a n\u00edvel de signific\u00e2ncia \\alpha_0 . No nosso exemplo, um intervalo para \\beta_1 \u00e9 [ beta1_hat - sigma_prime * ( 1 / np . sqrt ( sx2 )) * t . ppf ( 1 - alpha0 / 2 , df = n - 2 ), beta1_hat + sigma_prime * ( 1 / np . sqrt ( sx2 )) * t . ppf ( 1 - alpha0 / 2 , df = n - 2 )] [3.00233584703191, 4.044130469815932] Usando o pacote statsmodels model = OLS ( y , xlinha ) result = model . fit () print ( result . params ) [3.40285283 3.52323316] Observe que os par\u00e2metros estimados s\u00e3o os mesmo que encontramos com numpy e na m\u00e3o. Tamb\u00e9m observe que o tvalor calculado coincide com nossa estat\u00edstica U print ( 'Nossa: {} ' . format ( U )) print ( 'Statsmodels: {} ' . format ( result . tvalues [ 1 ])) Nossa: 14.210167788464632 Statsmodels: 14.21016778846464 Podemos colocar c_0 = 0 e c_1 = 1 e obteremos o intervalo de confian\u00e7a como desej\u00e1vamos! result . t_test ([ 0 , 1 ]) <class 'statsmodels.stats.contrast.ContrastResults'> Test for Constraints ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ c0 3.5232 0.248 14.210 0.000 3.002 4.044 ==============================================================================","title":"Modelo Linear"},{"location":"infestatistica/LinearModel/LinearModel/#modelo-linear","text":"","title":"Modelo Linear"},{"location":"infestatistica/LinearModel/LinearModel/#minimos-quadrados","text":"Sejam os pontos (x_1, y_1), ..., (x_n,y_n) . A reta que minimiza \\sum_{i=1}^n (y_i - (\\beta_0 + \\beta_1 x_i))^2 segundo \\beta_0 e \\beta_1 tem inclina\u00e7\u00e3o e intecepto \\hat{\\beta}_1 = \\dfrac{\\sum_{i=1}^n (y_i - \\bar{y})(x_i - \\bar{x})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x} A verifica\u00e7\u00e3o desse resultado \u00e9 direta ao derivar o funcional objetivo com respeito aos par\u00e2metros e igualando a 0. Al\u00e9m disso, observar que a Hessiana \u00e9 definida positiva, o que garante a exist\u00eancia do m\u00ednimo local (que nesse caso ser\u00e1 global). import numpy as np import matplotlib.pyplot as plt from scipy.stats import t from statsmodels.regression.linear_model import OLS plt . style . use ( 'ggplot' ) ro = np . random . RandomState ( 10000 ) Vamos gerar os n pontos e calcular a reta que minimiza os m\u00ednimos quadrados. n = 20 x = ro . uniform ( 0 , 10 , size = n ) epsilon = ro . normal ( 0 , scale = 3 , size = n ) beta0 , beta1 = ro . uniform ( 0 , 10 , size = 2 ) y = beta0 + beta1 * x + epsilon plt . scatter ( x , y ) plt . title ( 'Pontos gerados' ) plt . show () Estimando os coeficientes beta1_hat = np . dot ( y - y . mean (), x - x . mean ()) / np . dot ( x - x . mean (), x - x . mean ()) beta0_hat = y . mean () - beta1_hat * x . mean () t = np . linspace ( 0 , 10 , 1000 ) plt . scatter ( x , y , label = r '$\\beta_0$ = {:.2f} , $\\beta_1$ = {:.2f} ' . format ( beta0 , beta1 )) plt . plot ( t , beta0_hat + beta1_hat * t , color = 'blue' , label = r '$\\beta_0$ = {:.2f} , $\\beta_1$ = {:.2f} ' . format ( beta0_hat , beta1_hat )) plt . legend () plt . title ( 'Reta m\u00ednimos quadrados' ) plt . show () Agora vamos usar numpy para a estima\u00e7\u00e3o. Observe que o resultado coincide com o calculado usando a f\u00f3rmula derivada. # adiciona coluna de 1 para lidar com o beta_0 xlinha = np . c_ [ np . ones ( n ), x ] sol , _ , _ , _ = np . linalg . lstsq ( xlinha , y , rcond = None ) print ( sol ) [3.40285283 3.52323316]","title":"M\u00ednimos quadrados"},{"location":"infestatistica/LinearModel/LinearModel/#varias-variaveis","text":"Nesse caso, queremos encontrar um hiperplano que minimiza Q = \\sum_{i=1}^n (y_i - (\\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_k x_{ik}))^2 Nesse caso teremos k+1 equa\u00e7\u00f5es para resolver.","title":"V\u00e1rias vari\u00e1veis"},{"location":"infestatistica/LinearModel/LinearModel/#regressao","text":"Dizemos que X_1, \\dots, X_k s\u00e3o preditores e Y \u00e9 a resposta. A esperan\u00e7a condicional da resposta dados os preditores \u00e9 chamada de fun\u00e7\u00e3o regress\u00e3o. Assim a regress\u00e3o de Y sobre X_1, \\dots, X_k depende dos valores x_1, \\dots, x_k assumidos pelos preditores. Vamos assumir que estamos com uma regress\u00e3o linear e, portanto, E[Y|x_1,\\dots,x_k] = \\beta_0 + \\beta_1 x_1 + \\dots \\beta_k x_k onde os coeficientes \\beta_j s\u00e3o coeficientes de regress\u00e3o, que s\u00e3o desconhecidos (par\u00e2metros do modelo).","title":"Regress\u00e3o"},{"location":"infestatistica/LinearModel/LinearModel/#regressao-linear-simples","text":"Nesse caso Y = \\beta_0 + \\beta_1 x + \\varepsilon para cada X = x e, em geral, \\varepsilon \\sim N(0, \\sigma^2) . Assumimos que Os preditores s\u00e3o conhecidos; A distribui\u00e7\u00e3o de Y condicionada em x_1, ..., x_n \u00e9 normal. A m\u00e9dia condicional \u00e9 linear com par\u00e2metros \\beta_0 e \\beta_1 . Homoscedasticidade, isto \u00e9, vari\u00e2ncia constante. Independ\u00eancia das respostas dadas as covari\u00e1veis. Teorema: Os estimadores de m\u00e1xima verossimilhan\u00e7a de \\beta_0 e \\beta_1 s\u00e3o os de m\u00ednimos quadrados e de \\sigma^2 \u00e9 \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)^2 Distribui\u00e7\u00e3o dos estimadores: Considere os estimadores de m\u00ednimos quadrados como fun\u00e7\u00e3o das vari\u00e1veis aleat\u00f3rias Y_1,...,Y_n dados os preditores x_1,...,x_n . \\hat{\\beta}_1 \\sim \\mathcal{N}(\\beta_1, \\sigma^2/s_x^2) \\hat{\\beta}_0 \\sim \\mathcal{N}\\left(\\beta_0, \\sigma^2\\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{s_x^2}\\right)\\right) tal que Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) = \\frac{\\bar{x}\\sigma^2}{s_x^2} . Em particular a distribui\u00e7\u00e3o conjunta desses par\u00e2metros \u00e9 uma normal bivariada com as m\u00e9dias, vai\u00e2ncias e covari\u00e2ncia especificadas acima. As distribui\u00e7\u00f5es s\u00e3o todas condicionadas em X_i = x_i . Al\u00e9m disso, se n \\ge 3, \\hat{\\sigma}^2 \u00e9 independente de (\\hat{\\beta}_0, \\hat{\\beta}_1) e n\\hat{\\sigma}^2/\\sigma^2 tem distribui\u00e7\u00e3o \\chi^2 com n-2 graus de liberdade.","title":"Regress\u00e3o Linear Simples"},{"location":"infestatistica/LinearModel/LinearModel/#inferencia-sobre-regressao-linear-simples","text":"","title":"Infer\u00eancia sobre regress\u00e3o linear simples"},{"location":"infestatistica/LinearModel/LinearModel/#teste-de-hipoteses-sobre-os-coeficientes","text":"Defina S^2 = \\sum_{i=1}^n (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2 \\sigma ' = \\left(\\dfrac{S^2}{n-2}\\right)^{1/2} Sejam c_0, c_1 e \\bar{c} n\u00fameros especificados onde c_i \\neq 0 para algum i e suponha que queiramos testar H_0: c_0 \\beta_0 + c_1 \\beta_1 = \\bar{c} H_1: c_0 \\beta_0 + c_1 \\beta_1 \\neq \\bar{c} Para cada \\alpha_0 \\in (0,1) um teste de hip\u00f3teses com n\u00edvel \\alpha_0 \u00e9 rejeitar H_0 se |U| \\ge T_{n-2}^{-1}(1 - \\alpha_0/2) , onde U = \\left[\\frac{c_0^2}{n} + \\frac{(c_0\\bar{x} - c_1)^2}{s_x^2}\\right]^{-1/2}\\left(\\frac{c_0\\hat{\\beta}_0 + c_1\\hat{\\beta}_1 - \\bar{c}}{\\sigma '}\\right) Para ver isso, basta olhar a vari\u00e1vel W que tem distribui\u00e7\u00e3o normal padr\u00e3o W = \\left[\\frac{c_0^2}{n} + \\frac{(c_0\\bar{x} - c_1)^2}{s_x^2}\\right]^{-1/2}\\left(\\frac{c_0\\hat{\\beta}_0 + c_1\\hat{\\beta}_1 - \\bar{c}}{\\sigma}\\right) e que S^2/\\sigma^2 \\sim \\chi^2_{n-2} A mesma deriva\u00e7\u00e3o pode ser usada para testes unilaterais. A diferen\u00e7a \u00e9 que rejeitaremos quando U \\ge T_{n-2}^{-1}(1 - \\alpha_0) quando a hip\u00f3tese nula \u00e9 uma desiguldade do tipo menor igual ou U \\le -T_{n-2}^{-1}(1 - \\alpha_0) quando \u00e9 do tipo maior ou igual. Exemplo: Vamos testar a hip\u00f3tese de que \\beta_1 = 0 em rela\u00e7\u00e3o a curva anterior. S\u00f3 para lembrar: t = np . linspace ( 0 , 10 , 1000 ) plt . scatter ( x , y , label = r '$\\beta_0$ = {:.2f} , $\\beta_1$ = {:.2f} ' . format ( beta0 , beta1 )) plt . plot ( t , beta0_hat + beta1_hat * t , color = 'blue' , label = r '$\\beta_0$ = {:.2f} , $\\beta_1$ = {:.2f} ' . format ( beta0_hat , beta1_hat )) plt . legend () plt . title ( 'Reta m\u00ednimos quadrados' ) plt . show () Nesse caso temos c_0 = \\bar{c} = 0 e c_1 = 1 . Assim: # O denominador da divis\u00e3o \u00e9 n - ddof sx2 = np . var ( x , ddof = n - 1 ) S2 = sum (( y - beta0_hat - beta1_hat * x ) ** 2 ) sigma_prime = ( S2 / ( n - 2 )) ** ( 1 / 2 ) # Estat\u00edstica de teste U = np . sqrt ( sx2 ) * beta1_hat / sigma_prime Seja \\alpha_0 = 0.05 o n\u00edvel de signific\u00e2ncia alpha0 = 0.05 abs ( U ) >= t . ppf ( 1 - alpha0 / 2 , df = n - 2 ) True Como isso \u00e9 verdade, rejeitamos a hip\u00f3tese nula!","title":"Teste de hip\u00f3teses sobre os coeficientes"},{"location":"infestatistica/LinearModel/LinearModel/#intervalos-de-confianca","text":"Temos que c_0\\hat{\\beta}_0 + c_1\\hat{\\beta}_1 \\pm \\sigma '\\left[\\frac{c_0^2}{n} + \\frac{(c_0\\bar{x} - c_1)^2}{s_x^2}\\right]^{1/2}T_{n-2}^{-1}\\left(1 - \\frac{\\alpha_0}{2}\\right) \u00e9 un intervalo de confian\u00e7a 1 - \\alpha_0 para c_0\\beta_0 + c_1\\beta_1 . Para encontrar esse intervalo basta encontrar o conjunto de todos os valores \\bar{c} para que a hip\u00f3tese nula seja rejeitada a n\u00edvel de signific\u00e2ncia \\alpha_0 . No nosso exemplo, um intervalo para \\beta_1 \u00e9 [ beta1_hat - sigma_prime * ( 1 / np . sqrt ( sx2 )) * t . ppf ( 1 - alpha0 / 2 , df = n - 2 ), beta1_hat + sigma_prime * ( 1 / np . sqrt ( sx2 )) * t . ppf ( 1 - alpha0 / 2 , df = n - 2 )] [3.00233584703191, 4.044130469815932] Usando o pacote statsmodels model = OLS ( y , xlinha ) result = model . fit () print ( result . params ) [3.40285283 3.52323316] Observe que os par\u00e2metros estimados s\u00e3o os mesmo que encontramos com numpy e na m\u00e3o. Tamb\u00e9m observe que o tvalor calculado coincide com nossa estat\u00edstica U print ( 'Nossa: {} ' . format ( U )) print ( 'Statsmodels: {} ' . format ( result . tvalues [ 1 ])) Nossa: 14.210167788464632 Statsmodels: 14.21016778846464 Podemos colocar c_0 = 0 e c_1 = 1 e obteremos o intervalo de confian\u00e7a como desej\u00e1vamos! result . t_test ([ 0 , 1 ]) <class 'statsmodels.stats.contrast.ContrastResults'> Test for Constraints ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ c0 3.5232 0.248 14.210 0.000 3.002 4.044 ==============================================================================","title":"Intervalos de confian\u00e7a"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/","text":"Estimador de M\u00e1xima Verossimilhan\u00e7a Introdu\u00e7\u00e3o \"Tradicionalmente a infer\u00eancia estat\u00edstica sobre a m\u00e9dia de uma popula\u00e7\u00e3o se apoia no Teorema Central do Limite para construir Intervalos de Confian\u00e7a ou testar hip\u00f3teses sobre o valor do par\u00e2metro. Esta abordagem da estat\u00edstica tradicional pode ser extendida para infer\u00eancias a respeito de qualquer par\u00e2metro, n\u00e3o s\u00f3 a m\u00e9dia. Da mesma forma que no caso da m\u00e9dia populacional se usa a distribui\u00e7\u00e3o t-Student ou a distribui\u00e7\u00e3o Normal Padr\u00e3o , no caso de outros par\u00e2metros se utiliza outras distribui\u00e7\u00f5es amostrais. Essas distribui\u00e7\u00f5es s\u00e3o chamadas amostrais porque representam o comportamento das estimativas baseado na repeti\u00e7\u00e3o incont\u00e1vel do processo de amostragem . Na pr\u00e1tica cient\u00edfica, no entanto, sempre se realiza uma \u00fanica amostragem , o que resulta em uma \u00fanica amostra. Assim, o conceito de distribui\u00e7\u00e3o amostral \u00e9 at\u00e9 certo ponto artificial, pois em pesquisa cient\u00edfica n\u00e3o raciocinamos em termos de repeti\u00e7\u00f5es incont\u00e1veis de experimentos ou processos de observa\u00e7\u00e3o . O resultado disto \u00e9 que o conceito de teste estat\u00edstico de hip\u00f3tese e de intervalo de confian\u00e7a s\u00e3o frequentemente mal compreendidos. O desenvolvimento da infer\u00eancia estat\u00edstica a partir do conceito de verossimilhan\u00e7a tem sido utilizado como uma alternativa \u00e0 abordagem estat\u00edstica frequentista e, segundo alguns autores (como por exemplo Royall, 1997), \u00e9 mais coerente com a pr\u00e1tica cient\u00edfica .\" (Batista, 2009) Site de Refer\u00eancia Fun\u00e7\u00e3o Verossimilhan\u00e7a Quando a fun\u00e7\u00e3o de densidade de probabilidade f_n(x|\\theta) das observa\u00e7\u00f5es de uma amostra aleat\u00f3ria \u00e9 vista como uma fun\u00e7\u00e3o de \\theta , chamamos ela de fun\u00e7\u00e3o de verossimilhan\u00e7a. \\theta \\mapsto f_n(x|\\theta) := L(\\theta|x) Estimador de M\u00e1xima Verossimilhan\u00e7a (MLE) Para cada observa\u00e7\u00e3o x , seja \\delta(x) um valor de \\theta \\in \\Omega tal que a fun\u00e7\u00e3o de verossimilhna\u00e7a seja m\u00e1xima . Defina \\hat{\\theta} = \\delta(X) o estimador. \u00c9 importante observar que o m\u00e1ximo dessa fun\u00e7\u00e3o pode n\u00e3o estar em um ponto de \\Omega . Nesse caso, MLE n\u00e3o existe. Ele pode n\u00e3o estar unicamente definido, tamb\u00e9m. Limita\u00e7\u00f5es N\u00e3o exist\u00eancia em todos os casos, isso depende muito da fun\u00e7\u00e3o e do espa\u00e7o dos par\u00e2metros. N\u00e3o unicidade em todos os casos. N\u00e3o podemos interpretar MLE como o par\u00e2metro mais prov\u00e1vel, pois ter\u00edamos que ter um espa\u00e7o de probabilidade associado ao par\u00e2metro, o que n\u00e3o \u00e9 dado. Implementa\u00e7\u00e3o Como refer\u00eancia, estou utilizando este site . # importando bibliotecas import numpy as np , pandas as pd from matplotlib import pyplot as plt import seaborn as sns from scipy.optimize import minimize import scipy.stats as stats import pymc3 as pm3 import numdifftools as ndt import statsmodels.api as sm from statsmodels.base.model import GenericLikelihoodModel % matplotlib inline # Gerando os dados N = 100 x = np . linspace ( 0 , 20 , N ) # gerando lista igualmente espa\u00e7ada beta1 = 3 beta0 = 0 sigma = 5 error = np . random . normal ( 0 , sigma , size = N ) y = beta1 * x + beta0 + error data = pd . DataFrame ({ 'y' : y , 'x' : x }) data [ 'constant' ] = 1 sns . regplot ( 'x' , 'y' , data = data ) # Essa reta \u00e9 uma estimativa dos dados feito por seaborn plt . title ( 'Dados' ) plt . show () Y = \\beta_1 x + \\beta_0 + e Nesse exemplo, o nosso problema ser\u00e1 estimar a m\u00e9dia. Observe que os dados tem um comportamento linear. Sem nos concentrarmos muito na modelagem e os problemas que ela pode trazer, eu vou j\u00e1 supor que temos um problema de Regress\u00e3o Linear , onde os dados Y \\sim N(\\mu, \\sigma^2) , onde \\sigma^2 \u00e9 a vari\u00e2ncia do erro no processo, e \\mu = \\beta_0 + \\beta_1 x , isto \u00e9, depende de x, nesse caso. Essa \u00e9 uma dificuldade, as contas ficam mais dif\u00edceis e, por isso, vamos usar asrtif\u00edcios computacionais. Vamos supor que a vari\u00e2ncia \u00e9 conhecida . Al\u00e9m disso, vamos supor que temos uma amostra aleat\u00f3ria Y_i \\sim N(\\beta_0 + \\beta_1 x_i, \\sigma^2) Temos que a verossimilhan\u00e7a \u00e9 produto das pdfs(distribui\u00e7\u00e3o de densidade de probabilidade). Para otimizar podemos, entretanto, obter a soma dos logaritmos das pdfs . E por fim, vamos resolver um problema de minimizar o negativo desse valor. Veja que \u00e9 equivalente a maximixar a soma!! # Fun\u00e7\u00e3o de verossimilhan\u00e7a. Chamamos de Fun\u00e7\u00e3o de Perda def MLE ( params ): # Fun\u00e7\u00e3o Perda: - log-verossimilhan\u00e7a beta0 , beta1 = params [ 0 ], params [ 1 ] # Modelo Linear yhat = beta0 + beta1 * x #= mu #loc \u00e9 a m\u00e9dia e scale desvio padr\u00e3o. Note que sigma \u00e9 conhecido negLikelihood = - np . sum ( stats . norm . logpdf ( y , loc = yhat , scale = sigma )) return negLikelihood # Esse \u00e9 o chute inicial initial_guess = np . array ([ 3 , 6 ]) results = minimize ( MLE , initial_guess , method = 'Nelder-Mead' , options = { 'disp' : True }) Optimization terminated successfully. Current function value: 307.745486 Iterations: 56 Function evaluations: 107 print ( results ) final_simplex: (array([[-1.03428809, 3.11012856], [-1.0342294 , 3.110121 ], [-1.03433677, 3.11012912]]), array([293.95399071, 293.95399071, 293.95399071])) fun: 293.95399070678394 message: 'Optimization terminated successfully.' nfev: 103 nit: 55 status: 0 success: True x: array([-1.03428809, 3.11012856]) resultsdf = pd . DataFrame ({ 'coef' : results [ 'x' ]}) resultsdf . index = [ r '$\\beta_0$' , r '$\\beta_1$' ] np . round ( resultsdf . head ( 2 ), 4 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } coef $\\beta_0$ -1.0343 $\\beta_1$ 3.1101 Vamos estimar usando a biblioteca OLS. Ela faz esse processo e muito mais internamente. results_ols = sm . OLS ( data . y , data [[ 'constant' , 'x' ]]) . fit () results_ols . summary () OLS Regression Results Dep. Variable: y R-squared: 0.941 Model: OLS Adj. R-squared: 0.941 Method: Least Squares F-statistic: 1568. Date: Wed, 26 Aug 2020 Prob (F-statistic): 4.22e-62 Time: 21:20:55 Log-Likelihood: -293.06 No. Observations: 100 AIC: 590.1 Df Residuals: 98 BIC: 595.3 Df Model: 1 Covariance Type: nonrobust coef std err t P>|t| [0.025 0.975] constant -1.0343 0.909 -1.138 0.258 -2.839 0.770 x 3.1101 0.079 39.599 0.000 2.954 3.266 Omnibus: 1.778 Durbin-Watson: 2.306 Prob(Omnibus): 0.411 Jarque-Bera (JB): 1.423 Skew: -0.289 Prob(JB): 0.491 Kurtosis: 3.084 Cond. No. 23.1 Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Veja que a estima\u00e7\u00e3o dos coeficientes foi a mesma! Apesar de ambas estarem erradas p para \\beta_0 . Na verdade se olharmos o intervalo de confian\u00e7a que OLS nos d\u00e1, vemos que de fato 0 est\u00e1 nele. Mas ainda n\u00e3o esta na hora de voc\u00eas verem isso! Conclus\u00e3o Podemos usar uma fun\u00e7\u00e3o de perda (que no caso ser\u00e1 menos a log-verossimilhan\u00e7a) e usar um algoritmo de otimiza\u00e7\u00e3o! Propriedades Invari\u00e2ncia Se \\hat{\\theta} \u00e9 o estimador de m\u00e1xima verossimilhan\u00e7a de \\theta e g \u00e9 uma fun\u00e7\u00e3o injetiva, ent\u00e3o g(\\hat{\\theta}) \u00e9 o estimador de m\u00e1xima verossimilhan\u00e7a de g(\\theta) . Na verdade, podemos retirar condi\u00e7\u00e3o de injetividade. MLE de uma Fun\u00e7\u00e3o Seja g(\\theta) uma fun\u00e7\u00e3o arbitr\u00e1ria do par\u00e2metro e G = g(\\Omega) . Para cada t \\in G , definimos G_t := \\{\\theta : g(\\theta) = t\\} e L^*(t) := \\max_{\\theta \\in G_t} log f_n(x|\\theta) Definimos a ML.E.de g(\\theta) := arg\\,max_{t\\in G} L^*(t) Teorema Seja \\hat{\\theta} MLE de \\theta e g(\\theta) fun\u00e7\u00e3o de \\theta . Ent\u00e3o uma MLE de g(\\theta) \u00e9 g(\\hat{\\theta}) . Consist\u00eancia Suponha que para uma amostra suficientemente grantde, existe um MLE \u00fanico para \\theta . Ent\u00e3o, sob algumas condi\u00e7\u00f5es, a sequ\u00eancia de MLE \u00e9 uma sequ\u00eancia consistente de estimadores de \\theta . A seuq\u00eancia convergee em probabilidade para o valor desconhecido de \\theta . O mesmo acontece com o Estimador de Bayes, dadas condi\u00e7\u00f5es de regularidade. Fun\u00e7\u00e3o Digamma: \\frac{\\Gamma'(\\alpha)}{\\Gamma(\\alpha)} M\u00e9todo dos Momentos Assuma que a amostra aleat\u00f3ria X_1,...,X_n vem da distribui\u00e7\u00e3o indexada pelo par\u00e2metro \\theta k-dimensional. Por exemplo, a distribui\u00e7\u00e3o normal tem k = 2 . Tamb\u00e9m suponha que pelo menos os k primeiros momentos ( E[X_i^k] < \\infty ) sejam finitos. Defina \\mu_j(\\theta) = E[X_1^j|\\theta], j = 1,...k . Suponha que a fun\u00e7\u00e3o: \\begin{split} \\mu : ~&\\Omega \\to \\mathbb{R}^k \\\\ &\\theta \\mapsto \\mu(\\theta) = (\\mu_1(\\theta), ..., \\mu_k(\\theta)), \\end{split} \u00e9 injetiva em \\theta . Seja M(\\mu_1,...,\\mu_k) a fun\u00e7\u00e3o inversa, isto \u00e9, \\theta = M(\\mu_1,...,\\mu_k) O m\u00e9todo dos momentos ser\u00e1 M(m_1,...,m_j) , onde m_j = \\frac{1}{n}\\sum_{i=1}^n X_i^j, j = 1,...,k De forma mais simplificada, basta que sesolvemos o sistema: m_j = \\mu_j(\\theta), isto \u00e9, os momentos amostrais iguais aos momentos da amostra, condicionados em \\theta . Teorema Suponha que \\{X_n\\}_{n\\in\\mathbb{N}} i.i.d com distribui\u00e7\u00e3o indexada pelo par\u00e2metro \\theta , k -dimensional. Suponha que os primeiros k momentos existem e s\u00e3o finitos para todo \\theta . Suponha que a inversa M definida acima \u00e9 cont\u00ednua. Ent\u00e3o a sequ\u00eancia de estimadores do m\u00e9todo de momentos em X_1,...,X_n \u00e9 consistente. M.L.E e Estimador de Bayes Se tivermos condi\u00e7\u00f5es de suavidade em f(x|\\theta) , podemos provar que quando n \\to \\infty , teremos que: L(\\theta|x) \\to c(x)\\cdot \\exp\\{-\\frac{1}{2V_n(\\theta)/n}(\\theta - \\hat{\\theta})^2\\}, onde \\hat{\\theta} \u00e9 MLE e V_n(\\theta) \u00e9 uma sequ\u00eancia de vari\u00e1veis aleat\u00f3rias convergente. No caso de termos uma priori relativamente flat, a posteriori ser\u00e1 aproximadamente uma distribui\u00e7\u00e3o normal com m\u00e9dia \\hat{\\theta} e vari\u00e2ncia V_n(\\hat{\\theta})/n . Exemplo 7.6.12 (Mortes ex\u00e9rcito pr\u00fassio) Bortkiewicz contou o n\u00famero de soldados mortos por horsekick em 14 unidades do ex\u00e9rcito em 20 anos, com 280 contagens ao total. Das contagens temos Valor 0 1 2 3 4 Total Contagem 144 91 32 11 2 280 Modelamos X_1, ..., X_{280} como uma vari\u00e1vel de contagem. Considere a distribui\u00e7\u00e3o Poisson(\\theta) . Escolhemos a distribui\u00e7\u00e3o Gamma(\\alpha,\\beta) , dada que ela pertence \u00e0 familia conjungada. Em particular, a distribui\u00e7\u00e3o a posteriori ser\u00e1 Gamma(\\alpha + \\sum X_i, \\beta + n) , onde \\sum X_i = 196 . Se assumirmos \\alpha inteiro por simplicidade, vemos que a distribui\u00e7\u00e3o Gamma pode ser vista como a soma de \\alpha + \\sum X_i distribui\u00e7\u00f5es Exponencial(\\beta + n) . Logo a soma dessas vari\u00e1veis ser\u00e1 aproximadamente normal com m\u00e9dia 196/280 e vari\u00e2ncia 196/280^2 . import numpy as np import matplotlib.pyplot as plt from scipy.stats import gamma alpha = 1 beta = 1 # Esse \u00e9 o MLE, a m\u00e9dia. Vou supor que esse \u00e9 o par\u00e2metro verdadeiro s\u00f3 para mostrar. theta = 196 / 280 sum_xi = 196 fig , ax = plt . subplots ( 2 , 3 , figsize = ( 18 , 6 )) fig . suptitle ( 'Avaliando a converg\u00eancia da distribui\u00e7\u00e3o Gamma' ) for index , n in enumerate ([ 1 , 10 , 100 , 1000 , 10000 , 280 ]): i = int ( index / 3 ) j = index % 3 X = np . random . poisson ( theta , size = n ) if n != 280 : T = X . sum () ax [ i ][ j ] . set_title ( 'n = {} ' . format ( n )) else : T = sum_xi #Valor dos dados ax [ i ][ j ] . set_title ( 'Dados Oficiais: n = {} ' . format ( n )) t = np . linspace ( start = 0.00001 , stop = 3 - i - 1 , num = 1000 ) posteriori = gamma ( alpha + T , scale = 1 / ( beta + n )) y = posteriori . pdf ( t ) ax [ i ][ j ] . plot ( t , y , color = 'darkblue' ) ax [ i ][ j ] . grid ( color = 'grey' , alpha = 0.6 , linestyle = '--' ) ax [ i ][ j ] . vlines ( theta , ymin = 0 , ymax = max ( y ), color = 'black' , linestyle = '--' ) Veja que com os dados reais, j\u00e1 temos uma boa aproxima\u00e7\u00e3o!","title":"Estimador de M\u00e1xima Verossimilhan\u00e7a"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#estimador-de-maxima-verossimilhanca","text":"","title":"Estimador de M\u00e1xima Verossimilhan\u00e7a"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#introducao","text":"\"Tradicionalmente a infer\u00eancia estat\u00edstica sobre a m\u00e9dia de uma popula\u00e7\u00e3o se apoia no Teorema Central do Limite para construir Intervalos de Confian\u00e7a ou testar hip\u00f3teses sobre o valor do par\u00e2metro. Esta abordagem da estat\u00edstica tradicional pode ser extendida para infer\u00eancias a respeito de qualquer par\u00e2metro, n\u00e3o s\u00f3 a m\u00e9dia. Da mesma forma que no caso da m\u00e9dia populacional se usa a distribui\u00e7\u00e3o t-Student ou a distribui\u00e7\u00e3o Normal Padr\u00e3o , no caso de outros par\u00e2metros se utiliza outras distribui\u00e7\u00f5es amostrais. Essas distribui\u00e7\u00f5es s\u00e3o chamadas amostrais porque representam o comportamento das estimativas baseado na repeti\u00e7\u00e3o incont\u00e1vel do processo de amostragem . Na pr\u00e1tica cient\u00edfica, no entanto, sempre se realiza uma \u00fanica amostragem , o que resulta em uma \u00fanica amostra. Assim, o conceito de distribui\u00e7\u00e3o amostral \u00e9 at\u00e9 certo ponto artificial, pois em pesquisa cient\u00edfica n\u00e3o raciocinamos em termos de repeti\u00e7\u00f5es incont\u00e1veis de experimentos ou processos de observa\u00e7\u00e3o . O resultado disto \u00e9 que o conceito de teste estat\u00edstico de hip\u00f3tese e de intervalo de confian\u00e7a s\u00e3o frequentemente mal compreendidos. O desenvolvimento da infer\u00eancia estat\u00edstica a partir do conceito de verossimilhan\u00e7a tem sido utilizado como uma alternativa \u00e0 abordagem estat\u00edstica frequentista e, segundo alguns autores (como por exemplo Royall, 1997), \u00e9 mais coerente com a pr\u00e1tica cient\u00edfica .\" (Batista, 2009) Site de Refer\u00eancia","title":"Introdu\u00e7\u00e3o"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#funcao-verossimilhanca","text":"Quando a fun\u00e7\u00e3o de densidade de probabilidade f_n(x|\\theta) das observa\u00e7\u00f5es de uma amostra aleat\u00f3ria \u00e9 vista como uma fun\u00e7\u00e3o de \\theta , chamamos ela de fun\u00e7\u00e3o de verossimilhan\u00e7a. \\theta \\mapsto f_n(x|\\theta) := L(\\theta|x)","title":"Fun\u00e7\u00e3o Verossimilhan\u00e7a"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#estimador-de-maxima-verossimilhanca-mle","text":"Para cada observa\u00e7\u00e3o x , seja \\delta(x) um valor de \\theta \\in \\Omega tal que a fun\u00e7\u00e3o de verossimilhna\u00e7a seja m\u00e1xima . Defina \\hat{\\theta} = \\delta(X) o estimador. \u00c9 importante observar que o m\u00e1ximo dessa fun\u00e7\u00e3o pode n\u00e3o estar em um ponto de \\Omega . Nesse caso, MLE n\u00e3o existe. Ele pode n\u00e3o estar unicamente definido, tamb\u00e9m.","title":"Estimador de M\u00e1xima Verossimilhan\u00e7a (MLE)"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#limitacoes","text":"N\u00e3o exist\u00eancia em todos os casos, isso depende muito da fun\u00e7\u00e3o e do espa\u00e7o dos par\u00e2metros. N\u00e3o unicidade em todos os casos. N\u00e3o podemos interpretar MLE como o par\u00e2metro mais prov\u00e1vel, pois ter\u00edamos que ter um espa\u00e7o de probabilidade associado ao par\u00e2metro, o que n\u00e3o \u00e9 dado.","title":"Limita\u00e7\u00f5es"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#implementacao","text":"Como refer\u00eancia, estou utilizando este site . # importando bibliotecas import numpy as np , pandas as pd from matplotlib import pyplot as plt import seaborn as sns from scipy.optimize import minimize import scipy.stats as stats import pymc3 as pm3 import numdifftools as ndt import statsmodels.api as sm from statsmodels.base.model import GenericLikelihoodModel % matplotlib inline # Gerando os dados N = 100 x = np . linspace ( 0 , 20 , N ) # gerando lista igualmente espa\u00e7ada beta1 = 3 beta0 = 0 sigma = 5 error = np . random . normal ( 0 , sigma , size = N ) y = beta1 * x + beta0 + error data = pd . DataFrame ({ 'y' : y , 'x' : x }) data [ 'constant' ] = 1 sns . regplot ( 'x' , 'y' , data = data ) # Essa reta \u00e9 uma estimativa dos dados feito por seaborn plt . title ( 'Dados' ) plt . show () Y = \\beta_1 x + \\beta_0 + e Nesse exemplo, o nosso problema ser\u00e1 estimar a m\u00e9dia. Observe que os dados tem um comportamento linear. Sem nos concentrarmos muito na modelagem e os problemas que ela pode trazer, eu vou j\u00e1 supor que temos um problema de Regress\u00e3o Linear , onde os dados Y \\sim N(\\mu, \\sigma^2) , onde \\sigma^2 \u00e9 a vari\u00e2ncia do erro no processo, e \\mu = \\beta_0 + \\beta_1 x , isto \u00e9, depende de x, nesse caso. Essa \u00e9 uma dificuldade, as contas ficam mais dif\u00edceis e, por isso, vamos usar asrtif\u00edcios computacionais. Vamos supor que a vari\u00e2ncia \u00e9 conhecida . Al\u00e9m disso, vamos supor que temos uma amostra aleat\u00f3ria Y_i \\sim N(\\beta_0 + \\beta_1 x_i, \\sigma^2) Temos que a verossimilhan\u00e7a \u00e9 produto das pdfs(distribui\u00e7\u00e3o de densidade de probabilidade). Para otimizar podemos, entretanto, obter a soma dos logaritmos das pdfs . E por fim, vamos resolver um problema de minimizar o negativo desse valor. Veja que \u00e9 equivalente a maximixar a soma!! # Fun\u00e7\u00e3o de verossimilhan\u00e7a. Chamamos de Fun\u00e7\u00e3o de Perda def MLE ( params ): # Fun\u00e7\u00e3o Perda: - log-verossimilhan\u00e7a beta0 , beta1 = params [ 0 ], params [ 1 ] # Modelo Linear yhat = beta0 + beta1 * x #= mu #loc \u00e9 a m\u00e9dia e scale desvio padr\u00e3o. Note que sigma \u00e9 conhecido negLikelihood = - np . sum ( stats . norm . logpdf ( y , loc = yhat , scale = sigma )) return negLikelihood # Esse \u00e9 o chute inicial initial_guess = np . array ([ 3 , 6 ]) results = minimize ( MLE , initial_guess , method = 'Nelder-Mead' , options = { 'disp' : True }) Optimization terminated successfully. Current function value: 307.745486 Iterations: 56 Function evaluations: 107 print ( results ) final_simplex: (array([[-1.03428809, 3.11012856], [-1.0342294 , 3.110121 ], [-1.03433677, 3.11012912]]), array([293.95399071, 293.95399071, 293.95399071])) fun: 293.95399070678394 message: 'Optimization terminated successfully.' nfev: 103 nit: 55 status: 0 success: True x: array([-1.03428809, 3.11012856]) resultsdf = pd . DataFrame ({ 'coef' : results [ 'x' ]}) resultsdf . index = [ r '$\\beta_0$' , r '$\\beta_1$' ] np . round ( resultsdf . head ( 2 ), 4 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } coef $\\beta_0$ -1.0343 $\\beta_1$ 3.1101 Vamos estimar usando a biblioteca OLS. Ela faz esse processo e muito mais internamente. results_ols = sm . OLS ( data . y , data [[ 'constant' , 'x' ]]) . fit () results_ols . summary () OLS Regression Results Dep. Variable: y R-squared: 0.941 Model: OLS Adj. R-squared: 0.941 Method: Least Squares F-statistic: 1568. Date: Wed, 26 Aug 2020 Prob (F-statistic): 4.22e-62 Time: 21:20:55 Log-Likelihood: -293.06 No. Observations: 100 AIC: 590.1 Df Residuals: 98 BIC: 595.3 Df Model: 1 Covariance Type: nonrobust coef std err t P>|t| [0.025 0.975] constant -1.0343 0.909 -1.138 0.258 -2.839 0.770 x 3.1101 0.079 39.599 0.000 2.954 3.266 Omnibus: 1.778 Durbin-Watson: 2.306 Prob(Omnibus): 0.411 Jarque-Bera (JB): 1.423 Skew: -0.289 Prob(JB): 0.491 Kurtosis: 3.084 Cond. No. 23.1 Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Veja que a estima\u00e7\u00e3o dos coeficientes foi a mesma! Apesar de ambas estarem erradas p para \\beta_0 . Na verdade se olharmos o intervalo de confian\u00e7a que OLS nos d\u00e1, vemos que de fato 0 est\u00e1 nele. Mas ainda n\u00e3o esta na hora de voc\u00eas verem isso!","title":"Implementa\u00e7\u00e3o"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#conclusao","text":"Podemos usar uma fun\u00e7\u00e3o de perda (que no caso ser\u00e1 menos a log-verossimilhan\u00e7a) e usar um algoritmo de otimiza\u00e7\u00e3o!","title":"Conclus\u00e3o"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#propriedades","text":"","title":"Propriedades"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#invariancia","text":"Se \\hat{\\theta} \u00e9 o estimador de m\u00e1xima verossimilhan\u00e7a de \\theta e g \u00e9 uma fun\u00e7\u00e3o injetiva, ent\u00e3o g(\\hat{\\theta}) \u00e9 o estimador de m\u00e1xima verossimilhan\u00e7a de g(\\theta) . Na verdade, podemos retirar condi\u00e7\u00e3o de injetividade.","title":"Invari\u00e2ncia"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#mle-de-uma-funcao","text":"Seja g(\\theta) uma fun\u00e7\u00e3o arbitr\u00e1ria do par\u00e2metro e G = g(\\Omega) . Para cada t \\in G , definimos G_t := \\{\\theta : g(\\theta) = t\\} e L^*(t) := \\max_{\\theta \\in G_t} log f_n(x|\\theta) Definimos a ML.E.de g(\\theta) := arg\\,max_{t\\in G} L^*(t)","title":"MLE de uma Fun\u00e7\u00e3o"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#teorema","text":"Seja \\hat{\\theta} MLE de \\theta e g(\\theta) fun\u00e7\u00e3o de \\theta . Ent\u00e3o uma MLE de g(\\theta) \u00e9 g(\\hat{\\theta}) .","title":"Teorema"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#consistencia","text":"Suponha que para uma amostra suficientemente grantde, existe um MLE \u00fanico para \\theta . Ent\u00e3o, sob algumas condi\u00e7\u00f5es, a sequ\u00eancia de MLE \u00e9 uma sequ\u00eancia consistente de estimadores de \\theta . A seuq\u00eancia convergee em probabilidade para o valor desconhecido de \\theta . O mesmo acontece com o Estimador de Bayes, dadas condi\u00e7\u00f5es de regularidade.","title":"Consist\u00eancia"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#funcao-digamma","text":"\\frac{\\Gamma'(\\alpha)}{\\Gamma(\\alpha)}","title":"Fun\u00e7\u00e3o Digamma:"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#metodo-dos-momentos","text":"Assuma que a amostra aleat\u00f3ria X_1,...,X_n vem da distribui\u00e7\u00e3o indexada pelo par\u00e2metro \\theta k-dimensional. Por exemplo, a distribui\u00e7\u00e3o normal tem k = 2 . Tamb\u00e9m suponha que pelo menos os k primeiros momentos ( E[X_i^k] < \\infty ) sejam finitos. Defina \\mu_j(\\theta) = E[X_1^j|\\theta], j = 1,...k . Suponha que a fun\u00e7\u00e3o: \\begin{split} \\mu : ~&\\Omega \\to \\mathbb{R}^k \\\\ &\\theta \\mapsto \\mu(\\theta) = (\\mu_1(\\theta), ..., \\mu_k(\\theta)), \\end{split} \u00e9 injetiva em \\theta . Seja M(\\mu_1,...,\\mu_k) a fun\u00e7\u00e3o inversa, isto \u00e9, \\theta = M(\\mu_1,...,\\mu_k) O m\u00e9todo dos momentos ser\u00e1 M(m_1,...,m_j) , onde m_j = \\frac{1}{n}\\sum_{i=1}^n X_i^j, j = 1,...,k De forma mais simplificada, basta que sesolvemos o sistema: m_j = \\mu_j(\\theta), isto \u00e9, os momentos amostrais iguais aos momentos da amostra, condicionados em \\theta .","title":"M\u00e9todo dos Momentos"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#teorema_1","text":"Suponha que \\{X_n\\}_{n\\in\\mathbb{N}} i.i.d com distribui\u00e7\u00e3o indexada pelo par\u00e2metro \\theta , k -dimensional. Suponha que os primeiros k momentos existem e s\u00e3o finitos para todo \\theta . Suponha que a inversa M definida acima \u00e9 cont\u00ednua. Ent\u00e3o a sequ\u00eancia de estimadores do m\u00e9todo de momentos em X_1,...,X_n \u00e9 consistente.","title":"Teorema"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#mle-e-estimador-de-bayes","text":"Se tivermos condi\u00e7\u00f5es de suavidade em f(x|\\theta) , podemos provar que quando n \\to \\infty , teremos que: L(\\theta|x) \\to c(x)\\cdot \\exp\\{-\\frac{1}{2V_n(\\theta)/n}(\\theta - \\hat{\\theta})^2\\}, onde \\hat{\\theta} \u00e9 MLE e V_n(\\theta) \u00e9 uma sequ\u00eancia de vari\u00e1veis aleat\u00f3rias convergente. No caso de termos uma priori relativamente flat, a posteriori ser\u00e1 aproximadamente uma distribui\u00e7\u00e3o normal com m\u00e9dia \\hat{\\theta} e vari\u00e2ncia V_n(\\hat{\\theta})/n .","title":"M.L.E e Estimador de Bayes"},{"location":"infestatistica/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#exemplo-7612-mortes-exercito-prussio","text":"Bortkiewicz contou o n\u00famero de soldados mortos por horsekick em 14 unidades do ex\u00e9rcito em 20 anos, com 280 contagens ao total. Das contagens temos Valor 0 1 2 3 4 Total Contagem 144 91 32 11 2 280 Modelamos X_1, ..., X_{280} como uma vari\u00e1vel de contagem. Considere a distribui\u00e7\u00e3o Poisson(\\theta) . Escolhemos a distribui\u00e7\u00e3o Gamma(\\alpha,\\beta) , dada que ela pertence \u00e0 familia conjungada. Em particular, a distribui\u00e7\u00e3o a posteriori ser\u00e1 Gamma(\\alpha + \\sum X_i, \\beta + n) , onde \\sum X_i = 196 . Se assumirmos \\alpha inteiro por simplicidade, vemos que a distribui\u00e7\u00e3o Gamma pode ser vista como a soma de \\alpha + \\sum X_i distribui\u00e7\u00f5es Exponencial(\\beta + n) . Logo a soma dessas vari\u00e1veis ser\u00e1 aproximadamente normal com m\u00e9dia 196/280 e vari\u00e2ncia 196/280^2 . import numpy as np import matplotlib.pyplot as plt from scipy.stats import gamma alpha = 1 beta = 1 # Esse \u00e9 o MLE, a m\u00e9dia. Vou supor que esse \u00e9 o par\u00e2metro verdadeiro s\u00f3 para mostrar. theta = 196 / 280 sum_xi = 196 fig , ax = plt . subplots ( 2 , 3 , figsize = ( 18 , 6 )) fig . suptitle ( 'Avaliando a converg\u00eancia da distribui\u00e7\u00e3o Gamma' ) for index , n in enumerate ([ 1 , 10 , 100 , 1000 , 10000 , 280 ]): i = int ( index / 3 ) j = index % 3 X = np . random . poisson ( theta , size = n ) if n != 280 : T = X . sum () ax [ i ][ j ] . set_title ( 'n = {} ' . format ( n )) else : T = sum_xi #Valor dos dados ax [ i ][ j ] . set_title ( 'Dados Oficiais: n = {} ' . format ( n )) t = np . linspace ( start = 0.00001 , stop = 3 - i - 1 , num = 1000 ) posteriori = gamma ( alpha + T , scale = 1 / ( beta + n )) y = posteriori . pdf ( t ) ax [ i ][ j ] . plot ( t , y , color = 'darkblue' ) ax [ i ][ j ] . grid ( color = 'grey' , alpha = 0.6 , linestyle = '--' ) ax [ i ][ j ] . vlines ( theta , ymin = 0 , ymax = max ( y ), color = 'black' , linestyle = '--' ) Veja que com os dados reais, j\u00e1 temos uma boa aproxima\u00e7\u00e3o!","title":"Exemplo 7.6.12 (Mortes ex\u00e9rcito pr\u00fassio)"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/","text":"Distribui\u00e7\u00f5es a Priori e a Posteriori Priori Tratamos \\theta de um modelo como uma vari\u00e1vel aleat\u00f3ria e atribuimos uma distribui\u00e7\u00e3o para esse par\u00e2metro. O nome ser\u00e1 distribui\u00e7\u00e3o a priori. Ao fazer modelagens, ela \u00e9 em geral pr\u00e9-definida pelo modelador, que \u00e9 em geral aconselhado por um especialista. Posteriori Sejam X_1, ..., X_n v.a. observadas e um par\u00e2metro \\theta desconhecido. A distribui\u00e7\u00e3o de \\theta condicionado nas vari\u00e1veis aleat\u00f3rias \u00e9 a distribui\u00e7\u00e3o a posteriori. Observe a rela\u00e7\u00e3o com o Teorema de Bayes. Teorema Suponha que X_1, ..., X_n formam uma amostra aleat\u00f3ria de uma distribui\u00e7\u00e3o f(x|\\theta) . Suponha que o par\u00e2metro seja desconhecido e que a distribui\u00e7\u00e3o da priori seja \\xi(\\theta) . Ent\u00e3o, a distribui\u00e7\u00e3o a posteriori \u00e9: \\xi(\\theta|x) = \\frac{f(x_1|\\theta)...f(x_n|\\theta)\\xi(\\theta)}{g_n(x)}, \\theta \\in \\Omega Onde g_n \u00e9 a distribui\u00e7\u00e3o marginal conjunta de X_1,...,X_n Observe que, essencialmente \\xi(\\theta|x) \\propto f(x_1|\\theta)...f(x_n|\\theta)\\xi(\\theta) , mas que sua integral seja 1 . Queremos que essa fun\u00e7\u00e3o seja integr\u00e1vel e a integral sobre o dom\u00ednio seja 1 . Fun\u00e7\u00e3o de Verossimilhan\u00e7a Quando a fun\u00e7\u00e3o de densidade de probabilidade f_n(x|\\theta) das observa\u00e7\u00f5es de uma amostra aleat\u00f3ria \u00e9 vista como uma fun\u00e7\u00e3o de \\theta , chamamos ela de fun\u00e7\u00e3o de verossimilhan\u00e7a. \\theta \\mapsto f_n(x|\\theta) := L(\\theta|x) Observa\u00e7\u00f5es Sequenciais e Predi\u00e7\u00f5es Nesse caso a ordem das vari\u00e1veis X_1, ..., X_n importam (como uma s\u00e9rie temporal, por exemplo). Nesse caso, podemos, iterativamente fazer: \\xi(\\theta|x_1) \\propto f(x_1|\\theta)\\xi(\\theta) \\xi(\\theta|x_1,...,x_{n+1}) \\propto f(x_n|\\theta)\\xi(\\theta|x_1,....,x_n) Isso acontece dada a independ\u00eancia das vari\u00e1veis aleat\u00f3rias. Notebook de Refer\u00eancia Frequentistas Os dados observados s\u00e3o considerados aleat\u00f3rios, realidados de um processo aleat\u00f3rio. Os par\u00e2metros do modelo s\u00e3o fixos e desconhecidos Queremos derivas estimadores para os par\u00e2metros desconhecidos. Bayesianos Os dados s\u00e3o fixos, isto \u00e9, vieram de um processo aleat\u00f3rio, mas depois eles n\u00e3o se alteram. Os par\u00e2metros s\u00e3o usualmente representados por distribui\u00e7\u00f5es, s\u00e3o vari\u00e1veis aleat\u00f3rias. F\u00f3rmula de Bayes. Simples exemplo de infer\u00eancia Bayesiana Hemofilia \u00e9 uma disordem gen\u00e9tica que prejudica a coagula\u00e7\u00e3o em resposta a rupturas em vasos sangu\u00edneos. \u00c9 recessiva ligada ao cromossomo X. Isso implica que homens com 1 gene s\u00e3o afetados, enquanto as mulheres n\u00e3o s\u00e3o afetadas, mas portadoras. Considere uma mulher cuja m\u00e3e \u00e9 portadora e tem um irm\u00e3o afetado. Ela se casa com um homem n\u00e3o afetado. A mulher tem dois filhos consecutivos que n\u00e3o s\u00e3o afetados. Ser\u00e1 que a m\u00e3e \u00e9 portadora? A pergunra \u00e9 simples. Vamos tentar usar um pouco do que sabemos. Seja W = 1 se a mulher \u00e9 portadora e W = 0 se ela n\u00e3o for portadora. Queremos saber P(W = 1|s_1 = 0, s_2 = 0) , isto \u00e9, os filhos n\u00e3o s\u00e3o afetados. Que informa\u00e7\u00e3o n\u00f3s temos ? A m\u00e3e dela \u00e9 portadora, portanto uma priori interessante \u00e9: P(W = 1) = 0.5 \\Rightarrow O(W = 1) = \\frac{P(W=1)}{P(W=0)} = 1 \\text{ chances (odds) a priori } Podemos calcular a fun\u00e7\u00e3o de verossimilhan\u00e7a: L(W = 1|s_1 = 0, s_2 = 0) = F(s_1 = 0, s_2 = 0 | W = 1) = (0.5)(0.5) = 0.25 L(W = 0|s_1 = 0, s_2 = 0) = (1)(1) = 1 import numpy as np import matplotlib.pyplot as plt from scipy.stats import norm priori = 0.5 # P(W = 1) p = 0.5 # prob de um filho ser afetado Likelihood = lambda w , s : np . prod ([( 1 - i , p ** i * ( 1 - p ) ** ( 1 - i ))[ w ] for i in s ]) s = [ 0 , 0 ] posteriori = Likelihood ( 1 , s ) * priori / ( Likelihood ( 1 , s ) * priori + Likelihood ( 0 , s ) * ( 1 - priori )) print ( \"A probabilidade da m\u00e3e portar \u00e9 {} com dois filhos n\u00e3o portadores.\" . format ( posteriori )) A probabilidade da m\u00e3e portar \u00e9 0.2 com dois filhos n\u00e3o portadores. s = [ 0 ] # terceiro filho priori = posteriori posteriori = Likelihood ( 1 , s ) * priori / ( Likelihood ( 1 , s ) * priori + Likelihood ( 0 , s ) * ( 1 - priori )) print ( \"A probabilidade da m\u00e3e portar \u00e9 {:.3f} com tr\u00eas filhos n\u00e3o portadores.\" . format ( posteriori )) A probabilidade da m\u00e3e portar \u00e9 0.111 com tr\u00eas filhos n\u00e3o portadores. priori = 0.5 p = 0.5 s = [ 0 ] posteriori = [] for i in range ( 50 ): posteriori . append ( Likelihood ( 1 , s ) * priori / ( Likelihood ( 1 , s ) * priori + Likelihood ( 0 , s ) * ( 1 - priori ))) priori = posteriori [ - 1 ] priori = 0.5 posteriori2 = [] for i in range ( 50 ): posteriori2 . append ( Likelihood ( 1 , s ) * priori / ( Likelihood ( 1 , s ) * priori + Likelihood ( 0 , s ) * ( 1 - priori ))) priori = posteriori2 [ - 1 ] s = [ np . random . choice ([ 0 , 1 ], p = ( 0.9 , 0.1 ))] plt . plot ( range ( 50 ), posteriori , label = 'Situa\u00e7\u00e3o 1' ) plt . plot ( range ( 50 ), posteriori2 , label = 'Situa\u00e7\u00e3o 2' ) plt . legend () plt . title ( 'Probabilidade dado cada filho' ) plt . show () Princ\u00edpio de Verossimilhan\u00e7a Afirma que para uma infer\u00eancia sobre um par\u00e2metro \\theta , toda evid\u00eancia de qualquer observa\u00e7\u00e3o de uma vari\u00e1vel aleat\u00f3ria X = x com distribui\u00e7\u00e3o X \\sim f(x|\\theta) se encontra na fun\u00e7\u00e3o de verossimilhan\u00e7a L(\\theta|x) . A interpreta\u00e7\u00e3o \u00e9 de que qualquer observa\u00e7\u00e3o de X pode construir conclus\u00f5es sobre \\theta . Al\u00e9m disso, se pud\u00e9ssemos obter informa\u00e7\u00e3o de \\theta sobre outra vari\u00e1vel aleat\u00f3ria Y com verossimilhan\u00e7a \\tilde{L} , teremos que L(\\theta|x) = c\\cdot \\tilde{L}(\\theta|y) . Isto \u00e9, as conclus\u00f5es sobre o par\u00e2metro n\u00e3o dependem da observa\u00e7\u00e3o feita. Qual o problema? Jeffreys : \"An hypothesis that may be true is rejected because it has failed to predict observable results that have not occurred. \" import pymc3 as pm from pymc3 import Model , Normal , Slice from pymc3 import sample from pymc3 import traceplot from pymc3.distributions import Interpolated from scipy import stats import matplotlib as mpl plt . style . use ( 'seaborn-darkgrid' ) Gerando os dados Y = \\alpha + \\beta_0\\cdot X_1 + \\beta_1\\cdot X_2 + \\text{erro} # True parameters alpha_true = 5 beta0_true = 7 beta1_true = 13 # Size of the dataset size = 100 # Random variables np . random . seed ( 1 ) X1 = np . random . randn ( size ) X2 = np . random . randn ( size ) * 0.2 e = np . random . randn ( size ) Y = alpha_true + beta0_true * X1 + beta1_true * X2 + e Especificando o modelo Vamos fazer um modelo simples aqui, s\u00f3 para mostrar essa biblioteca nova. A ideia \u00e9 mostrar como funciona a ideia de priori e posteriori. Vamos dizer que Y \\sim N(\\mu, 1) , onde \\mu = \\alpha + \\beta_1 X_1 + \\beta_2 X_2 model = Model () #cria um novo modelo, que em Python, \u00e9 um objeto with model : # Isso cria um contexto em python # Vamos dizer nossas prioris! alpha = Normal ( 'alpha' , mu = 0 , sigma = 1 ) beta0 = Normal ( 'beta0' , mu = 12 , sigma = 1 ) beta1 = Normal ( 'beta1' , mu = 18 , sigma = 1 ) # Valor esperado da sa\u00edda mu = alpha + beta0 * X1 + beta1 * X2 # Verossimilhan\u00e7a das observa\u00e7\u00f5es Y_obs = Normal ( 'Y_obs' , mu = mu , sigma = 1 , observed = Y ) # Amostras da distribui\u00e7\u00e3o trace = sample ( 1000 ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. A pr\u00f3xima fun\u00e7\u00e3o \u00e9 um plot de distribui\u00e7\u00e3o a priori amostrada (que \u00e9 basicamente um histograma) de cada par\u00e2metro. Veja que parece um chiado em torno da m\u00e9dia. traceplot ( trace ); Agora que n\u00f3s temos os dados gerados Y , vamos atualizar nosso conhecimento, nossa confian\u00e7a nos par\u00e2metros, atrav\u00e9s da distribui\u00e7\u00e3o a posteriori. Os dados devem ser independentes a ada intera\u00e7\u00e3o para que valha o que estudamos. Para isso, precisamos calcular a posteriori de cada par\u00e2metro. Nesse caso, vamos utilizar uma aproxima\u00e7\u00e3o para a distribui\u00e7\u00e3o, utlizando aproxima\u00e7\u00e3o Kernel . N\u00e3o se preocupe com isso, \u00e9 s\u00f3 para mostrar que estamos calculado a posteriori def from_posterior ( param , samples ): smin , smax = np . min ( samples ), np . max ( samples ) width = smax - smin x = np . linspace ( smin , smax , 100 ) y = stats . gaussian_kde ( samples )( x ) # what was never sampled should have a small probability but not 0, # so we'll extend the domain and use linear approximation of density on it x = np . concatenate ([[ x [ 0 ] - 3 * width ], x , [ x [ - 1 ] + 3 * width ]]) y = np . concatenate ([[ 0 ], y , [ 0 ]]) return Interpolated ( param , x , y ) Agora, vamos gerar mais dados e usar a F\u00f3rmula de Bayes e usaremos uma forma sequencial das observa\u00e7\u00f5es, isto \u00e9, a posteriori da itera\u00e7\u00e3o n-1 ser\u00e1 a priori da itera\u00e7\u00e3o n . traces = [ trace ] # salva os tra\u00e7os para que plotamos depois. for _ in range ( 10 ): # _ indica uma vari\u00e1vel que n\u00e3o \u00e9 usada # Gerando mais e mais dados! X1 = np . random . randn ( size ) X2 = np . random . randn ( size ) * 0.2 e = np . random . randn ( size ) Y = alpha_true + beta0_true * X1 + beta1_true * X2 + e model = Model () with model : # As novas prioris s\u00e3o as posterioris alpha = from_posterior ( 'alpha' , trace [ 'alpha' ]) beta0 = from_posterior ( 'beta0' , trace [ 'beta0' ]) beta1 = from_posterior ( 'beta1' , trace [ 'beta1' ]) # EValor esperado da sa\u00edda mu = alpha + beta0 * X1 + beta1 * X2 # Calculando a verossimilhan\u00e7a dos novos dados Y_obs = Normal ( 'Y_obs' , mu = mu , sigma = 1 , observed = Y ) # Amostrando da posteriori, porque n\u00e3o estamos calculando a forma fechada trace = sample ( 1000 ) traces . append ( trace ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 5 seconds. The number of effective samples is smaller than 25% for some parameters. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 5 seconds. The estimated number of effective samples is smaller than 200 for some parameters. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. The rhat statistic is larger than 1.4 for some parameters. The sampler did not converge. The estimated number of effective samples is smaller than 200 for some parameters. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. The number of effective samples is smaller than 25% for some parameters. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. The acceptance probability does not match the target. It is 0.8800702431829607, but should be close to 0.8. Try to increase the number of tuning steps. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. fig , ax = plt . subplots ( 1 , 3 , figsize = ( 20 , 5 )) # Definindo cores cmap = mpl . cm . autumn for index , param in enumerate ([ 'alpha' , 'beta0' , 'beta1' ]): for update_i , trace in enumerate ( traces ): samples = trace [ param ] smin , smax = np . min ( samples ), np . max ( samples ) x = np . linspace ( smin , smax , 100 ) y = stats . gaussian_kde ( samples )( x ) ax [ index ] . plot ( x , y , color = cmap ( 1 - update_i / len ( traces )), alpha = update_i / len ( traces )) ax [ index ] . axvline ({ 'alpha' : alpha_true , 'beta0' : beta0_true , 'beta1' : beta1_true }[ param ], c = 'k' ) ax [ index ] . set_ylabel ( 'Frequency' ) ax [ index ] . set_title ( param ) plt . tight_layout ();","title":"Distribui\u00e7\u00f5es a Priori e a Posteriori"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#distribuicoes-a-priori-e-a-posteriori","text":"","title":"Distribui\u00e7\u00f5es a Priori e a Posteriori"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#priori","text":"Tratamos \\theta de um modelo como uma vari\u00e1vel aleat\u00f3ria e atribuimos uma distribui\u00e7\u00e3o para esse par\u00e2metro. O nome ser\u00e1 distribui\u00e7\u00e3o a priori. Ao fazer modelagens, ela \u00e9 em geral pr\u00e9-definida pelo modelador, que \u00e9 em geral aconselhado por um especialista.","title":"Priori"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#posteriori","text":"Sejam X_1, ..., X_n v.a. observadas e um par\u00e2metro \\theta desconhecido. A distribui\u00e7\u00e3o de \\theta condicionado nas vari\u00e1veis aleat\u00f3rias \u00e9 a distribui\u00e7\u00e3o a posteriori. Observe a rela\u00e7\u00e3o com o Teorema de Bayes.","title":"Posteriori"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#teorema","text":"Suponha que X_1, ..., X_n formam uma amostra aleat\u00f3ria de uma distribui\u00e7\u00e3o f(x|\\theta) . Suponha que o par\u00e2metro seja desconhecido e que a distribui\u00e7\u00e3o da priori seja \\xi(\\theta) . Ent\u00e3o, a distribui\u00e7\u00e3o a posteriori \u00e9: \\xi(\\theta|x) = \\frac{f(x_1|\\theta)...f(x_n|\\theta)\\xi(\\theta)}{g_n(x)}, \\theta \\in \\Omega Onde g_n \u00e9 a distribui\u00e7\u00e3o marginal conjunta de X_1,...,X_n Observe que, essencialmente \\xi(\\theta|x) \\propto f(x_1|\\theta)...f(x_n|\\theta)\\xi(\\theta) , mas que sua integral seja 1 . Queremos que essa fun\u00e7\u00e3o seja integr\u00e1vel e a integral sobre o dom\u00ednio seja 1 .","title":"Teorema"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#funcao-de-verossimilhanca","text":"Quando a fun\u00e7\u00e3o de densidade de probabilidade f_n(x|\\theta) das observa\u00e7\u00f5es de uma amostra aleat\u00f3ria \u00e9 vista como uma fun\u00e7\u00e3o de \\theta , chamamos ela de fun\u00e7\u00e3o de verossimilhan\u00e7a. \\theta \\mapsto f_n(x|\\theta) := L(\\theta|x)","title":"Fun\u00e7\u00e3o de Verossimilhan\u00e7a"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#observacoes-sequenciais-e-predicoes","text":"Nesse caso a ordem das vari\u00e1veis X_1, ..., X_n importam (como uma s\u00e9rie temporal, por exemplo). Nesse caso, podemos, iterativamente fazer: \\xi(\\theta|x_1) \\propto f(x_1|\\theta)\\xi(\\theta) \\xi(\\theta|x_1,...,x_{n+1}) \\propto f(x_n|\\theta)\\xi(\\theta|x_1,....,x_n) Isso acontece dada a independ\u00eancia das vari\u00e1veis aleat\u00f3rias. Notebook de Refer\u00eancia","title":"Observa\u00e7\u00f5es Sequenciais e Predi\u00e7\u00f5es"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#frequentistas","text":"Os dados observados s\u00e3o considerados aleat\u00f3rios, realidados de um processo aleat\u00f3rio. Os par\u00e2metros do modelo s\u00e3o fixos e desconhecidos Queremos derivas estimadores para os par\u00e2metros desconhecidos.","title":"Frequentistas"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#bayesianos","text":"Os dados s\u00e3o fixos, isto \u00e9, vieram de um processo aleat\u00f3rio, mas depois eles n\u00e3o se alteram. Os par\u00e2metros s\u00e3o usualmente representados por distribui\u00e7\u00f5es, s\u00e3o vari\u00e1veis aleat\u00f3rias. F\u00f3rmula de Bayes.","title":"Bayesianos"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#simples-exemplo-de-inferencia-bayesiana","text":"Hemofilia \u00e9 uma disordem gen\u00e9tica que prejudica a coagula\u00e7\u00e3o em resposta a rupturas em vasos sangu\u00edneos. \u00c9 recessiva ligada ao cromossomo X. Isso implica que homens com 1 gene s\u00e3o afetados, enquanto as mulheres n\u00e3o s\u00e3o afetadas, mas portadoras. Considere uma mulher cuja m\u00e3e \u00e9 portadora e tem um irm\u00e3o afetado. Ela se casa com um homem n\u00e3o afetado. A mulher tem dois filhos consecutivos que n\u00e3o s\u00e3o afetados. Ser\u00e1 que a m\u00e3e \u00e9 portadora? A pergunra \u00e9 simples. Vamos tentar usar um pouco do que sabemos. Seja W = 1 se a mulher \u00e9 portadora e W = 0 se ela n\u00e3o for portadora. Queremos saber P(W = 1|s_1 = 0, s_2 = 0) , isto \u00e9, os filhos n\u00e3o s\u00e3o afetados. Que informa\u00e7\u00e3o n\u00f3s temos ? A m\u00e3e dela \u00e9 portadora, portanto uma priori interessante \u00e9: P(W = 1) = 0.5 \\Rightarrow O(W = 1) = \\frac{P(W=1)}{P(W=0)} = 1 \\text{ chances (odds) a priori } Podemos calcular a fun\u00e7\u00e3o de verossimilhan\u00e7a: L(W = 1|s_1 = 0, s_2 = 0) = F(s_1 = 0, s_2 = 0 | W = 1) = (0.5)(0.5) = 0.25 L(W = 0|s_1 = 0, s_2 = 0) = (1)(1) = 1 import numpy as np import matplotlib.pyplot as plt from scipy.stats import norm priori = 0.5 # P(W = 1) p = 0.5 # prob de um filho ser afetado Likelihood = lambda w , s : np . prod ([( 1 - i , p ** i * ( 1 - p ) ** ( 1 - i ))[ w ] for i in s ]) s = [ 0 , 0 ] posteriori = Likelihood ( 1 , s ) * priori / ( Likelihood ( 1 , s ) * priori + Likelihood ( 0 , s ) * ( 1 - priori )) print ( \"A probabilidade da m\u00e3e portar \u00e9 {} com dois filhos n\u00e3o portadores.\" . format ( posteriori )) A probabilidade da m\u00e3e portar \u00e9 0.2 com dois filhos n\u00e3o portadores. s = [ 0 ] # terceiro filho priori = posteriori posteriori = Likelihood ( 1 , s ) * priori / ( Likelihood ( 1 , s ) * priori + Likelihood ( 0 , s ) * ( 1 - priori )) print ( \"A probabilidade da m\u00e3e portar \u00e9 {:.3f} com tr\u00eas filhos n\u00e3o portadores.\" . format ( posteriori )) A probabilidade da m\u00e3e portar \u00e9 0.111 com tr\u00eas filhos n\u00e3o portadores. priori = 0.5 p = 0.5 s = [ 0 ] posteriori = [] for i in range ( 50 ): posteriori . append ( Likelihood ( 1 , s ) * priori / ( Likelihood ( 1 , s ) * priori + Likelihood ( 0 , s ) * ( 1 - priori ))) priori = posteriori [ - 1 ] priori = 0.5 posteriori2 = [] for i in range ( 50 ): posteriori2 . append ( Likelihood ( 1 , s ) * priori / ( Likelihood ( 1 , s ) * priori + Likelihood ( 0 , s ) * ( 1 - priori ))) priori = posteriori2 [ - 1 ] s = [ np . random . choice ([ 0 , 1 ], p = ( 0.9 , 0.1 ))] plt . plot ( range ( 50 ), posteriori , label = 'Situa\u00e7\u00e3o 1' ) plt . plot ( range ( 50 ), posteriori2 , label = 'Situa\u00e7\u00e3o 2' ) plt . legend () plt . title ( 'Probabilidade dado cada filho' ) plt . show ()","title":"Simples exemplo de infer\u00eancia Bayesiana"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#principio-de-verossimilhanca","text":"Afirma que para uma infer\u00eancia sobre um par\u00e2metro \\theta , toda evid\u00eancia de qualquer observa\u00e7\u00e3o de uma vari\u00e1vel aleat\u00f3ria X = x com distribui\u00e7\u00e3o X \\sim f(x|\\theta) se encontra na fun\u00e7\u00e3o de verossimilhan\u00e7a L(\\theta|x) . A interpreta\u00e7\u00e3o \u00e9 de que qualquer observa\u00e7\u00e3o de X pode construir conclus\u00f5es sobre \\theta . Al\u00e9m disso, se pud\u00e9ssemos obter informa\u00e7\u00e3o de \\theta sobre outra vari\u00e1vel aleat\u00f3ria Y com verossimilhan\u00e7a \\tilde{L} , teremos que L(\\theta|x) = c\\cdot \\tilde{L}(\\theta|y) . Isto \u00e9, as conclus\u00f5es sobre o par\u00e2metro n\u00e3o dependem da observa\u00e7\u00e3o feita. Qual o problema? Jeffreys : \"An hypothesis that may be true is rejected because it has failed to predict observable results that have not occurred. \" import pymc3 as pm from pymc3 import Model , Normal , Slice from pymc3 import sample from pymc3 import traceplot from pymc3.distributions import Interpolated from scipy import stats import matplotlib as mpl plt . style . use ( 'seaborn-darkgrid' )","title":"Princ\u00edpio de Verossimilhan\u00e7a"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#gerando-os-dados","text":"Y = \\alpha + \\beta_0\\cdot X_1 + \\beta_1\\cdot X_2 + \\text{erro} # True parameters alpha_true = 5 beta0_true = 7 beta1_true = 13 # Size of the dataset size = 100 # Random variables np . random . seed ( 1 ) X1 = np . random . randn ( size ) X2 = np . random . randn ( size ) * 0.2 e = np . random . randn ( size ) Y = alpha_true + beta0_true * X1 + beta1_true * X2 + e","title":"Gerando os dados"},{"location":"infestatistica/PrioriPosteriori/PrioriPosteriori/#especificando-o-modelo","text":"Vamos fazer um modelo simples aqui, s\u00f3 para mostrar essa biblioteca nova. A ideia \u00e9 mostrar como funciona a ideia de priori e posteriori. Vamos dizer que Y \\sim N(\\mu, 1) , onde \\mu = \\alpha + \\beta_1 X_1 + \\beta_2 X_2 model = Model () #cria um novo modelo, que em Python, \u00e9 um objeto with model : # Isso cria um contexto em python # Vamos dizer nossas prioris! alpha = Normal ( 'alpha' , mu = 0 , sigma = 1 ) beta0 = Normal ( 'beta0' , mu = 12 , sigma = 1 ) beta1 = Normal ( 'beta1' , mu = 18 , sigma = 1 ) # Valor esperado da sa\u00edda mu = alpha + beta0 * X1 + beta1 * X2 # Verossimilhan\u00e7a das observa\u00e7\u00f5es Y_obs = Normal ( 'Y_obs' , mu = mu , sigma = 1 , observed = Y ) # Amostras da distribui\u00e7\u00e3o trace = sample ( 1000 ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. A pr\u00f3xima fun\u00e7\u00e3o \u00e9 um plot de distribui\u00e7\u00e3o a priori amostrada (que \u00e9 basicamente um histograma) de cada par\u00e2metro. Veja que parece um chiado em torno da m\u00e9dia. traceplot ( trace ); Agora que n\u00f3s temos os dados gerados Y , vamos atualizar nosso conhecimento, nossa confian\u00e7a nos par\u00e2metros, atrav\u00e9s da distribui\u00e7\u00e3o a posteriori. Os dados devem ser independentes a ada intera\u00e7\u00e3o para que valha o que estudamos. Para isso, precisamos calcular a posteriori de cada par\u00e2metro. Nesse caso, vamos utilizar uma aproxima\u00e7\u00e3o para a distribui\u00e7\u00e3o, utlizando aproxima\u00e7\u00e3o Kernel . N\u00e3o se preocupe com isso, \u00e9 s\u00f3 para mostrar que estamos calculado a posteriori def from_posterior ( param , samples ): smin , smax = np . min ( samples ), np . max ( samples ) width = smax - smin x = np . linspace ( smin , smax , 100 ) y = stats . gaussian_kde ( samples )( x ) # what was never sampled should have a small probability but not 0, # so we'll extend the domain and use linear approximation of density on it x = np . concatenate ([[ x [ 0 ] - 3 * width ], x , [ x [ - 1 ] + 3 * width ]]) y = np . concatenate ([[ 0 ], y , [ 0 ]]) return Interpolated ( param , x , y ) Agora, vamos gerar mais dados e usar a F\u00f3rmula de Bayes e usaremos uma forma sequencial das observa\u00e7\u00f5es, isto \u00e9, a posteriori da itera\u00e7\u00e3o n-1 ser\u00e1 a priori da itera\u00e7\u00e3o n . traces = [ trace ] # salva os tra\u00e7os para que plotamos depois. for _ in range ( 10 ): # _ indica uma vari\u00e1vel que n\u00e3o \u00e9 usada # Gerando mais e mais dados! X1 = np . random . randn ( size ) X2 = np . random . randn ( size ) * 0.2 e = np . random . randn ( size ) Y = alpha_true + beta0_true * X1 + beta1_true * X2 + e model = Model () with model : # As novas prioris s\u00e3o as posterioris alpha = from_posterior ( 'alpha' , trace [ 'alpha' ]) beta0 = from_posterior ( 'beta0' , trace [ 'beta0' ]) beta1 = from_posterior ( 'beta1' , trace [ 'beta1' ]) # EValor esperado da sa\u00edda mu = alpha + beta0 * X1 + beta1 * X2 # Calculando a verossimilhan\u00e7a dos novos dados Y_obs = Normal ( 'Y_obs' , mu = mu , sigma = 1 , observed = Y ) # Amostrando da posteriori, porque n\u00e3o estamos calculando a forma fechada trace = sample ( 1000 ) traces . append ( trace ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 5 seconds. The number of effective samples is smaller than 25% for some parameters. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 5 seconds. The estimated number of effective samples is smaller than 200 for some parameters. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. The rhat statistic is larger than 1.4 for some parameters. The sampler did not converge. The estimated number of effective samples is smaller than 200 for some parameters. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. The number of effective samples is smaller than 25% for some parameters. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. The acceptance probability does not match the target. It is 0.8800702431829607, but should be close to 0.8. Try to increase the number of tuning steps. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. fig , ax = plt . subplots ( 1 , 3 , figsize = ( 20 , 5 )) # Definindo cores cmap = mpl . cm . autumn for index , param in enumerate ([ 'alpha' , 'beta0' , 'beta1' ]): for update_i , trace in enumerate ( traces ): samples = trace [ param ] smin , smax = np . min ( samples ), np . max ( samples ) x = np . linspace ( smin , smax , 100 ) y = stats . gaussian_kde ( samples )( x ) ax [ index ] . plot ( x , y , color = cmap ( 1 - update_i / len ( traces )), alpha = update_i / len ( traces )) ax [ index ] . axvline ({ 'alpha' : alpha_true , 'beta0' : beta0_true , 'beta1' : beta1_true }[ param ], c = 'k' ) ax [ index ] . set_ylabel ( 'Frequency' ) ax [ index ] . set_title ( param ) plt . tight_layout ();","title":"Especificando o modelo"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/","text":"Distribui\u00e7\u00e3o Chi-Quadrado Para cada m positivo, a distribui\u00e7\u00e3o Gamma(m/2, 1/2) \u00e9 chamada de distribui\u00e7\u00e3o \\chi^2 . Ela foi primeiramente descrita por Helmert para computar a distribui\u00e7\u00e3o amostral de uma popula\u00e7\u00e3o normal. Vamos ver como a normal se relaciona mais a frente. f(x) = \\frac{1}{2^{m/2}\\Gamma(m/2)}x^{m/2 - 1}e^{-x/2} Propriedades Se X \\sim \\chi^2(m) , ent\u00e3o: E(X) = m Var(X) = 2m \\psi(t) = \\left(\\frac{1}{1-2t}\\right)^{m/2}, t < \\frac{1}{2} Soma de \\chi^2 Se X_1, ..., X_k s\u00e3o independentes e cada uma tem grau de liberdade m_i , ent\u00e3o X_1 + ... + X_n tem distribui\u00e7\u00e3o \\chi^2(m_1 + .... + m_k) Rela\u00e7\u00e3o com a Normal Se X tem distribui\u00e7\u00e3o normal padr\u00e3o, Y = X^2 \\sim \\chi^2(1) De fato, se juntarmos as \u00faltimos dois teoremas, veremos que a soma de quadrados de normais independentes e identicamente distribuidas ser\u00e1 \\chi^2(m) , onde m \u00e9 o n\u00famero de parcelas. Implementa\u00e7\u00e3o import numpy as np import matplotlib.pyplot as plt from scipy.stats import chi2 from matplotlib import animation , cm from IPython.display import HTML # Random Object ro = np . random . default_rng ( 1000 ) # Para assegurar reprodutibilidade degree_freedom = 10 mean , var , skew , kurt = chi2 . stats ( degree_freedom , moments = 'mvsk' ) print ( 'Propriedades' ) print ( 'M\u00e9dia: {} ' . format ( mean )) print ( 'Var: {} ' . format ( var )) print ( 'Assimetria: {} ' . format ( skew )) print ( 'Curtose: {} ' . format ( kurt )) Propriedades M\u00e9dia: 10.0 Var: 20.0 Assimetria: 0.8944271909999159 Curtose: 1.2 fig , ax = plt . subplots ( 1 , 1 ) x = np . linspace ( chi2 . ppf ( 0.01 , degree_freedom ), chi2 . ppf ( 0.99 , degree_freedom ), 100 ) ax . plot ( x , chi2 . pdf ( x , degree_freedom ), 'r-' , lw = 5 , alpha = 0.6 , label = 'chi2 pdf' ) r = chi2 . rvs ( degree_freedom , size = 10000 ) ax . hist ( r , density = True , alpha = 0.2 ) ax . legend () plt . show () fig , ax = plt . subplots () line , = ax . plot ( x , chi2 . pdf ( x , degree_freedom ), 'r-' , lw = 5 , alpha = 0.6 ) ax . set_xlim (( 0 , 150 )) ax . set_title ( 'Chi-Square' ) def animate ( i , degree_freedom ): x = np . linspace ( 0 , chi2 . ppf ( 0.99 , degree_freedom + i ), 100 ) line . set_data ( x , chi2 . pdf ( x , degree_freedom + i )) return line , anim = animation . FuncAnimation ( fig , animate , frames = 100 , interval = 50 , fargs = ( degree_freedom ,), repeat = False ) HTML ( anim . to_html5_video ()) Your browser does not support the video tag. Distribui\u00e7\u00e3o Conjunta da m\u00e9dia e vari\u00e2ncia amostrais X_1,...,X_n formam uma amostra aleat\u00f3ria com distribui\u00e7\u00e3o normal e com m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 desconhecidos. Estamos interessados na distribui\u00e7\u00e3o conjunta dos estimadores de m\u00e1xima verossimilhan\u00e7a para m\u00e9dia e vari\u00e2ncia da amostra. Teorema de Basu Sejam \\hat{\\mu} = \\bar{X}_n e \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2 a m\u00e9dia e vari\u00e2ncia amostrais, respectivamente. Ent\u00e3o \\hat{\\mu} tem distribui\u00e7\u00e3o normal com m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 /n , enquanto n\\hat{\\sigma}^2/\\sigma^2 tem a distribui\u00e7\u00e3o \\chi^2(n-1) , isto \u00e9, com n-1 grau de liberdade. Al\u00e9m disso elas s\u00e3o independentes. Esse teorema \u00e9 um pouco mais complexo e, na verdade, essa seria uma esp\u00e9cie de aplica\u00e7\u00e3o do teorema, na verdade. O teorema de Basu diz que: Se T \u00e9 uma estat\u00edstica suficiente completa (Considere, nesse teorema, g uma fun\u00e7\u00e3o integr\u00e1vel limitada) para \\theta e A uma estat\u00edstica ancillary, ent\u00e3o T \u00e9 independente de A . Nesse caso \\hat{\\mu} \u00e9 completa suficiente e \\hat{\\sigma}^2 \u00e9 ancillary, por que n\u00e3o depende de \\mu . O mais interessante \u00e9 que essa propriedade \u00e9 s\u00f3 vista com a distribui\u00e7\u00e3o normal ! Olhem a p\u00e1gina 9. Demonstra\u00e7\u00e3o O livro tem uma abordagem um pouco mais voltado \u00e0 \u00c1lgebra Linear. Aqui vou mostrar uma ideia um pouco diferente, onde voc\u00eas podem demonstrar os passos, como exerc\u00edcio. Passo 1: \\sum_{i=1}^n X_i^2 = n\\hat{\\sigma}^2 + n\\hat{\\mu}^2 Dica: Escrever \\hat{\\sigma}^2 e abrir em tr\u00eas somat\u00f3rios. Passo 2: \\sum_{i=1}^n (X_i - \\mu)^2 = n\\hat{\\sigma}^2 + n(\\hat{\\mu} - \\mu)^2 Dica: O Passo 1 \u00e9 um caso especial do Passo 2. O processo \u00e9 o mesmo. Passo 3: \\hat{\\mu} \u00e9 independente de X_i - \\hat{\\mu}, i = 1,...,n . Dica: Montar a pdf conjunta de X_1, ..., X_n (j\u00e1 fizemos isso atrave\u015b da verossimilhan\u00e7a) e fazer uma mudan\u00e7a de vari\u00e1vel Y_1 = \\hat{\\mu}, Y_2 = X_2 - \\hat{\\mu}, ..., Y_n = X_n - \\hat{\\mu} . Com essa mudan\u00e7a, \u00e9 poss\u00edvel montar a pdf como fun\u00e7\u00e3o de y_1,...,y_n . Esse processo \u00e9 um pouco mais chato, mas \u00e9 bom lembrar como fazez mudan\u00e7a de vari\u00e1vel para pdfs. Aqui voc\u00ea pode conferir como . \u00c9 importante lembrar que \u00e9 uma fun\u00e7\u00e3o de y ap\u00f3s transformada e n\u00e3o de x . Dica 2: Fatorizar a pdf conjunta. Voc\u00ea vai ver como se destaca a independ\u00eancia aqui. Passo 4: Mostrar que \\hat{\\mu} e \\hat{\\sigma}^2 s\u00e3o independentes. Refer\u00eancias 1 2 Simples visualiza\u00e7\u00e3o Eu gostaria de comparar o que acontece com a m\u00e9dia e vari\u00e2ncia amostral da distribui\u00e7\u00e3o normal e da distribui\u00e7\u00e3o gamma. Para isso, geero amostras de tamanho n , calculo as estat\u00edsticas e salvo. Fa\u00e7o esse procedimento o n\u00famero de pontos que quiser. ite = 10000 n = 10000 # Par\u00e2metros da Normal mu = 5 sigma = 2 # Par\u00e2metros da Gamma alpha = 5 beta = 4 means = np . zeros (( ite , 2 )) variances = np . zeros (( ite , 2 )) for i in range ( ite ): X = ro . normal ( loc = mu , scale = sigma , size = n ) Y = ro . gamma ( shape = alpha , scale = 1 / beta , size = n ) means [ i , 0 ] = np . mean ( X ) means [ i , 1 ] = np . mean ( Y ) variances [ i , 0 ] = np . var ( X , ddof = 0 ) variances [ i , 1 ] = np . var ( Y , ddof = 0 ) coef_normal = np . polyfit ( x = means [:, 0 ], y = variances [:, 0 ], deg = 1 ) coef_gamma = np . polyfit ( x = means [:, 1 ], y = variances [:, 1 ], deg = 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 14 , 5 )) fig . suptitle ( 'Comparando m\u00e9dia e vari\u00e2ncia amostral' ) ax [ 0 ] . scatter ( means [:, 0 ], variances [:, 0 ]) ax [ 1 ] . scatter ( means [:, 1 ], variances [:, 1 ]) ax [ 0 ] . plot ( means [:, 0 ], coef_normal [ 0 ] * means [:, 0 ] + coef_normal [ 1 ], color = 'red' ) ax [ 1 ] . plot ( means [:, 1 ], coef_gamma [ 0 ] * means [:, 1 ] + coef_gamma [ 1 ], color = 'red' ) ax [ 0 ] . set_xlabel ( r '$\\bar {X} _n$' , fontsize = 18 ) ax [ 1 ] . set_xlabel ( r '$\\bar {X} _n$' , fontsize = 18 ) ax [ 0 ] . set_ylabel ( r '$\\sum (X_i - \\bar {X} _n)^2$' , fontsize = 18 ) ax [ 1 ] . set_ylabel ( r '$\\sum (X_i - \\bar {X} _n)^2$' , fontsize = 18 ) ax [ 0 ] . set_title ( 'Distribui\u00e7\u00e3o Normal' ) ax [ 1 ] . set_title ( 'Distribui\u00e7\u00e3o Gamma' ) ax [ 0 ] . grid ( alpha = 0.5 , linestyle = '--' ) ax [ 1 ] . grid ( alpha = 0.5 , linestyle = '--' ) plt . show () Obs: A n\u00e3o inclina\u00e7\u00e3o da reta n\u00e3o significa que existe independ\u00eancia, mas como s\u00e3o independentes, a gente espera que a inclina\u00e7\u00e3o seja pequena. Distribui\u00e7\u00f5es T Student Artigo original : Olhe a p\u00e1gina 9! Defini\u00e7\u00e3o Sejam Y \\sim \\chi^2(m) e Z \\sim N(0,1) independentes. Ent\u00e3o X = \\frac{Z}{\\left(\\frac{Y}{m}\\right)^{1/2}} \\sim t(m) onde t(m) \u00e9 a distribui\u00e7\u00e3o t-student com m graus de liberdade. Fun\u00e7\u00e3o densidade de probabilidade Para escrever essa fun\u00e7\u00e3o de probabilidade, defina X como acima e W = Y . J\u00e1 sabemos a distribui\u00e7\u00e3o conjunta de Y e Z , pois eles s\u00e3o independentes. Com essa mudan\u00e7a de vari\u00e1vel ( confira aqui se n\u00e3o lembra como \u00e9 feito ), voc\u00ea conseque escrever a distribui\u00e7\u00e3o conjunta de X e W . Depois, basta calcular a distribui\u00e7\u00e3o marginal de X , integrando em W . f(x) = \\frac{\\Gamma\\left(\\frac{m+1}{2}\\right)}{(m\\pi)^{1/2}\\Gamma\\left(\\frac{m}{2}\\right)}\\left(1 + \\frac{x^2}{m} \\right)^{-(m+1)/2}, x \\in \\mathbb{R}, onde \\Gamma \u00e9 a fun\u00e7\u00e3o Gamma , tal que, n \\in \\mathbb{N}, \\Gamma(n) = (n-1)! \\Gamma(z+1) = z\\Gamma(z) \\Gamma(1/2) = \\sqrt{\\pi} Quando m \\leq 1 , a m\u00e9dia \u00e9 divergente. Isso pode ser vizualizado pelo expoente que ser\u00e1 \\leq -1 , o que diverge (lembre de \\int 1/x ). Quando m > 1 , a m\u00e9dia existe e \u00e9 0 pela simetria da distribui\u00e7\u00e3o. Em particular, podemos mostrar que se k < m , E[|X^k|] < + \\infty e se k \\geq m , o momento diverge. Se X \\sim t(m), m > 2 , Var(X) = \\frac{m}{m-2} Teorema Seja X_1, ..., X_n \\overset{iid}{\\sim} N(\\mu,\\sigma^2) . Seja \\sigma ' = \\left[\\frac{\\sum_{i=1}^n (X_i - \\bar{X}_n)^2}{n-1}\\right]^{1/2} Ent\u00e3o n^{1/2}(\\bar{X}_n - \\mu)/\\sigma ' \\sim t(n-1) Rela\u00e7\u00e3o com a Normal e Cauchy Da mesma forma que a distribui\u00e7\u00e3o normal e a distribui\u00e7\u00e3o Cauchy, a distribui\u00e7\u00e3o t \u00e9 centrada em 0 e tem sua moda nesse valor. Entretanto a cauda a distribui\u00e7\u00e3o t (quando x \\to -\\infty ou x \\to +\\infty ), \u00e9 mais pesada, no sentido de que tende para 0 em uma velocidade menor do que a normal. Outra coisa interessante \u00e9 que a ditrivui\u00e7\u00e3o t(1) \u00e9 a distribui\u00e7\u00e3o Cauchy . Al\u00e9m disso, quando n \\to \\infty , converge para a pdf da normal padr\u00e3o ( Normal(0,1) ). Ferramentas para demonstrar a converg\u00eancia Teorema de Slutsky : Considere o corol\u00e1rio com f(x,y) = \\frac{x}{y} Lei dos Grandes N\u00fameros : Escreva a qui-quadrado como soma de normais. from scipy.stats import t , norm , cauchy Implementa\u00e7\u00e3o Primeiro vamos ver a cara da distribui\u00e7\u00e3o t m = 10 X = t ( df = m ) w = np . arange ( - 3 , 3 , 0.1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 12 , 5 )) ax [ 0 ] . plot ( w , X . pdf ( w ), lw = 5 , color = 'orange' ) ax [ 1 ] . plot ( w , X . cdf ( w ), lw = 5 , color = 'orange' ) ax [ 0 ] . set_title ( 'PDF t-Student' ) ax [ 1 ] . set_title ( 'CDF t-Student' ) plt . show () Vamos ver o que acontece quando m \\leq 1 ? ite = 1000 n = 10000 m1 = 10 m2 = 0.5 means = np . zeros (( ite , 2 )) for i in range ( ite ): X = ro . standard_t ( df = m1 , size = n ) Y = ro . standard_t ( df = m2 , size = n ) means [ i , 0 ] = np . mean ( X ) means [ i , 1 ] = np . mean ( Y ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 14 , 5 )) ax [ 0 ] . hist ( means [:, 0 ], bins = 100 ) ax [ 1 ] . hist ( np . log ( means [:, 1 ]), bins = 10 ) ax [ 0 ] . set_xlabel ( 'E[X]' ) ax [ 1 ] . set_xlabel ( 'log E[X]' ) ax [ 0 ] . set_title ( 'm = 10' ) ax [ 1 ] . set_title ( 'm = 0.5' ) plt . show () <ipython-input-11-2bfd1961d53b>:3: RuntimeWarning: invalid value encountered in log ax[1].hist(np.log(means[:,1]), bins = 10) No eixo x do segundo gr\u00e1fico plotei o logaritmo, dado que alguns resultados eram extremamente grandes! Isso indica visualmente que a m\u00e9dia diverge! Rela\u00e7\u00e3o com a Normal e com Cauchy C = cauchy () Z = norm ( loc = 0 , scale = 1 ) T = t ( df = 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 14 , 5 )) ax [ 0 ] . plot ( w , C . pdf ( w ), label = 'Cauchy' ) ax [ 0 ] . scatter ( w , T . pdf ( w ), c = 'red' , marker = \"*\" , label = 't-Student' ) ax [ 0 ] . legend () ax [ 0 ] . set_title ( 't-Student e Cauchy quando m = 1' ) ax [ 1 ] . plot ( w , Z . pdf ( w ), label = 'N(0,1)' ) ax [ 1 ] . set_title ( 'Converg\u00eancia da t para a normal' ) for i in np . logspace ( np . log10 ( 1 ), np . log10 ( 20 ), 5 ): T = t ( df = int ( i )) ax [ 1 ] . plot ( w , T . pdf ( w ), linestyle = '--' , alpha = i / 40 + 0.5 , color = 'grey' , label = 't( {} )' . format ( int ( i ))) ax [ 1 ] . legend ( loc = 'upper right' ) plt . show () Distribui\u00e7\u00e3o F Sejam Y \\sim \\chi^2_m e W \\sim \\chi^2_n independentes. Defina X = \\frac{Y/m}{W/n} = \\frac{nY}{mW} Dizemos que X tem distribui\u00e7\u00e3o F . A sua motiva\u00e7\u00e3o vem do teste de hip\u00f3teses que compara vari\u00e2ncias de duas normais. Fun\u00e7\u00e3o de densidade de probabilidade Seja X \\sim F_{m,n} . Ent\u00e3o sua pdf tem suporte em x > 0 e pe definida f(x) = \\frac{\\Gamma\\left[\\frac{1}{2}(m+n)\\right]m^{m/2}n^{n/2}}{\\Gamma\\left(\\frac{1}{2}m\\right)\\Gamma\\left(\\frac{1}{2}n\\right)}\\cdot \\frac{x^{(m/2) - 1}}{(mx + n)^{(m+n)/2)}} Observe que ela n\u00e3o \u00e9 sim\u00e9trica em m e n . Assim, se trocarmos eles de lugar, teremos um resultado diferente. Propriedades Seja X \\sim F_{m,n} . Ent\u00e3o 1/X \\sim F_{n,m} . Se Y \\sim t_n , ent\u00e3o Y^2 \\sim F_{1,n} . Existem diversas rela\u00e7\u00f5es que s\u00e3o encontradas com outras distribui\u00e7\u00f5es. Confira aqui E[X] = \\frac{n}{n-2}, n > 2 Var[X] = \\frac{2n^2(m + n - 2)}{m(n-2)^2(n-4)} import numpy as np from scipy.stats import f import matplotlib.pyplot as plt Vamos ver como \u00e9 a cara dessa distribui\u00e7\u00e3o: m , n = 20 , 10 X = f ( dfn = m , dfd = n ) w = np . arange ( 0 , 5 , 0.1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 12 , 5 )) ax [ 0 ] . plot ( w , X . pdf ( w ), lw = 5 , color = 'orange' ) ax [ 1 ] . plot ( w , X . cdf ( w ), lw = 5 , color = 'orange' ) ax [ 0 ] . set_title ( 'PDF Distribui\u00e7\u00e3o F' ) ax [ 1 ] . set_title ( 'CDF Distribui\u00e7\u00e3o F' ) plt . show ()","title":"Distribui\u00e7\u00e3o Chi-Quadrado"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#distribuicao-chi-quadrado","text":"Para cada m positivo, a distribui\u00e7\u00e3o Gamma(m/2, 1/2) \u00e9 chamada de distribui\u00e7\u00e3o \\chi^2 . Ela foi primeiramente descrita por Helmert para computar a distribui\u00e7\u00e3o amostral de uma popula\u00e7\u00e3o normal. Vamos ver como a normal se relaciona mais a frente. f(x) = \\frac{1}{2^{m/2}\\Gamma(m/2)}x^{m/2 - 1}e^{-x/2}","title":"Distribui\u00e7\u00e3o Chi-Quadrado"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#propriedades","text":"Se X \\sim \\chi^2(m) , ent\u00e3o: E(X) = m Var(X) = 2m \\psi(t) = \\left(\\frac{1}{1-2t}\\right)^{m/2}, t < \\frac{1}{2}","title":"Propriedades"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#soma-de-chi2","text":"Se X_1, ..., X_k s\u00e3o independentes e cada uma tem grau de liberdade m_i , ent\u00e3o X_1 + ... + X_n tem distribui\u00e7\u00e3o \\chi^2(m_1 + .... + m_k)","title":"Soma de \\chi^2"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#relacao-com-a-normal","text":"Se X tem distribui\u00e7\u00e3o normal padr\u00e3o, Y = X^2 \\sim \\chi^2(1) De fato, se juntarmos as \u00faltimos dois teoremas, veremos que a soma de quadrados de normais independentes e identicamente distribuidas ser\u00e1 \\chi^2(m) , onde m \u00e9 o n\u00famero de parcelas.","title":"Rela\u00e7\u00e3o com a Normal"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#implementacao","text":"import numpy as np import matplotlib.pyplot as plt from scipy.stats import chi2 from matplotlib import animation , cm from IPython.display import HTML # Random Object ro = np . random . default_rng ( 1000 ) # Para assegurar reprodutibilidade degree_freedom = 10 mean , var , skew , kurt = chi2 . stats ( degree_freedom , moments = 'mvsk' ) print ( 'Propriedades' ) print ( 'M\u00e9dia: {} ' . format ( mean )) print ( 'Var: {} ' . format ( var )) print ( 'Assimetria: {} ' . format ( skew )) print ( 'Curtose: {} ' . format ( kurt )) Propriedades M\u00e9dia: 10.0 Var: 20.0 Assimetria: 0.8944271909999159 Curtose: 1.2 fig , ax = plt . subplots ( 1 , 1 ) x = np . linspace ( chi2 . ppf ( 0.01 , degree_freedom ), chi2 . ppf ( 0.99 , degree_freedom ), 100 ) ax . plot ( x , chi2 . pdf ( x , degree_freedom ), 'r-' , lw = 5 , alpha = 0.6 , label = 'chi2 pdf' ) r = chi2 . rvs ( degree_freedom , size = 10000 ) ax . hist ( r , density = True , alpha = 0.2 ) ax . legend () plt . show () fig , ax = plt . subplots () line , = ax . plot ( x , chi2 . pdf ( x , degree_freedom ), 'r-' , lw = 5 , alpha = 0.6 ) ax . set_xlim (( 0 , 150 )) ax . set_title ( 'Chi-Square' ) def animate ( i , degree_freedom ): x = np . linspace ( 0 , chi2 . ppf ( 0.99 , degree_freedom + i ), 100 ) line . set_data ( x , chi2 . pdf ( x , degree_freedom + i )) return line , anim = animation . FuncAnimation ( fig , animate , frames = 100 , interval = 50 , fargs = ( degree_freedom ,), repeat = False ) HTML ( anim . to_html5_video ()) Your browser does not support the video tag.","title":"Implementa\u00e7\u00e3o"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#distribuicao-conjunta-da-media-e-variancia-amostrais","text":"X_1,...,X_n formam uma amostra aleat\u00f3ria com distribui\u00e7\u00e3o normal e com m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 desconhecidos. Estamos interessados na distribui\u00e7\u00e3o conjunta dos estimadores de m\u00e1xima verossimilhan\u00e7a para m\u00e9dia e vari\u00e2ncia da amostra.","title":"Distribui\u00e7\u00e3o Conjunta da m\u00e9dia e vari\u00e2ncia amostrais"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#teorema-de-basu","text":"Sejam \\hat{\\mu} = \\bar{X}_n e \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2 a m\u00e9dia e vari\u00e2ncia amostrais, respectivamente. Ent\u00e3o \\hat{\\mu} tem distribui\u00e7\u00e3o normal com m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 /n , enquanto n\\hat{\\sigma}^2/\\sigma^2 tem a distribui\u00e7\u00e3o \\chi^2(n-1) , isto \u00e9, com n-1 grau de liberdade. Al\u00e9m disso elas s\u00e3o independentes. Esse teorema \u00e9 um pouco mais complexo e, na verdade, essa seria uma esp\u00e9cie de aplica\u00e7\u00e3o do teorema, na verdade. O teorema de Basu diz que: Se T \u00e9 uma estat\u00edstica suficiente completa (Considere, nesse teorema, g uma fun\u00e7\u00e3o integr\u00e1vel limitada) para \\theta e A uma estat\u00edstica ancillary, ent\u00e3o T \u00e9 independente de A . Nesse caso \\hat{\\mu} \u00e9 completa suficiente e \\hat{\\sigma}^2 \u00e9 ancillary, por que n\u00e3o depende de \\mu . O mais interessante \u00e9 que essa propriedade \u00e9 s\u00f3 vista com a distribui\u00e7\u00e3o normal ! Olhem a p\u00e1gina 9.","title":"Teorema de Basu"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#demonstracao","text":"O livro tem uma abordagem um pouco mais voltado \u00e0 \u00c1lgebra Linear. Aqui vou mostrar uma ideia um pouco diferente, onde voc\u00eas podem demonstrar os passos, como exerc\u00edcio. Passo 1: \\sum_{i=1}^n X_i^2 = n\\hat{\\sigma}^2 + n\\hat{\\mu}^2 Dica: Escrever \\hat{\\sigma}^2 e abrir em tr\u00eas somat\u00f3rios. Passo 2: \\sum_{i=1}^n (X_i - \\mu)^2 = n\\hat{\\sigma}^2 + n(\\hat{\\mu} - \\mu)^2 Dica: O Passo 1 \u00e9 um caso especial do Passo 2. O processo \u00e9 o mesmo. Passo 3: \\hat{\\mu} \u00e9 independente de X_i - \\hat{\\mu}, i = 1,...,n . Dica: Montar a pdf conjunta de X_1, ..., X_n (j\u00e1 fizemos isso atrave\u015b da verossimilhan\u00e7a) e fazer uma mudan\u00e7a de vari\u00e1vel Y_1 = \\hat{\\mu}, Y_2 = X_2 - \\hat{\\mu}, ..., Y_n = X_n - \\hat{\\mu} . Com essa mudan\u00e7a, \u00e9 poss\u00edvel montar a pdf como fun\u00e7\u00e3o de y_1,...,y_n . Esse processo \u00e9 um pouco mais chato, mas \u00e9 bom lembrar como fazez mudan\u00e7a de vari\u00e1vel para pdfs. Aqui voc\u00ea pode conferir como . \u00c9 importante lembrar que \u00e9 uma fun\u00e7\u00e3o de y ap\u00f3s transformada e n\u00e3o de x . Dica 2: Fatorizar a pdf conjunta. Voc\u00ea vai ver como se destaca a independ\u00eancia aqui. Passo 4: Mostrar que \\hat{\\mu} e \\hat{\\sigma}^2 s\u00e3o independentes.","title":"Demonstra\u00e7\u00e3o"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#referencias","text":"1 2","title":"Refer\u00eancias"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#simples-visualizacao","text":"Eu gostaria de comparar o que acontece com a m\u00e9dia e vari\u00e2ncia amostral da distribui\u00e7\u00e3o normal e da distribui\u00e7\u00e3o gamma. Para isso, geero amostras de tamanho n , calculo as estat\u00edsticas e salvo. Fa\u00e7o esse procedimento o n\u00famero de pontos que quiser. ite = 10000 n = 10000 # Par\u00e2metros da Normal mu = 5 sigma = 2 # Par\u00e2metros da Gamma alpha = 5 beta = 4 means = np . zeros (( ite , 2 )) variances = np . zeros (( ite , 2 )) for i in range ( ite ): X = ro . normal ( loc = mu , scale = sigma , size = n ) Y = ro . gamma ( shape = alpha , scale = 1 / beta , size = n ) means [ i , 0 ] = np . mean ( X ) means [ i , 1 ] = np . mean ( Y ) variances [ i , 0 ] = np . var ( X , ddof = 0 ) variances [ i , 1 ] = np . var ( Y , ddof = 0 ) coef_normal = np . polyfit ( x = means [:, 0 ], y = variances [:, 0 ], deg = 1 ) coef_gamma = np . polyfit ( x = means [:, 1 ], y = variances [:, 1 ], deg = 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 14 , 5 )) fig . suptitle ( 'Comparando m\u00e9dia e vari\u00e2ncia amostral' ) ax [ 0 ] . scatter ( means [:, 0 ], variances [:, 0 ]) ax [ 1 ] . scatter ( means [:, 1 ], variances [:, 1 ]) ax [ 0 ] . plot ( means [:, 0 ], coef_normal [ 0 ] * means [:, 0 ] + coef_normal [ 1 ], color = 'red' ) ax [ 1 ] . plot ( means [:, 1 ], coef_gamma [ 0 ] * means [:, 1 ] + coef_gamma [ 1 ], color = 'red' ) ax [ 0 ] . set_xlabel ( r '$\\bar {X} _n$' , fontsize = 18 ) ax [ 1 ] . set_xlabel ( r '$\\bar {X} _n$' , fontsize = 18 ) ax [ 0 ] . set_ylabel ( r '$\\sum (X_i - \\bar {X} _n)^2$' , fontsize = 18 ) ax [ 1 ] . set_ylabel ( r '$\\sum (X_i - \\bar {X} _n)^2$' , fontsize = 18 ) ax [ 0 ] . set_title ( 'Distribui\u00e7\u00e3o Normal' ) ax [ 1 ] . set_title ( 'Distribui\u00e7\u00e3o Gamma' ) ax [ 0 ] . grid ( alpha = 0.5 , linestyle = '--' ) ax [ 1 ] . grid ( alpha = 0.5 , linestyle = '--' ) plt . show () Obs: A n\u00e3o inclina\u00e7\u00e3o da reta n\u00e3o significa que existe independ\u00eancia, mas como s\u00e3o independentes, a gente espera que a inclina\u00e7\u00e3o seja pequena.","title":"Simples visualiza\u00e7\u00e3o"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#distribuicoes-t-student","text":"Artigo original : Olhe a p\u00e1gina 9!","title":"Distribui\u00e7\u00f5es T Student"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#definicao","text":"Sejam Y \\sim \\chi^2(m) e Z \\sim N(0,1) independentes. Ent\u00e3o X = \\frac{Z}{\\left(\\frac{Y}{m}\\right)^{1/2}} \\sim t(m) onde t(m) \u00e9 a distribui\u00e7\u00e3o t-student com m graus de liberdade.","title":"Defini\u00e7\u00e3o"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#funcao-densidade-de-probabilidade","text":"Para escrever essa fun\u00e7\u00e3o de probabilidade, defina X como acima e W = Y . J\u00e1 sabemos a distribui\u00e7\u00e3o conjunta de Y e Z , pois eles s\u00e3o independentes. Com essa mudan\u00e7a de vari\u00e1vel ( confira aqui se n\u00e3o lembra como \u00e9 feito ), voc\u00ea conseque escrever a distribui\u00e7\u00e3o conjunta de X e W . Depois, basta calcular a distribui\u00e7\u00e3o marginal de X , integrando em W . f(x) = \\frac{\\Gamma\\left(\\frac{m+1}{2}\\right)}{(m\\pi)^{1/2}\\Gamma\\left(\\frac{m}{2}\\right)}\\left(1 + \\frac{x^2}{m} \\right)^{-(m+1)/2}, x \\in \\mathbb{R}, onde \\Gamma \u00e9 a fun\u00e7\u00e3o Gamma , tal que, n \\in \\mathbb{N}, \\Gamma(n) = (n-1)! \\Gamma(z+1) = z\\Gamma(z) \\Gamma(1/2) = \\sqrt{\\pi} Quando m \\leq 1 , a m\u00e9dia \u00e9 divergente. Isso pode ser vizualizado pelo expoente que ser\u00e1 \\leq -1 , o que diverge (lembre de \\int 1/x ). Quando m > 1 , a m\u00e9dia existe e \u00e9 0 pela simetria da distribui\u00e7\u00e3o. Em particular, podemos mostrar que se k < m , E[|X^k|] < + \\infty e se k \\geq m , o momento diverge. Se X \\sim t(m), m > 2 , Var(X) = \\frac{m}{m-2}","title":"Fun\u00e7\u00e3o densidade de probabilidade"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#teorema","text":"Seja X_1, ..., X_n \\overset{iid}{\\sim} N(\\mu,\\sigma^2) . Seja \\sigma ' = \\left[\\frac{\\sum_{i=1}^n (X_i - \\bar{X}_n)^2}{n-1}\\right]^{1/2} Ent\u00e3o n^{1/2}(\\bar{X}_n - \\mu)/\\sigma ' \\sim t(n-1)","title":"Teorema"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#relacao-com-a-normal-e-cauchy","text":"Da mesma forma que a distribui\u00e7\u00e3o normal e a distribui\u00e7\u00e3o Cauchy, a distribui\u00e7\u00e3o t \u00e9 centrada em 0 e tem sua moda nesse valor. Entretanto a cauda a distribui\u00e7\u00e3o t (quando x \\to -\\infty ou x \\to +\\infty ), \u00e9 mais pesada, no sentido de que tende para 0 em uma velocidade menor do que a normal. Outra coisa interessante \u00e9 que a ditrivui\u00e7\u00e3o t(1) \u00e9 a distribui\u00e7\u00e3o Cauchy . Al\u00e9m disso, quando n \\to \\infty , converge para a pdf da normal padr\u00e3o ( Normal(0,1) ).","title":"Rela\u00e7\u00e3o com a Normal e Cauchy"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#ferramentas-para-demonstrar-a-convergencia","text":"Teorema de Slutsky : Considere o corol\u00e1rio com f(x,y) = \\frac{x}{y} Lei dos Grandes N\u00fameros : Escreva a qui-quadrado como soma de normais. from scipy.stats import t , norm , cauchy","title":"Ferramentas para demonstrar a converg\u00eancia"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#implementacao_1","text":"Primeiro vamos ver a cara da distribui\u00e7\u00e3o t m = 10 X = t ( df = m ) w = np . arange ( - 3 , 3 , 0.1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 12 , 5 )) ax [ 0 ] . plot ( w , X . pdf ( w ), lw = 5 , color = 'orange' ) ax [ 1 ] . plot ( w , X . cdf ( w ), lw = 5 , color = 'orange' ) ax [ 0 ] . set_title ( 'PDF t-Student' ) ax [ 1 ] . set_title ( 'CDF t-Student' ) plt . show () Vamos ver o que acontece quando m \\leq 1 ? ite = 1000 n = 10000 m1 = 10 m2 = 0.5 means = np . zeros (( ite , 2 )) for i in range ( ite ): X = ro . standard_t ( df = m1 , size = n ) Y = ro . standard_t ( df = m2 , size = n ) means [ i , 0 ] = np . mean ( X ) means [ i , 1 ] = np . mean ( Y ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 14 , 5 )) ax [ 0 ] . hist ( means [:, 0 ], bins = 100 ) ax [ 1 ] . hist ( np . log ( means [:, 1 ]), bins = 10 ) ax [ 0 ] . set_xlabel ( 'E[X]' ) ax [ 1 ] . set_xlabel ( 'log E[X]' ) ax [ 0 ] . set_title ( 'm = 10' ) ax [ 1 ] . set_title ( 'm = 0.5' ) plt . show () <ipython-input-11-2bfd1961d53b>:3: RuntimeWarning: invalid value encountered in log ax[1].hist(np.log(means[:,1]), bins = 10) No eixo x do segundo gr\u00e1fico plotei o logaritmo, dado que alguns resultados eram extremamente grandes! Isso indica visualmente que a m\u00e9dia diverge!","title":"Implementa\u00e7\u00e3o"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#relacao-com-a-normal-e-com-cauchy","text":"C = cauchy () Z = norm ( loc = 0 , scale = 1 ) T = t ( df = 1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 14 , 5 )) ax [ 0 ] . plot ( w , C . pdf ( w ), label = 'Cauchy' ) ax [ 0 ] . scatter ( w , T . pdf ( w ), c = 'red' , marker = \"*\" , label = 't-Student' ) ax [ 0 ] . legend () ax [ 0 ] . set_title ( 't-Student e Cauchy quando m = 1' ) ax [ 1 ] . plot ( w , Z . pdf ( w ), label = 'N(0,1)' ) ax [ 1 ] . set_title ( 'Converg\u00eancia da t para a normal' ) for i in np . logspace ( np . log10 ( 1 ), np . log10 ( 20 ), 5 ): T = t ( df = int ( i )) ax [ 1 ] . plot ( w , T . pdf ( w ), linestyle = '--' , alpha = i / 40 + 0.5 , color = 'grey' , label = 't( {} )' . format ( int ( i ))) ax [ 1 ] . legend ( loc = 'upper right' ) plt . show ()","title":"Rela\u00e7\u00e3o com a Normal e com Cauchy"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#distribuicao-f","text":"Sejam Y \\sim \\chi^2_m e W \\sim \\chi^2_n independentes. Defina X = \\frac{Y/m}{W/n} = \\frac{nY}{mW} Dizemos que X tem distribui\u00e7\u00e3o F . A sua motiva\u00e7\u00e3o vem do teste de hip\u00f3teses que compara vari\u00e2ncias de duas normais.","title":"Distribui\u00e7\u00e3o F"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#funcao-de-densidade-de-probabilidade","text":"Seja X \\sim F_{m,n} . Ent\u00e3o sua pdf tem suporte em x > 0 e pe definida f(x) = \\frac{\\Gamma\\left[\\frac{1}{2}(m+n)\\right]m^{m/2}n^{n/2}}{\\Gamma\\left(\\frac{1}{2}m\\right)\\Gamma\\left(\\frac{1}{2}n\\right)}\\cdot \\frac{x^{(m/2) - 1}}{(mx + n)^{(m+n)/2)}} Observe que ela n\u00e3o \u00e9 sim\u00e9trica em m e n . Assim, se trocarmos eles de lugar, teremos um resultado diferente.","title":"Fun\u00e7\u00e3o de densidade de probabilidade"},{"location":"infestatistica/SamplingDistribution/SamplingDistribution/#propriedades_1","text":"Seja X \\sim F_{m,n} . Ent\u00e3o 1/X \\sim F_{n,m} . Se Y \\sim t_n , ent\u00e3o Y^2 \\sim F_{1,n} . Existem diversas rela\u00e7\u00f5es que s\u00e3o encontradas com outras distribui\u00e7\u00f5es. Confira aqui E[X] = \\frac{n}{n-2}, n > 2 Var[X] = \\frac{2n^2(m + n - 2)}{m(n-2)^2(n-4)} import numpy as np from scipy.stats import f import matplotlib.pyplot as plt Vamos ver como \u00e9 a cara dessa distribui\u00e7\u00e3o: m , n = 20 , 10 X = f ( dfn = m , dfd = n ) w = np . arange ( 0 , 5 , 0.1 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 12 , 5 )) ax [ 0 ] . plot ( w , X . pdf ( w ), lw = 5 , color = 'orange' ) ax [ 1 ] . plot ( w , X . cdf ( w ), lw = 5 , color = 'orange' ) ax [ 0 ] . set_title ( 'PDF Distribui\u00e7\u00e3o F' ) ax [ 1 ] . set_title ( 'CDF Distribui\u00e7\u00e3o F' ) plt . show ()","title":"Propriedades"},{"location":"infestatistica/StatisticalInference/StatisticalInference/","text":"Infer\u00eancia Estat\u00edstica Procedimento que objetiva produzir uma proposi\u00e7\u00e3o probabil\u00edsitca sobre um modelo estat\u00edstico. Modelo Estat\u00edstico Identificar vari\u00e1veis aleat\u00f3rias de interesse, especificar uma distribui\u00e7\u00e3o conjunta (ou fam\u00edlia), par\u00e2metros relevantes e uma especifica\u00e7\u00e3o para uma distribui\u00e7\u00e3o para os par\u00e2metros desconhecidos (baysianos adoram essa parte, p \\sim N(0,1) ) Espa\u00e7o dos Par\u00e2metros Uma caracter\u00edstica ou uma combina\u00e7\u00e3o de caracter\u00edsticas para determinar uma distribui\u00e7\u00e3o conjunta para as vari\u00e1veis aleat\u00f3rias forma o par\u00e2metro, que pertence a um espa\u00e7o denominado \\Omega . Estat\u00edstica Fun\u00e7\u00e3o das vari\u00e1veis aleat\u00f3rias observ\u00e1veis Problemas estudados Predi\u00e7\u00e3o: Baseado na \u00e9poca do ano que estamos, fatores climatol\u00f3gicos dos \u00faltimos dias, entre outros fatores, qual a probabilidade de chuva amanh\u00e3? Problemas de decis\u00e3o estat\u00edstica: \u00c9 relacionado ao risco e teste de hip\u00f3teses. Resposta consider\u00e1vel Desenho de experimentos: um psic\u00f3logo quer inferir qu\u00e3o avesso ao risco \u00e9 uma determinada popula\u00e7\u00e3o. Ele pode determinar, desenhar o experimento para isso. Infer\u00eancia Estat\u00edstica com Python import numpy as np import pandas as pd from scipy.stats import poisson import matplotlib.pyplot as plt import seaborn as sns sns . set () Importando os Dados Este banco de dados inclui um registro para cada vazamento ou derramamento de oleoduto relatado \u00e0 Administra\u00e7\u00e3o de Seguran\u00e7a de Dutos e Materiais Perigosos desde 2010. Esses registros incluem a data e hora do incidente, operador e oleoduto, causa do incidente, tipo de l\u00edquido perigoso e quantidade perdida, ferimentos e fatalidades e custos associados. oil_accident_df = pd . read_csv ( '../data/oil_pipeline.csv' ) oil_accident_df . sample () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Report Number Supplemental Number Accident Year Accident Date/Time Operator ID Operator Name Pipeline/Facility Name Pipeline Location Pipeline Type Liquid Type ... Other Fatalities Public Fatalities All Fatalities Property Damage Costs Lost Commodity Costs Public/Private Property Damage Costs Emergency Response Costs Environmental Remediation Costs Other Costs All Costs 871 20120202 17135 2012 6/15/2012 3:50 PM 31476 ROSE ROCK MIDSTREAM L.P. BURKETT DISCHARGE ONSHORE UNDERGROUND CRUDE OIL ... NaN NaN NaN 6020.0 200.0 2500.0 10500.0 8500.0 16000.0 43720 1 rows \u00d7 48 columns cols_of_interest = [ 'Accident Date/Time' , 'Accident State' , 'Pipeline Location' , 'Liquid Type' , 'Net Loss (Barrels)' , 'All Costs' ] data = oil_accident_df [ cols_of_interest ] data [ 'All Costs' ] = data [ 'All Costs' ] / 1000000 # unidade em milh\u00e3o. data . sample () /home/lucasmoschen/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy after removing the cwd from sys.path. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Accident Date/Time Accident State Pipeline Location Liquid Type Net Loss (Barrels) All Costs 263 10/11/2010 4:10 PM NJ ONSHORE REFINED AND/OR PETROLEUM PRODUCT (NON-HVL), LI... 0.0 0.0 Vamos entender um pouco como esta informa\u00e7\u00e3o esta organizada. data . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Net Loss (Barrels) All Costs count 2795.000000 2795.000000 mean 132.194050 0.834033 std 1185.019252 16.578298 min 0.000000 0.000000 25% 0.000000 0.005040 50% 0.000000 0.023129 75% 2.000000 0.117232 max 30565.000000 840.526118 Vamos analisar os dados utilizando leis da probabilidade para aprender sobre a popula\u00e7\u00e3o. Veja que n\u00e3o temos a informa\u00e7\u00e3o completa, apenas a partir de 2010. fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 4 )) sns . boxplot ( data [ 'All Costs' ], data = data , ax = ax [ 0 ]) ax [ 0 ] . set_title ( 'Custos dos Acidentes por Milh\u00e3o US$' ) sns . boxplot ( data [ 'Net Loss (Barrels)' ], data = data , ax = ax [ 1 ]) ax [ 1 ] . set_title ( 'Preju\u00edzo L\u00edquido (Barris)' ) plt . show () Mas esse n\u00e3o era para ser um boxplot? Cade a caixa? Isso indica que valores grandes nos dois dados s\u00e3o muito maiores relativamente aos outros dados. Poder\u00edamos prever o custo de um acidente usando a mediana dos valores? \u00c9 de fato um modelo, mas nesse caso, parece ser ruim dado os valores grandes. O que s\u00e3o esses valores grandes, afinal? Em alguns casos, podem realmente apresentar erros, mas nesse caso fica dif\u00edcil de afirmar. Bom. Podemos, dados esses problemas, trabalhar com outra vari\u00e1vel dispon\u00edvel: o tempo do acidente. Conhecemos uma fam\u00edlia de distribui\u00e7\u00f5es de probabilidade que modela frequ\u00eancia de acidentes em um intervalo de tempo? Distribui\u00e7\u00e3o de Poisson: probabilidade de uma s\u00e9rie de eventos ocorrer num certo per\u00edodo de tempo se estes eventos ocorrem independentemente de quando ocorreu o \u00faltimo evento. De forma geral, podemos dizer que isso \u00e9 verdade para acidentes de \u00f3leo. Assim, temos uma vari\u00e1vel aleat\u00f3ria de interesser X , que indica o n\u00famero de acidentes, j\u00e1 temos uma distribui\u00e7\u00e3o para essa vari\u00e1vel (Poisson) e j\u00e1 temos o par\u00e2metro \\lambda desconhecido. data [ 'Accident Date/Time' ] = pd . to_datetime ( data [ 'Accident Date/Time' ]) totaltimespan = np . max ( data [ 'Accident Date/Time' ]) - np . min ( data [ 'Accident Date/Time' ]) totaltime_hour = ( totaltimespan . days * 24 + totaltimespan . seconds / ( 3600 )) totaltime_month = ( totaltimespan . days + totaltimespan . seconds / ( 3600 * 24 )) * 12 / 365 lmda_h = len ( data ) / totaltime_hour lmda_m = len ( data ) / totaltime_month print ( 'N\u00famero estimado de acidentes por hora: {} ' . format ( lmda_h )) print ( 'N\u00famero estimado de acidentes por m\u00eas {} ' . format ( lmda_m )) N\u00famero estimado de acidentes por hora: 0.04540255169379675 N\u00famero estimado de acidentes por m\u00eas 33.14386273647162 /home/lucasmoschen/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy \"\"\"Entry point for launching an IPython kernel. Poder\u00edamos ter procedimentos para estimar \\lambda , mas por hora, vamos tomar ele como a m\u00e9dia das observa\u00e7\u00f5es. Pela Lei dos Grandes N\u00fameros, sabemos que a m\u00e9dia da Poisson \u00e9 \\lambda e a m\u00e9dia amostral tende para ela. lamda = 33 X = poisson ( lamda ) I = np . arange ( 0 , 60 , 1 ) #intervalo(0,60), passo = 1 samples_poisson = np . sort ( np . random . poisson ( lamda , 10000 )) Y = X . cdf ( samples_poisson ) #fun\u00e7\u00e3o de densidade acumulada fig , ax = plt . subplots ( 1 , 2 , figsize = ( 20 , 8 )) ax [ 0 ] . scatter ( I , X . pmf ( I ) , color = 'purple' ) ax [ 0 ] . set_xlabel ( 'N\u00famero de Acidentes por m\u00eas (n)' ) ax [ 0 ] . set_ylabel ( 'P(X <= n)' ) ax [ 0 ] . set_title ( 'Fun\u00e7\u00e3o de Massa de Probabilidade' ) ax [ 1 ] . scatter ( samples_poisson , Y , color = 'purple' ) ax [ 1 ] . hlines ( 0.5 , xmin = min ( samples_poisson ), xmax = max ( samples_poisson ), linestyle = '--' , color = 'black' ) ax [ 1 ] . set_xlabel ( 'N\u00famero de acidentes por m\u00eas (n)' ) ax [ 1 ] . set_ylabel ( 'P(X <= n)' ) ax [ 1 ] . set_title ( 'Fun\u00e7\u00e3o de Distribui\u00e7\u00e3o Acumulada' ) plt . show () A partir de nosso modelo, j\u00e1 podemos fazer acerta\u00e7\u00f5es probabil\u00edstica! real_data = np . array ( data [ 'Accident Date/Time' ] . apply ( lambda x : ( x . year , x . month ))) accidents_count = { 2010 + i : { m : 0 for m in range ( 1 , 13 )} for i in range ( 8 )} for info in real_data : accidents_count [ info [ 0 ]][ info [ 1 ]] += 1 distribution = [ accidents_count [ y ][ m ] for y in accidents_count . keys () for m in accidents_count [ y ] . keys ()] distribution = distribution [: - 12 ] #Tirando 2 observa\u00e7\u00f5es de 2017 fig , ax = plt . subplots () sns . distplot ( distribution , bins = 15 , ax = ax , label = 'Original data' , kde = False , norm_hist = True ) ax . scatter ( I , X . pmf ( I ) , color = 'purple' , label = 'Nosso modelo' ) ax . legend () ax . set_title ( 'Comparando modelo com dados reais' ) plt . show ()","title":"Infer\u00eancia Estat\u00edstica"},{"location":"infestatistica/StatisticalInference/StatisticalInference/#inferencia-estatistica","text":"Procedimento que objetiva produzir uma proposi\u00e7\u00e3o probabil\u00edsitca sobre um modelo estat\u00edstico.","title":"Infer\u00eancia Estat\u00edstica"},{"location":"infestatistica/StatisticalInference/StatisticalInference/#modelo-estatistico","text":"Identificar vari\u00e1veis aleat\u00f3rias de interesse, especificar uma distribui\u00e7\u00e3o conjunta (ou fam\u00edlia), par\u00e2metros relevantes e uma especifica\u00e7\u00e3o para uma distribui\u00e7\u00e3o para os par\u00e2metros desconhecidos (baysianos adoram essa parte, p \\sim N(0,1) )","title":"Modelo Estat\u00edstico"},{"location":"infestatistica/StatisticalInference/StatisticalInference/#espaco-dos-parametros","text":"Uma caracter\u00edstica ou uma combina\u00e7\u00e3o de caracter\u00edsticas para determinar uma distribui\u00e7\u00e3o conjunta para as vari\u00e1veis aleat\u00f3rias forma o par\u00e2metro, que pertence a um espa\u00e7o denominado \\Omega .","title":"Espa\u00e7o dos Par\u00e2metros"},{"location":"infestatistica/StatisticalInference/StatisticalInference/#estatistica","text":"Fun\u00e7\u00e3o das vari\u00e1veis aleat\u00f3rias observ\u00e1veis","title":"Estat\u00edstica"},{"location":"infestatistica/StatisticalInference/StatisticalInference/#problemas-estudados","text":"Predi\u00e7\u00e3o: Baseado na \u00e9poca do ano que estamos, fatores climatol\u00f3gicos dos \u00faltimos dias, entre outros fatores, qual a probabilidade de chuva amanh\u00e3? Problemas de decis\u00e3o estat\u00edstica: \u00c9 relacionado ao risco e teste de hip\u00f3teses. Resposta consider\u00e1vel Desenho de experimentos: um psic\u00f3logo quer inferir qu\u00e3o avesso ao risco \u00e9 uma determinada popula\u00e7\u00e3o. Ele pode determinar, desenhar o experimento para isso. Infer\u00eancia Estat\u00edstica com Python import numpy as np import pandas as pd from scipy.stats import poisson import matplotlib.pyplot as plt import seaborn as sns sns . set ()","title":"Problemas estudados"},{"location":"infestatistica/StatisticalInference/StatisticalInference/#importando-os-dados","text":"Este banco de dados inclui um registro para cada vazamento ou derramamento de oleoduto relatado \u00e0 Administra\u00e7\u00e3o de Seguran\u00e7a de Dutos e Materiais Perigosos desde 2010. Esses registros incluem a data e hora do incidente, operador e oleoduto, causa do incidente, tipo de l\u00edquido perigoso e quantidade perdida, ferimentos e fatalidades e custos associados. oil_accident_df = pd . read_csv ( '../data/oil_pipeline.csv' ) oil_accident_df . sample () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Report Number Supplemental Number Accident Year Accident Date/Time Operator ID Operator Name Pipeline/Facility Name Pipeline Location Pipeline Type Liquid Type ... Other Fatalities Public Fatalities All Fatalities Property Damage Costs Lost Commodity Costs Public/Private Property Damage Costs Emergency Response Costs Environmental Remediation Costs Other Costs All Costs 871 20120202 17135 2012 6/15/2012 3:50 PM 31476 ROSE ROCK MIDSTREAM L.P. BURKETT DISCHARGE ONSHORE UNDERGROUND CRUDE OIL ... NaN NaN NaN 6020.0 200.0 2500.0 10500.0 8500.0 16000.0 43720 1 rows \u00d7 48 columns cols_of_interest = [ 'Accident Date/Time' , 'Accident State' , 'Pipeline Location' , 'Liquid Type' , 'Net Loss (Barrels)' , 'All Costs' ] data = oil_accident_df [ cols_of_interest ] data [ 'All Costs' ] = data [ 'All Costs' ] / 1000000 # unidade em milh\u00e3o. data . sample () /home/lucasmoschen/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy after removing the cwd from sys.path. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Accident Date/Time Accident State Pipeline Location Liquid Type Net Loss (Barrels) All Costs 263 10/11/2010 4:10 PM NJ ONSHORE REFINED AND/OR PETROLEUM PRODUCT (NON-HVL), LI... 0.0 0.0 Vamos entender um pouco como esta informa\u00e7\u00e3o esta organizada. data . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Net Loss (Barrels) All Costs count 2795.000000 2795.000000 mean 132.194050 0.834033 std 1185.019252 16.578298 min 0.000000 0.000000 25% 0.000000 0.005040 50% 0.000000 0.023129 75% 2.000000 0.117232 max 30565.000000 840.526118 Vamos analisar os dados utilizando leis da probabilidade para aprender sobre a popula\u00e7\u00e3o. Veja que n\u00e3o temos a informa\u00e7\u00e3o completa, apenas a partir de 2010. fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 4 )) sns . boxplot ( data [ 'All Costs' ], data = data , ax = ax [ 0 ]) ax [ 0 ] . set_title ( 'Custos dos Acidentes por Milh\u00e3o US$' ) sns . boxplot ( data [ 'Net Loss (Barrels)' ], data = data , ax = ax [ 1 ]) ax [ 1 ] . set_title ( 'Preju\u00edzo L\u00edquido (Barris)' ) plt . show () Mas esse n\u00e3o era para ser um boxplot? Cade a caixa? Isso indica que valores grandes nos dois dados s\u00e3o muito maiores relativamente aos outros dados. Poder\u00edamos prever o custo de um acidente usando a mediana dos valores? \u00c9 de fato um modelo, mas nesse caso, parece ser ruim dado os valores grandes. O que s\u00e3o esses valores grandes, afinal? Em alguns casos, podem realmente apresentar erros, mas nesse caso fica dif\u00edcil de afirmar. Bom. Podemos, dados esses problemas, trabalhar com outra vari\u00e1vel dispon\u00edvel: o tempo do acidente. Conhecemos uma fam\u00edlia de distribui\u00e7\u00f5es de probabilidade que modela frequ\u00eancia de acidentes em um intervalo de tempo? Distribui\u00e7\u00e3o de Poisson: probabilidade de uma s\u00e9rie de eventos ocorrer num certo per\u00edodo de tempo se estes eventos ocorrem independentemente de quando ocorreu o \u00faltimo evento. De forma geral, podemos dizer que isso \u00e9 verdade para acidentes de \u00f3leo. Assim, temos uma vari\u00e1vel aleat\u00f3ria de interesser X , que indica o n\u00famero de acidentes, j\u00e1 temos uma distribui\u00e7\u00e3o para essa vari\u00e1vel (Poisson) e j\u00e1 temos o par\u00e2metro \\lambda desconhecido. data [ 'Accident Date/Time' ] = pd . to_datetime ( data [ 'Accident Date/Time' ]) totaltimespan = np . max ( data [ 'Accident Date/Time' ]) - np . min ( data [ 'Accident Date/Time' ]) totaltime_hour = ( totaltimespan . days * 24 + totaltimespan . seconds / ( 3600 )) totaltime_month = ( totaltimespan . days + totaltimespan . seconds / ( 3600 * 24 )) * 12 / 365 lmda_h = len ( data ) / totaltime_hour lmda_m = len ( data ) / totaltime_month print ( 'N\u00famero estimado de acidentes por hora: {} ' . format ( lmda_h )) print ( 'N\u00famero estimado de acidentes por m\u00eas {} ' . format ( lmda_m )) N\u00famero estimado de acidentes por hora: 0.04540255169379675 N\u00famero estimado de acidentes por m\u00eas 33.14386273647162 /home/lucasmoschen/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy \"\"\"Entry point for launching an IPython kernel. Poder\u00edamos ter procedimentos para estimar \\lambda , mas por hora, vamos tomar ele como a m\u00e9dia das observa\u00e7\u00f5es. Pela Lei dos Grandes N\u00fameros, sabemos que a m\u00e9dia da Poisson \u00e9 \\lambda e a m\u00e9dia amostral tende para ela. lamda = 33 X = poisson ( lamda ) I = np . arange ( 0 , 60 , 1 ) #intervalo(0,60), passo = 1 samples_poisson = np . sort ( np . random . poisson ( lamda , 10000 )) Y = X . cdf ( samples_poisson ) #fun\u00e7\u00e3o de densidade acumulada fig , ax = plt . subplots ( 1 , 2 , figsize = ( 20 , 8 )) ax [ 0 ] . scatter ( I , X . pmf ( I ) , color = 'purple' ) ax [ 0 ] . set_xlabel ( 'N\u00famero de Acidentes por m\u00eas (n)' ) ax [ 0 ] . set_ylabel ( 'P(X <= n)' ) ax [ 0 ] . set_title ( 'Fun\u00e7\u00e3o de Massa de Probabilidade' ) ax [ 1 ] . scatter ( samples_poisson , Y , color = 'purple' ) ax [ 1 ] . hlines ( 0.5 , xmin = min ( samples_poisson ), xmax = max ( samples_poisson ), linestyle = '--' , color = 'black' ) ax [ 1 ] . set_xlabel ( 'N\u00famero de acidentes por m\u00eas (n)' ) ax [ 1 ] . set_ylabel ( 'P(X <= n)' ) ax [ 1 ] . set_title ( 'Fun\u00e7\u00e3o de Distribui\u00e7\u00e3o Acumulada' ) plt . show () A partir de nosso modelo, j\u00e1 podemos fazer acerta\u00e7\u00f5es probabil\u00edstica! real_data = np . array ( data [ 'Accident Date/Time' ] . apply ( lambda x : ( x . year , x . month ))) accidents_count = { 2010 + i : { m : 0 for m in range ( 1 , 13 )} for i in range ( 8 )} for info in real_data : accidents_count [ info [ 0 ]][ info [ 1 ]] += 1 distribution = [ accidents_count [ y ][ m ] for y in accidents_count . keys () for m in accidents_count [ y ] . keys ()] distribution = distribution [: - 12 ] #Tirando 2 observa\u00e7\u00f5es de 2017 fig , ax = plt . subplots () sns . distplot ( distribution , bins = 15 , ax = ax , label = 'Original data' , kde = False , norm_hist = True ) ax . scatter ( I , X . pmf ( I ) , color = 'purple' , label = 'Nosso modelo' ) ax . legend () ax . set_title ( 'Comparando modelo com dados reais' ) plt . show ()","title":"Importando os Dados"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/","text":"Teste de Hip\u00f3teses: Defini\u00e7\u00f5es Hip\u00f3tese Nula e Alternativa Regi\u00e3o Cr\u00edtica Estat\u00edstica de Teste Fun\u00e7\u00e3o de Poder Tipos de Erro N\u00edvel e tamanho do teste P-valor Temos um problema estat\u00edstico que envolve um par\u00e2metro \\theta tal que tenha valor desconhecido, mas reside em um espa\u00e7o \\Omega . Suponha que particionemos \\Omega = \\Omega_1 \\dot\\cup ~\\Omega_2 e o estat\u00edstico est\u00e1 interessado se \\theta est\u00e1 em \\Omega_0 ou est\u00e1 em \\Omega_1 . Hip\u00f3tese Nula e Alternativa Dizemos que H_0 \u00e9 a hip\u00f3tese de que \\theta \\in \\Omega_0 e chamamos H_0 de hip\u00f3tese nula , enquanto H_1 \u00e9 a hip\u00f3tese alternativa e representa \\theta \\in H_1 . Queremos decidir qual das hip\u00f3teses \u00e9 verdadeira (e s\u00f3 uma ser\u00e1, porque a parti\u00e7\u00e3o \u00e9 disjunta). Se decidimos que \\theta \\in \\Omega_1 , rejeitamos H_0 , e se \\theta \\in \\Omega_0 , n\u00e3o rejeitamos H_0 . Hip\u00f3tese Simples e Composta Suponha que X_1, ..., X_n formam uma amostra aleat\u00f3ria com pdf f(x|\\theta) . Queremos testar a hip\u00f3tese de que H_0: \\theta \\in \\Omega_0 H_1: \\theta \\in \\Omega_1 Se \\Omega_i contem apenas um valor, ent\u00e3o H_i \u00e9 dita hip\u00f3tese simples. Se cont\u00e9m mais de um valor, dizemos que \u00e9 composta. Hip\u00f3tese Unilateral e Bilateral Seja \\theta um par\u00e2metro unidimensional. Dizemos que a hip\u00f3tese H_0 \u00e9 unilateral (ou one tailed ) quando \u00e9 da forma \\theta \\leq \\theta_0 ou \\theta \\geq \\theta_0 . Ela ser\u00e1 bilateral quando \u00e9 do tipo H_0 \\neq \\theta_0 . Regi\u00e3o Cr\u00edtica Suponha que queremos testar a hip\u00f3tese de que H_0: \\theta \\in \\Omega_0 \\text{ e } H_1: \\theta \\in \\Omega_1 Quando queremos decidir qual hip\u00f3tese escolher, observamos uma amostra dessa distribui\u00e7\u00e3o no espa\u00e7o de amostras S . O dever do estat\u00edstico \u00e9 especificar um procedimento que particione o conjunto em dois subconjuntos S_0 e S_1 , onde S_1 cont\u00e9m os valores de X que rejeitam H_0 . Regi\u00e3o cr\u00edtica \u00e9 o conjunto S_1 , isto \u00e9, o conjunto de amostras que, a partir de um procedimento, rejeita H_0 . Estat\u00edstica de Teste Seja X_1, ..., X_n \\overset{iid}{\\sim} F(\\theta) . Sejam T = r(X) uma estat\u00edstica e R um subconjunto da reta. Suponha que nosso procedimento de teste \u00e9 o seguinte: Rejeitamos H_0 se T\\in R . Chamamos T de estat\u00edstica de teste e R de regi\u00e3o de rejei\u00e7\u00e3o . Dessa forma a regi\u00e3o cr\u00edtica ser\u00e1: S_1 = \\{x \\in S: r(x) \\in R\\} . Na pr\u00e1tica a maioria dos testes \u00e9 do tipo Rejeitamos H_0 se T \\geq c, c \\in \\mathbb{R} . Observa\u00e7\u00e3o sobre a divis\u00e3o de conjuntos \u00c9 importante lembrar que h\u00e1 duas diferentes divis\u00f5es: \\Omega = \\Omega_0 \\dot\\cup \\Omega_1 , que \u00e9 a divis\u00e3o do espa\u00e7o dos par\u00e2metros, e S = S_1 \\dot\\cup S_1 \u00e9 a divis\u00e3o do espa\u00e7o das amostras. Mas qual a rela\u00e7\u00e3o entre eles? Se X \\in S_1 , ent\u00e3o rejeitamos a hip\u00f3tese \\theta \\in \\Omega_0 . Al\u00e9m do mais, podemos encontrar S_1 e S_2 , mas dificilmente saberemos em qual dos conjuntos \\theta pertence. Fun\u00e7\u00e3o de Poder e Tipos de Erro Fun\u00e7\u00e3o Poder Seja \\delta um procedimento de teste (como esse assinalado acima). Se S_1 \u00e9 a regi\u00e3o cr\u00edtica, \\pi(\\theta|\\delta) = P(X \\in S_1|\\theta) = P(T \\in R|\\theta) Sendo que a \u00faltima igualdade ocorre quando o proocedimento de teste \u00e9 o citado acima. O seu significado? \u00c9 a probabilidade, para cada valor de \\theta , de que \\delta rejeita H_0 . Queremos, intuitivamente que: \\theta \\in \\Omega_0 \\rightarrow \\text{ Queremos n\u00e3o rejeitar} H_0 \\rightarrow \\pi(\\theta|\\delta) = 0 \\theta \\in \\Omega_1 \\rightarrow \\text{ Queremos rejeitar} H_0 \\rightarrow \\pi(\\theta|\\delta) = 1 Entretanto isso n\u00e3o \u00e9 em geral o que acontece. Por isso definimos: Erros I e II \\theta \\in \\Omega_0 \\theta \\in \\Omega_1 \\delta rejeita H_0 Erro Tipo I Certo \\delta n\u00e3o rejeita H_0 Certo Erro Tipo II Portanto se \\theta \\in \\Omega_0, \\pi(\\theta|\\delta) \u00e9 a probabilidade de cometermos o erro do tipo I. Se \\theta \\in \\Omega_1, 1 - \\pi(\\theta|\\delta) \u00e9 a probabilidade de cometer o erro do tipo II. N\u00edvel/Tamanho Um teste que satisfaz \\pi(\\theta|\\delta) \\leq \\alpha_0, \\forall \\theta \\in \\Omega_0 \u00e9 chamado de teste n\u00edvel \\alpha_0 , ou que o teste tem n\u00edvel de signific\u00e2ncia \\alpha_0 . O tamanho de um teste \u00e9 \\alpha(\\delta) = \\sup_{\\theta \\in \\Omega_0} \\pi(\\theta, \\delta) . Um teste ter\u00e1 n\u00edvel \\alpha_0 se, e s\u00f3 se, seu tamanho for no m\u00e1ximo \\alpha_0 . P-valor \u00c9 o menor n\u00edvel \\alpha_0 tal que rejeitar\u00edamos a hip\u00f3tese nula a n\u00edvel \\alpha_0 com os dados observados. Se rejeitamos a hip\u00f3tese nula se, e somente se, o p-valor \u00e9 no m\u00e1ximo \\alpha_0 , estamos usando um teste com n\u00edvel de signific\u00e2ncia \\alpha_0 . Equival\u00eancia entre Testes e Conjuntos de Confian\u00e7a Teorema Seja \\vec{X} = (X_1,...,X_n) \\overset{iid}{\\sim} F(\\theta) . Seja g(\\theta) , e suponha que para todo valor c na imagem de g (ou seja, c = g(x) , para algum x ), exista um teste \\delta_c de n\u00edvel \\alpha_0 para a hip\u00f3tese H_{0,c}:g(\\theta) = c, ~ H_{1,c}: g(\\theta) \\neq c Defina \\omega(x) := \\{c: \\delta_c \\text{ n\u00e3o rejeita } H_{0,c} \\text{ se } \\vec{X} = \\vec{x} \\text{ \u00e9 observado } \\} . Ent\u00e3o: P[g(\\theta_0) \\in \\omega(\\vec{X})|\\theta = \\theta_0] \\geq 1 - \\alpha_0, para todo valor \\theta \\in \\Omega . Compreens\u00e3o e Implementa\u00e7\u00e3o Teste de hip\u00f3tese \u00e9 um m\u00e9todo para que fa\u00e7amos decis\u00f5es estat\u00edsticas a partir dos dados. \u00c9 uma forma de compreender (fazer infer\u00e2ncia sobre) um par\u00e2metro. Exemplo: Belgas tem, em m\u00e9dia, maior altura do que peruanos. Exemplo 2: Temperatura n\u00e3o \u00e9 um fator relevante para o processo de cultivo de uva. Estamos avaliando afirma\u00e7\u00f5es mutualmente exclusivas, ou os belgas tem maior altura do que os peruanos, ou n\u00e3o tem! Queremos saber qual dessas afirma\u00e7\u00f5es \u00e9 suportada pelos dados que obtivermos. A hip\u00f3tese nula \u00e9 a afirma\u00e7\u00e3o a ser testada e muitas vezes estabelece uma conjectura de que as caracter\u00edsticas observadas em uma popula\u00e7\u00e3o s\u00e3o por um acaso, isto \u00e9, o fator a ser estudado \"n\u00e3o existe\". Por exemplo: o n\u00famero de voos entre Rio de Janeiro e S\u00e3o Paulo n\u00e3o tem correla\u00e7\u00e3o com o n\u00edvel do mar no Jap\u00e3o. Em geral queremos anul\u00e1-la, rejeit\u00e1-la (da\u00ed o nome). Exemplo Vamos considerar um exemplo simples utilizando a distribui\u00e7\u00e3o normal. \u00c9 a distribui\u00e7\u00e3o com c\u00e1lculos simples e uma boa visualiza\u00e7\u00e3o. A ideia nesse exemplo vai ser a seguinte: O pre\u00e7o do quilo ouro varia diariamente e essa varia\u00e7\u00e3o em unidade de d\u00f3lares ser\u00e1 nosso objeto de interesse. Por exemplo: Se no dia 1 o pre\u00e7o mil e no dia 2 o pre\u00e7o era 1 050 e no dia 3 o pre\u00e7o \u00e9 1 025 temos que X_1 = 50 e X_2 = -25 . Vamos supor que as varia\u00e7\u00f5es entre dois diferentes pares de dias s\u00e3o independentes (essa j\u00e1 uma simplifica\u00e7\u00e3o da realidade!) Primeiro vamos importar os dados. import pandas as pd import numpy as np import matplotlib.pyplot as plt import matplotlib.patches as mpatches import seaborn as sns sns . set () gold_df = pd . read_csv ( '../data/gold.csv' , low_memory = False , header = [ 2 , 3 , 4 ]) # There has a lot of data. I will get average diary from USD price gold_df = gold_df [[( 'Priced In' , 'Price Type' , 'Summary' ), ( 'USD' , 'Ask' , 'Average' )]] gold_df . columns = [ 'Day' , 'Price' ] gold_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Day Price 0 1/01/68 NaN 1 2/01/68 NaN 2 3/01/68 NaN 3 4/01/68 NaN 4 5/01/68 NaN Observe que existem diversos Nan values. Na pr\u00e1tica eu teria que fazer alguma esp\u00e9cie de limpeza rigorosa. Nesse caso, para tornar tudo bem simples, vou apenas limpar. Tamb\u00e9m precisamos garantir que as informa\u00e7\u00f5es estejam em formato float . # Inplace assegura que eu n\u00e3o crie outro DataFrame gold_df . dropna ( inplace = True ) gold_df . Price = gold_df . Price . apply ( lambda x : float ( x . strip () . replace ( ',' , '' ))) plt . plot ( gold_df [ 'Price' ]) plt . title ( 'Pre\u00e7o do Ouro em D\u00f3lares' ) plt . show () variation = gold_df . Price . diff () sns . violinplot ( x = variation ) plt . xlim (( - 50 , 50 )) plt . title ( 'Distribui\u00e7\u00e3o da varia\u00e7\u00e3o di\u00e1ria' ) plt . show () De fato n\u00e3o parece uma normal (na verdade uma distribui\u00e7\u00e3o de cauda mais pesada talvez fosse interessante. Mas tudo bem!), mas vamos modelar dessa forma. A partir de agora vamos nos preocupar mais com as defini\u00e7\u00f5es para dar a devida interpreta\u00e7\u00e3o. O exemplo \u00e9 s\u00f3 motivador. Qual hip\u00f3tese queremos testar? O que queremos saber sobre a varia\u00e7\u00e3o? A pergunta que nasce \u00e9 o seguinte: ser\u00e1 que a m\u00e9dia dessa distribui\u00e7\u00e3o \u00e9 0? Isto \u00e9, ser\u00e1 que se calcularmos as m\u00e9dias das varia\u00e7\u00f5es, teremos que com infinitas observa\u00e7\u00f5es, o resultado seria 0? Isso \u00e9 importante porque vai nos ajudar a identificar se existe uma tend\u00eancia de crescimento nas varia\u00e7\u00f5es di\u00e1rias. Hip\u00f3tese Nula: \\mu = 0 , onde X_1, ..., X_n \\sim N(\\mu, \\sigma^2) . Hip\u00f3tese Alternativa: \\mu \\neq 0 . Vamos supor que \\sigma \u00e9 conhecido e que \\sigma^2 = 73 , mas a m\u00e9dia \u00e9 desconhecida. Quem \u00e9 \\Omega_0 e \\Omega_1 ? \\Omega_0 \u00e9 a regi\u00e3o dos par\u00e2metros onde a hip\u00f3tese nula \u00e9 verdadeira, isto \u00e9 \\Omega_0 = \\{0\\} , \u00e9 um conjunto unit\u00e1rio. Por outro lado \\Omega_1 = \\mathbb{R} - \\{0\\} , por que a m\u00e9dia pode assumir, em teoria, qualquer valor real. Qual \u00e9 a regi\u00e3o cr\u00edtica? Bom, ainda n\u00e3o podemos determinar essa resposta, afinal para determinar a regi\u00e3o cr\u00edtica (subconjunto do espa\u00e7o dos estados em que se rejeita a hip\u00f3tese nula), precisamos de um procedimento de teste. Mas vamos imaginar que o espa\u00e7o de estados \u00e9 S = \\mathbb{R}^2 , pois vamos considerar apenas duas amostras, inicialmente (queremos visualizar S). Qual ser\u00e1 nosso procedimento de teste? Procedimento de teste \u00e9 uma maneira de tomarmos uma decis\u00e3o. Ele tem a forma: rejeitamos H_0 se isso acontecer. Um exemplo bobo seria: Rejeitamos a hip\u00f3tese nula se 0 \\not \\in (\\min(X_1, X_2), \\max(X_1, X_2)) O problema \u00e9 que em geral esse tipo de procedimento n\u00e3o \u00e9 interessante. Para isso estabelecemos uma estat\u00edstica de teste T e uma Regi\u00e3o de Rejei\u00e7\u00e3o T , tal que nosso procedimento seja: Rejeitamos H_0 se T \\in R . Nesse caso vamos considerar T = \\frac{X_1 + X_2}{2} e vamos rejeitar a hip\u00f3tese se |T| estiver muito longe de 0 , isto \u00e9, se |T| \\geq c . Portanto definimor nossa Regi\u00e3o de Rejei\u00e7\u00e3o como (-\\infty,-c]\\cup[c, + \\infty) , o que reduz nosso problema a determinar c . Qual c seria razo\u00e1vel? 4, 5, 1? Essa pergunta n\u00e3o vai ser respondida. Antes vamos visualizar como fica a regi\u00e3o cr\u00edtica (2). Rejeitamos a hip\u00f3tese se \\left|\\frac{X_1 + X_2}{2}\\right| \\geq c \\rightarrow |X_1 + X_2| \\geq 2c \\rightarrow X_1 + X_2 \\geq 2c \\text{ ou } X_1 + X_2 \\leq -2c Assim: S_1 = \\{(x_1, x_2): x_1 + x_2 \\geq 2c \\text{ ou } x_1 + x_2 \\leq -2c\\} C = [ 1 , 10 , 30 ] decision = lambda x1 , x2 , c : 1 * np . logical_or ( x1 + x2 >= 2 * c , x1 + x2 <= - 2 * c ) x1 , x2 = np . meshgrid ( np . arange ( - 50 , 50 , 0.5 ), np . arange ( - 50 , 50 , 0.5 )) fig , ax = plt . subplots ( 1 , 3 , figsize = ( 21 , 7 )) for i , c in enumerate ( C ): ax [ i ] . contourf ( x1 , x2 , decision ( x1 , x2 , c ), levels = [ 0 , 0.5 , 1 ], colors = [ '#fdcdac' , '#cbd5e8' ]) ax [ i ] . set_xlabel ( r '$x_1$' , fontsize = 20 ) ax [ i ] . set_ylabel ( r '$x_2$' , fontsize = 20 ) ax [ i ] . set_title ( 'Regi\u00e3o Cr\u00edtica quando c = {} ' . format ( c ), fontsize = 20 ) ax [ i ] . legend ( handles = [ mpatches . Patch ( color = '#fdcdac' , label = r '$S_0$' ), mpatches . Patch ( color = '#cbd5e8' , label = r '$S_1$' )]) sample = gold_df . Price . diff () . sample ( n = 2 ) X1 , X2 = sample . iloc [ 0 ], sample . iloc [ 1 ] ax [ i ] . scatter ( X1 , X2 , color = 'black' ) ax [ i ] . text ( X1 + 1 , X2 + 1 , s = r '$(X_1, X_2)$' , fontsize = 15 ) Como a fun\u00e7\u00e3o poder entra nessa hist\u00f3ria? A fun\u00e7\u00e3o poder \u00e9 uma fun\u00e7\u00e3o do par\u00e2metro, no caso \\mu , e retorna a probabilidade de rejeitarmos a hip\u00f3tese, considerando esse par\u00e2metro. Isto \u00e9, \\pi(\\mu) = P_{\\mu}((X_1, X_2) \\in S_1) O que poder\u00edamos fazer, ent\u00e3o, \u00e9 obter a distribui\u00e7\u00e3o conjunta de (X_1, X_2) e integrar na regi\u00e3o S_1 . Vamos considerar dois casos separados: \\mu \\in \\Omega_0 : N\u00e3o sabemos disso, e em geral n\u00e3o \u00e9 poss\u00edvel sabermos. Nesse caso \\pi(\\mu) indica a probabilidade de rejeitarmos a hip\u00f3tese nula, mesmo ela sendo verdadeira (chamamos isso de Erro do Tipo I). \\mu \\in \\Omega_1 : Nesse caso \\pi(\\mu) indica a probabilidade de rejeitarmos a hip\u00f3tese nula, quando de ela \u00e9 falsa. Nesse caso 1 - \\pi(\\mu) \u00e9 a probabilidade de n\u00e3o rejeitarmos a hip\u00f3tese nula, quando de fato dever\u00edamos (chamamos de Erro do Tipo II). ] A fun\u00e7\u00e3o poder se trata mais do teste que estamos usando do que os dados em si. Por isso, podemos comparar testes usando essa fun\u00e7\u00e3o poder. Ent\u00e3o vamos ver nesse caso quem \u00e9 a fun\u00e7\u00e3o poder! Vou calcular usando um m\u00e9todo num\u00e9rico para que peguem a ideia. Agora \u00e9 poss\u00edvel fazer as contas, mas nem sempre \u00e9 trivial. Assim teremos apenas aproxima\u00e7\u00e3o da fun\u00e7\u00e3o poder. C = [ 1 , 10 , 30 ] mu_v = np . arange ( - 100 , 100 , 1 ) n = 10000 power = np . zeros (( len ( C ), len ( mu_v ))) for i , c in enumerate ( C ): for j , mu in enumerate ( mu_v ): X1 = np . random . normal ( loc = mu , scale = 73 , size = n ) X2 = np . random . normal ( loc = mu , scale = 73 , size = n ) p = ( sum ( X1 + X2 >= 2 * c ) + sum ( X1 + X2 <= - 2 * c )) / n power [ i , j ] = p plt . plot ( mu_v , power [ i ,:], label = 'c = {} ' . format ( c )) plt . title ( 'Fun\u00e7\u00e3o poder' ) plt . xlabel ( r '$\\mu$' ) plt . ylabel ( 'Prob' ) plt . legend () plt . show () Vamos definir c agora? Sim, vamos. Para isso vamos usar o ponto (4) e a defini\u00e7\u00e3o de tamanho do teste. Uma forma poss\u00edvel de se fazer isso \u00e9 a seguinte: limitamos o Erro I por \\alpha_0 e miniminizamos o Erro II, isso \u00e9 minimizamos 1 - \\pi(\\mu) quando \\mu \\neq 0 , ou melhor, maximizamos \\pi(\\mu) . Para isso dizemos que o tamanho do teste \u00e9 o m\u00e1ximo da fun\u00e7\u00e3o poder, quando \\theta \\in \\Omega_0 . Nesse caso \\alpha(\\delta) = \\pi(0) . Queremos, ent\u00e3o que: \\pi(0) = P_{\\mu = 0}(T \\geq c) \\leq \\alpha_0 Precisamos ent\u00e3o encontrar c tal que \\pi(\\mu) = P_{\\mu \\neq 0}(T \\ge c) seja maximado. Observamos que, quando \\mu = 0 , T \\sim N(0, \\sigma^2/2) \\rightarrow \\sqrt{2}T/\\sigma = Z \\sim N(0,1) . Logo P(|T| \\ge c) = P(|Z| \\ge \\sqrt{2}c/\\sigma) = 2(1 - \\Phi(\\sqrt{2}c/\\sigma)) \\leq \\alpha_0 Para maximizar \\pi(\\mu) em \\Omega_1 , observamos que \\pi(\\mu) decresce com c (os gr\u00e1ficos acima representam bem isso). Como queremos maximizar, gostar\u00edamos de tomar c o m\u00ednimo poss\u00edvel, restrito a 2(1 - \\Phi(\\sqrt{2}c/\\sigma)) \\leq \\alpha_0 \\rightarrow 1 - \\alpha_0/2 \\leq \\Phi(\\sqrt{2}c/\\sigma) como vimos acima. Estamos lidando com uma fun\u00e7\u00e3o invers\u00edvel, ent\u00e3o \\frac{\\sigma}{\\sqrt{2}}\\Phi^{-1}(1 - \\alpha_0/2) \\leq c O melhor valor de c que respeita essa condi\u00e7\u00e3o e maximiza a rela\u00e7\u00e3o \u00e9, portanto c = \\frac{\\sigma}{\\sqrt{2}}\\Phi^{-1}(1 - \\alpha_0/2) from scipy.stats import norm Lembre que \\alpha_0 indica o m\u00e1ximo de Erro I que aceitamos. alpha0 = 0.05 c = np . sqrt ( 73 ) / np . sqrt ( 2 ) * norm . ppf ( 1 - alpha0 / 2 ) print ( c ) 11.841167465893536 \u00c9 bem pr\u00f3ximo do gr\u00e1fico acima mostrado, quando testamos para c = 10 . t = np . arange ( - 20 , 20 , 0.1 ) X = norm ( loc = 0 , scale = np . sqrt ( 73 ) / np . sqrt ( 2 )) plt . plot ( t , X . pdf ( t )) plt . fill_between ( t [( t < - c )], X . pdf ( t [( t < - c )]), color = 'blue' ) plt . fill_between ( t [( t > c )], X . pdf ( t [( t > c )]), color = 'blue' ) plt . title ( 'Distribui\u00e7\u00e3o Normal e Regi\u00e3o de Rejei\u00e7\u00e3o' ) plt . show () Por exemplo vamos tirar duas amostras de nossa distribui\u00e7\u00e3o X1 , X2 = gold_df . Price . diff () . sample ( 2 ) T = np . abs ( X1 + X2 ) / 2 T >= c False Mas como escolher \\alpha_0 agora? Agora entra o conceito mais complexo, o do p-valor. Ele est\u00e1 associado \u00e0 ideia de escolher o menor \\alpha_0 poss\u00edvel, para que rejeitemos a hip\u00f3tese nula. Isso significa o seguinte: Queremos minimizar o Erro do Tipo I e rejeitar a Hip\u00f3tese Nula com os dados que obtivemos. Se o p-valor for muito alto, significa que o Erro do Tipo I \u00e9 grande se rejeitarmos a hip\u00f3tese nula. Voc\u00ea apostaria que podemos rejeitar a hip\u00f3tese nula nesse caso? Agora, se o p-valor for pequeno e rejeitarmos nossa hip\u00f3tese nula, o erro do tipo I vai ser pequeno, ent\u00e3o apostar que a hip\u00f3tese nula deva ser rejeitada \u00e9 mais confort\u00e1vel. Assim n\u00e3o escolhemos \\alpha_0 , s\u00f3 observamos seu menor valor e vemos se faz sentido. Em geral se p-valor < 0.05, as pessoas rejeitam a hip\u00f3tese nula. No nosso caso calcular o p-valor \u00e9 tranquilo. Para calcular o p-valor, precisamos dos dados . Queremos rejeitar a hip\u00f3tese nula, isto \u00e9, queremos que t \\geq c que \\alpha_0 seja o menor poss\u00edvel, onde t \u00e9 o valor observado de T . Vamos diminuindo \\alpha_0 e para cada \\alpha_0 podemos calcular c e verificamos se t \\ge c . Podemos fazer isso at\u00e9 que c = t , assim: t = \\frac{\\sigma}{\\sqrt{2}}\\Phi^{-1}(1 - \\alpha_0/2) \\rightarrow \\alpha_0 = 2\\left(1 - \\Phi\\left(\\frac{\\sqrt{2}}{\\sigma}t\\right)\\right) p_value = 2 * ( 1 - norm . cdf ( np . sqrt ( 2 ) / np . sqrt ( 73 ) * T )) print ( p_value ) 0.8477386286989927 Como o p-valor \u00e9 alto, n\u00e3o faz sentido rejeitar a hip\u00f3tese nula. Encerramos a atividade aqui! Testes de Raz\u00e3o Verossimilhan\u00e7a S\u00e3o testes baseados na verossimilha\u00e7a do modelo f_n(x|\\theta) . Suponha que queremos testar a hip\u00f3tese: H_0: \\theta \\in \\Omega_0 H_1: \\theta \\in \\Omega_1 Vamos lembrar que a fun\u00e7\u00e3o de verossimilhan\u00e7a tende a ser mais alta pr\u00f3ximo do valor verdadeiro do par\u00e2metro. Com isso em mente, gostar\u00edamos de saber se a verossimilhan\u00e7a \u00e9 maior em \\Omega_0 ou em \\Omega_1 . Para isso, definimos a estat\u00edstica de raz\u00e3o de verossimilhan\u00e7a : \\Lambda(x) = \\frac{\\sup_{\\theta \\in \\Omega_0}f_n(x|\\theta)}{\\sup_{\\theta \\in \\Omega}f_n(x|\\theta)} Observe que o denominador \u00e9 o valor da fun\u00e7\u00e3o de verossimilhan\u00e7a no Estimador de M\u00e1xima Verossimilhan\u00e7a. Se o par\u00e2metro verdadeiro estiver em \\Omega_0 , o n\u00famerador deve ser mais alto em \\Omega_0 , ent\u00e3o a estat\u00edstica se aproxima de 1. Baseado nisso, o teste de raz\u00e3o de verossimilhan\u00e7a \u00e9: Rejeitamos H_0 se \\Lambda(x) \\le k , para algum k . Teorema: Seja \\Omega \\in \\mathbb{R}^p aberto e suponha que H_0 seja \\theta_{i_1} = \\theta_{01}, ..., \\theta_{i_k} = \\theta_{0k} , onde \\theta = (\\theta_1, ..., \\theta_p) . Assuma que H_0 seja verdadeira e a fun\u00e7\u00e3o de verossimilhan\u00e7a satisfa\u00e7a as condi\u00e7\u00f5es para que o MLE seja assintoticamente normal e assintoticamente eficiente. Ent\u00e3o: -2\\log\\Lambda(x) \\overset{d}{\\to} \\chi^2(k) (converge em distribui\u00e7\u00e3o quando n \\to \\infty ). A demonstra\u00e7\u00e3o pode ser encontrada no StatLect Testes n\u00e3o enviesados Um teste \u00e9 dito n\u00e3o enviesado se \\forall \\theta \\in \\Omega_0 e \\theta ' \\in \\Omega_1, \\pi(\\theta|\\delta) \\le \\pi(\\theta '|\\delta) N\u00e3o \u00e9 muito utilizado dado seu dif\u00edcil c\u00e1lculo num\u00e9rico e n\u00e3o traz resultados quem valem a pena.","title":"Teste de Hip\u00f3teses: Defini\u00e7\u00f5es"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#teste-de-hipoteses-definicoes","text":"Hip\u00f3tese Nula e Alternativa Regi\u00e3o Cr\u00edtica Estat\u00edstica de Teste Fun\u00e7\u00e3o de Poder Tipos de Erro N\u00edvel e tamanho do teste P-valor Temos um problema estat\u00edstico que envolve um par\u00e2metro \\theta tal que tenha valor desconhecido, mas reside em um espa\u00e7o \\Omega . Suponha que particionemos \\Omega = \\Omega_1 \\dot\\cup ~\\Omega_2 e o estat\u00edstico est\u00e1 interessado se \\theta est\u00e1 em \\Omega_0 ou est\u00e1 em \\Omega_1 .","title":"Teste de Hip\u00f3teses: Defini\u00e7\u00f5es"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#hipotese-nula-e-alternativa","text":"Dizemos que H_0 \u00e9 a hip\u00f3tese de que \\theta \\in \\Omega_0 e chamamos H_0 de hip\u00f3tese nula , enquanto H_1 \u00e9 a hip\u00f3tese alternativa e representa \\theta \\in H_1 . Queremos decidir qual das hip\u00f3teses \u00e9 verdadeira (e s\u00f3 uma ser\u00e1, porque a parti\u00e7\u00e3o \u00e9 disjunta). Se decidimos que \\theta \\in \\Omega_1 , rejeitamos H_0 , e se \\theta \\in \\Omega_0 , n\u00e3o rejeitamos H_0 .","title":"Hip\u00f3tese Nula e Alternativa"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#hipotese-simples-e-composta","text":"Suponha que X_1, ..., X_n formam uma amostra aleat\u00f3ria com pdf f(x|\\theta) . Queremos testar a hip\u00f3tese de que H_0: \\theta \\in \\Omega_0 H_1: \\theta \\in \\Omega_1 Se \\Omega_i contem apenas um valor, ent\u00e3o H_i \u00e9 dita hip\u00f3tese simples. Se cont\u00e9m mais de um valor, dizemos que \u00e9 composta.","title":"Hip\u00f3tese Simples e Composta"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#hipotese-unilateral-e-bilateral","text":"Seja \\theta um par\u00e2metro unidimensional. Dizemos que a hip\u00f3tese H_0 \u00e9 unilateral (ou one tailed ) quando \u00e9 da forma \\theta \\leq \\theta_0 ou \\theta \\geq \\theta_0 . Ela ser\u00e1 bilateral quando \u00e9 do tipo H_0 \\neq \\theta_0 .","title":"Hip\u00f3tese Unilateral e Bilateral"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#regiao-critica","text":"Suponha que queremos testar a hip\u00f3tese de que H_0: \\theta \\in \\Omega_0 \\text{ e } H_1: \\theta \\in \\Omega_1 Quando queremos decidir qual hip\u00f3tese escolher, observamos uma amostra dessa distribui\u00e7\u00e3o no espa\u00e7o de amostras S . O dever do estat\u00edstico \u00e9 especificar um procedimento que particione o conjunto em dois subconjuntos S_0 e S_1 , onde S_1 cont\u00e9m os valores de X que rejeitam H_0 . Regi\u00e3o cr\u00edtica \u00e9 o conjunto S_1 , isto \u00e9, o conjunto de amostras que, a partir de um procedimento, rejeita H_0 .","title":"Regi\u00e3o Cr\u00edtica"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#estatistica-de-teste","text":"Seja X_1, ..., X_n \\overset{iid}{\\sim} F(\\theta) . Sejam T = r(X) uma estat\u00edstica e R um subconjunto da reta. Suponha que nosso procedimento de teste \u00e9 o seguinte: Rejeitamos H_0 se T\\in R . Chamamos T de estat\u00edstica de teste e R de regi\u00e3o de rejei\u00e7\u00e3o . Dessa forma a regi\u00e3o cr\u00edtica ser\u00e1: S_1 = \\{x \\in S: r(x) \\in R\\} . Na pr\u00e1tica a maioria dos testes \u00e9 do tipo Rejeitamos H_0 se T \\geq c, c \\in \\mathbb{R} .","title":"Estat\u00edstica de Teste"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#observacao-sobre-a-divisao-de-conjuntos","text":"\u00c9 importante lembrar que h\u00e1 duas diferentes divis\u00f5es: \\Omega = \\Omega_0 \\dot\\cup \\Omega_1 , que \u00e9 a divis\u00e3o do espa\u00e7o dos par\u00e2metros, e S = S_1 \\dot\\cup S_1 \u00e9 a divis\u00e3o do espa\u00e7o das amostras. Mas qual a rela\u00e7\u00e3o entre eles? Se X \\in S_1 , ent\u00e3o rejeitamos a hip\u00f3tese \\theta \\in \\Omega_0 . Al\u00e9m do mais, podemos encontrar S_1 e S_2 , mas dificilmente saberemos em qual dos conjuntos \\theta pertence.","title":"Observa\u00e7\u00e3o sobre a divis\u00e3o de conjuntos"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#funcao-de-poder-e-tipos-de-erro","text":"","title":"Fun\u00e7\u00e3o de Poder e Tipos de Erro"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#funcao-poder","text":"Seja \\delta um procedimento de teste (como esse assinalado acima). Se S_1 \u00e9 a regi\u00e3o cr\u00edtica, \\pi(\\theta|\\delta) = P(X \\in S_1|\\theta) = P(T \\in R|\\theta) Sendo que a \u00faltima igualdade ocorre quando o proocedimento de teste \u00e9 o citado acima. O seu significado? \u00c9 a probabilidade, para cada valor de \\theta , de que \\delta rejeita H_0 . Queremos, intuitivamente que: \\theta \\in \\Omega_0 \\rightarrow \\text{ Queremos n\u00e3o rejeitar} H_0 \\rightarrow \\pi(\\theta|\\delta) = 0 \\theta \\in \\Omega_1 \\rightarrow \\text{ Queremos rejeitar} H_0 \\rightarrow \\pi(\\theta|\\delta) = 1 Entretanto isso n\u00e3o \u00e9 em geral o que acontece. Por isso definimos:","title":"Fun\u00e7\u00e3o Poder"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#erros-i-e-ii","text":"\\theta \\in \\Omega_0 \\theta \\in \\Omega_1 \\delta rejeita H_0 Erro Tipo I Certo \\delta n\u00e3o rejeita H_0 Certo Erro Tipo II Portanto se \\theta \\in \\Omega_0, \\pi(\\theta|\\delta) \u00e9 a probabilidade de cometermos o erro do tipo I. Se \\theta \\in \\Omega_1, 1 - \\pi(\\theta|\\delta) \u00e9 a probabilidade de cometer o erro do tipo II.","title":"Erros I e II"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#niveltamanho","text":"Um teste que satisfaz \\pi(\\theta|\\delta) \\leq \\alpha_0, \\forall \\theta \\in \\Omega_0 \u00e9 chamado de teste n\u00edvel \\alpha_0 , ou que o teste tem n\u00edvel de signific\u00e2ncia \\alpha_0 . O tamanho de um teste \u00e9 \\alpha(\\delta) = \\sup_{\\theta \\in \\Omega_0} \\pi(\\theta, \\delta) . Um teste ter\u00e1 n\u00edvel \\alpha_0 se, e s\u00f3 se, seu tamanho for no m\u00e1ximo \\alpha_0 .","title":"N\u00edvel/Tamanho"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#p-valor","text":"\u00c9 o menor n\u00edvel \\alpha_0 tal que rejeitar\u00edamos a hip\u00f3tese nula a n\u00edvel \\alpha_0 com os dados observados. Se rejeitamos a hip\u00f3tese nula se, e somente se, o p-valor \u00e9 no m\u00e1ximo \\alpha_0 , estamos usando um teste com n\u00edvel de signific\u00e2ncia \\alpha_0 .","title":"P-valor"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#equivalencia-entre-testes-e-conjuntos-de-confianca","text":"","title":"Equival\u00eancia entre Testes e Conjuntos de Confian\u00e7a"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#teorema","text":"Seja \\vec{X} = (X_1,...,X_n) \\overset{iid}{\\sim} F(\\theta) . Seja g(\\theta) , e suponha que para todo valor c na imagem de g (ou seja, c = g(x) , para algum x ), exista um teste \\delta_c de n\u00edvel \\alpha_0 para a hip\u00f3tese H_{0,c}:g(\\theta) = c, ~ H_{1,c}: g(\\theta) \\neq c Defina \\omega(x) := \\{c: \\delta_c \\text{ n\u00e3o rejeita } H_{0,c} \\text{ se } \\vec{X} = \\vec{x} \\text{ \u00e9 observado } \\} . Ent\u00e3o: P[g(\\theta_0) \\in \\omega(\\vec{X})|\\theta = \\theta_0] \\geq 1 - \\alpha_0, para todo valor \\theta \\in \\Omega .","title":"Teorema"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#compreensao-e-implementacao","text":"Teste de hip\u00f3tese \u00e9 um m\u00e9todo para que fa\u00e7amos decis\u00f5es estat\u00edsticas a partir dos dados. \u00c9 uma forma de compreender (fazer infer\u00e2ncia sobre) um par\u00e2metro. Exemplo: Belgas tem, em m\u00e9dia, maior altura do que peruanos. Exemplo 2: Temperatura n\u00e3o \u00e9 um fator relevante para o processo de cultivo de uva. Estamos avaliando afirma\u00e7\u00f5es mutualmente exclusivas, ou os belgas tem maior altura do que os peruanos, ou n\u00e3o tem! Queremos saber qual dessas afirma\u00e7\u00f5es \u00e9 suportada pelos dados que obtivermos. A hip\u00f3tese nula \u00e9 a afirma\u00e7\u00e3o a ser testada e muitas vezes estabelece uma conjectura de que as caracter\u00edsticas observadas em uma popula\u00e7\u00e3o s\u00e3o por um acaso, isto \u00e9, o fator a ser estudado \"n\u00e3o existe\". Por exemplo: o n\u00famero de voos entre Rio de Janeiro e S\u00e3o Paulo n\u00e3o tem correla\u00e7\u00e3o com o n\u00edvel do mar no Jap\u00e3o. Em geral queremos anul\u00e1-la, rejeit\u00e1-la (da\u00ed o nome).","title":"Compreens\u00e3o e Implementa\u00e7\u00e3o"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#exemplo","text":"Vamos considerar um exemplo simples utilizando a distribui\u00e7\u00e3o normal. \u00c9 a distribui\u00e7\u00e3o com c\u00e1lculos simples e uma boa visualiza\u00e7\u00e3o. A ideia nesse exemplo vai ser a seguinte: O pre\u00e7o do quilo ouro varia diariamente e essa varia\u00e7\u00e3o em unidade de d\u00f3lares ser\u00e1 nosso objeto de interesse. Por exemplo: Se no dia 1 o pre\u00e7o mil e no dia 2 o pre\u00e7o era 1 050 e no dia 3 o pre\u00e7o \u00e9 1 025 temos que X_1 = 50 e X_2 = -25 . Vamos supor que as varia\u00e7\u00f5es entre dois diferentes pares de dias s\u00e3o independentes (essa j\u00e1 uma simplifica\u00e7\u00e3o da realidade!) Primeiro vamos importar os dados. import pandas as pd import numpy as np import matplotlib.pyplot as plt import matplotlib.patches as mpatches import seaborn as sns sns . set () gold_df = pd . read_csv ( '../data/gold.csv' , low_memory = False , header = [ 2 , 3 , 4 ]) # There has a lot of data. I will get average diary from USD price gold_df = gold_df [[( 'Priced In' , 'Price Type' , 'Summary' ), ( 'USD' , 'Ask' , 'Average' )]] gold_df . columns = [ 'Day' , 'Price' ] gold_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Day Price 0 1/01/68 NaN 1 2/01/68 NaN 2 3/01/68 NaN 3 4/01/68 NaN 4 5/01/68 NaN Observe que existem diversos Nan values. Na pr\u00e1tica eu teria que fazer alguma esp\u00e9cie de limpeza rigorosa. Nesse caso, para tornar tudo bem simples, vou apenas limpar. Tamb\u00e9m precisamos garantir que as informa\u00e7\u00f5es estejam em formato float . # Inplace assegura que eu n\u00e3o crie outro DataFrame gold_df . dropna ( inplace = True ) gold_df . Price = gold_df . Price . apply ( lambda x : float ( x . strip () . replace ( ',' , '' ))) plt . plot ( gold_df [ 'Price' ]) plt . title ( 'Pre\u00e7o do Ouro em D\u00f3lares' ) plt . show () variation = gold_df . Price . diff () sns . violinplot ( x = variation ) plt . xlim (( - 50 , 50 )) plt . title ( 'Distribui\u00e7\u00e3o da varia\u00e7\u00e3o di\u00e1ria' ) plt . show () De fato n\u00e3o parece uma normal (na verdade uma distribui\u00e7\u00e3o de cauda mais pesada talvez fosse interessante. Mas tudo bem!), mas vamos modelar dessa forma. A partir de agora vamos nos preocupar mais com as defini\u00e7\u00f5es para dar a devida interpreta\u00e7\u00e3o. O exemplo \u00e9 s\u00f3 motivador. Qual hip\u00f3tese queremos testar? O que queremos saber sobre a varia\u00e7\u00e3o? A pergunta que nasce \u00e9 o seguinte: ser\u00e1 que a m\u00e9dia dessa distribui\u00e7\u00e3o \u00e9 0? Isto \u00e9, ser\u00e1 que se calcularmos as m\u00e9dias das varia\u00e7\u00f5es, teremos que com infinitas observa\u00e7\u00f5es, o resultado seria 0? Isso \u00e9 importante porque vai nos ajudar a identificar se existe uma tend\u00eancia de crescimento nas varia\u00e7\u00f5es di\u00e1rias. Hip\u00f3tese Nula: \\mu = 0 , onde X_1, ..., X_n \\sim N(\\mu, \\sigma^2) . Hip\u00f3tese Alternativa: \\mu \\neq 0 . Vamos supor que \\sigma \u00e9 conhecido e que \\sigma^2 = 73 , mas a m\u00e9dia \u00e9 desconhecida. Quem \u00e9 \\Omega_0 e \\Omega_1 ? \\Omega_0 \u00e9 a regi\u00e3o dos par\u00e2metros onde a hip\u00f3tese nula \u00e9 verdadeira, isto \u00e9 \\Omega_0 = \\{0\\} , \u00e9 um conjunto unit\u00e1rio. Por outro lado \\Omega_1 = \\mathbb{R} - \\{0\\} , por que a m\u00e9dia pode assumir, em teoria, qualquer valor real. Qual \u00e9 a regi\u00e3o cr\u00edtica? Bom, ainda n\u00e3o podemos determinar essa resposta, afinal para determinar a regi\u00e3o cr\u00edtica (subconjunto do espa\u00e7o dos estados em que se rejeita a hip\u00f3tese nula), precisamos de um procedimento de teste. Mas vamos imaginar que o espa\u00e7o de estados \u00e9 S = \\mathbb{R}^2 , pois vamos considerar apenas duas amostras, inicialmente (queremos visualizar S). Qual ser\u00e1 nosso procedimento de teste? Procedimento de teste \u00e9 uma maneira de tomarmos uma decis\u00e3o. Ele tem a forma: rejeitamos H_0 se isso acontecer. Um exemplo bobo seria: Rejeitamos a hip\u00f3tese nula se 0 \\not \\in (\\min(X_1, X_2), \\max(X_1, X_2)) O problema \u00e9 que em geral esse tipo de procedimento n\u00e3o \u00e9 interessante. Para isso estabelecemos uma estat\u00edstica de teste T e uma Regi\u00e3o de Rejei\u00e7\u00e3o T , tal que nosso procedimento seja: Rejeitamos H_0 se T \\in R . Nesse caso vamos considerar T = \\frac{X_1 + X_2}{2} e vamos rejeitar a hip\u00f3tese se |T| estiver muito longe de 0 , isto \u00e9, se |T| \\geq c . Portanto definimor nossa Regi\u00e3o de Rejei\u00e7\u00e3o como (-\\infty,-c]\\cup[c, + \\infty) , o que reduz nosso problema a determinar c . Qual c seria razo\u00e1vel? 4, 5, 1? Essa pergunta n\u00e3o vai ser respondida. Antes vamos visualizar como fica a regi\u00e3o cr\u00edtica (2). Rejeitamos a hip\u00f3tese se \\left|\\frac{X_1 + X_2}{2}\\right| \\geq c \\rightarrow |X_1 + X_2| \\geq 2c \\rightarrow X_1 + X_2 \\geq 2c \\text{ ou } X_1 + X_2 \\leq -2c Assim: S_1 = \\{(x_1, x_2): x_1 + x_2 \\geq 2c \\text{ ou } x_1 + x_2 \\leq -2c\\} C = [ 1 , 10 , 30 ] decision = lambda x1 , x2 , c : 1 * np . logical_or ( x1 + x2 >= 2 * c , x1 + x2 <= - 2 * c ) x1 , x2 = np . meshgrid ( np . arange ( - 50 , 50 , 0.5 ), np . arange ( - 50 , 50 , 0.5 )) fig , ax = plt . subplots ( 1 , 3 , figsize = ( 21 , 7 )) for i , c in enumerate ( C ): ax [ i ] . contourf ( x1 , x2 , decision ( x1 , x2 , c ), levels = [ 0 , 0.5 , 1 ], colors = [ '#fdcdac' , '#cbd5e8' ]) ax [ i ] . set_xlabel ( r '$x_1$' , fontsize = 20 ) ax [ i ] . set_ylabel ( r '$x_2$' , fontsize = 20 ) ax [ i ] . set_title ( 'Regi\u00e3o Cr\u00edtica quando c = {} ' . format ( c ), fontsize = 20 ) ax [ i ] . legend ( handles = [ mpatches . Patch ( color = '#fdcdac' , label = r '$S_0$' ), mpatches . Patch ( color = '#cbd5e8' , label = r '$S_1$' )]) sample = gold_df . Price . diff () . sample ( n = 2 ) X1 , X2 = sample . iloc [ 0 ], sample . iloc [ 1 ] ax [ i ] . scatter ( X1 , X2 , color = 'black' ) ax [ i ] . text ( X1 + 1 , X2 + 1 , s = r '$(X_1, X_2)$' , fontsize = 15 ) Como a fun\u00e7\u00e3o poder entra nessa hist\u00f3ria? A fun\u00e7\u00e3o poder \u00e9 uma fun\u00e7\u00e3o do par\u00e2metro, no caso \\mu , e retorna a probabilidade de rejeitarmos a hip\u00f3tese, considerando esse par\u00e2metro. Isto \u00e9, \\pi(\\mu) = P_{\\mu}((X_1, X_2) \\in S_1) O que poder\u00edamos fazer, ent\u00e3o, \u00e9 obter a distribui\u00e7\u00e3o conjunta de (X_1, X_2) e integrar na regi\u00e3o S_1 . Vamos considerar dois casos separados: \\mu \\in \\Omega_0 : N\u00e3o sabemos disso, e em geral n\u00e3o \u00e9 poss\u00edvel sabermos. Nesse caso \\pi(\\mu) indica a probabilidade de rejeitarmos a hip\u00f3tese nula, mesmo ela sendo verdadeira (chamamos isso de Erro do Tipo I). \\mu \\in \\Omega_1 : Nesse caso \\pi(\\mu) indica a probabilidade de rejeitarmos a hip\u00f3tese nula, quando de ela \u00e9 falsa. Nesse caso 1 - \\pi(\\mu) \u00e9 a probabilidade de n\u00e3o rejeitarmos a hip\u00f3tese nula, quando de fato dever\u00edamos (chamamos de Erro do Tipo II). ] A fun\u00e7\u00e3o poder se trata mais do teste que estamos usando do que os dados em si. Por isso, podemos comparar testes usando essa fun\u00e7\u00e3o poder. Ent\u00e3o vamos ver nesse caso quem \u00e9 a fun\u00e7\u00e3o poder! Vou calcular usando um m\u00e9todo num\u00e9rico para que peguem a ideia. Agora \u00e9 poss\u00edvel fazer as contas, mas nem sempre \u00e9 trivial. Assim teremos apenas aproxima\u00e7\u00e3o da fun\u00e7\u00e3o poder. C = [ 1 , 10 , 30 ] mu_v = np . arange ( - 100 , 100 , 1 ) n = 10000 power = np . zeros (( len ( C ), len ( mu_v ))) for i , c in enumerate ( C ): for j , mu in enumerate ( mu_v ): X1 = np . random . normal ( loc = mu , scale = 73 , size = n ) X2 = np . random . normal ( loc = mu , scale = 73 , size = n ) p = ( sum ( X1 + X2 >= 2 * c ) + sum ( X1 + X2 <= - 2 * c )) / n power [ i , j ] = p plt . plot ( mu_v , power [ i ,:], label = 'c = {} ' . format ( c )) plt . title ( 'Fun\u00e7\u00e3o poder' ) plt . xlabel ( r '$\\mu$' ) plt . ylabel ( 'Prob' ) plt . legend () plt . show () Vamos definir c agora? Sim, vamos. Para isso vamos usar o ponto (4) e a defini\u00e7\u00e3o de tamanho do teste. Uma forma poss\u00edvel de se fazer isso \u00e9 a seguinte: limitamos o Erro I por \\alpha_0 e miniminizamos o Erro II, isso \u00e9 minimizamos 1 - \\pi(\\mu) quando \\mu \\neq 0 , ou melhor, maximizamos \\pi(\\mu) . Para isso dizemos que o tamanho do teste \u00e9 o m\u00e1ximo da fun\u00e7\u00e3o poder, quando \\theta \\in \\Omega_0 . Nesse caso \\alpha(\\delta) = \\pi(0) . Queremos, ent\u00e3o que: \\pi(0) = P_{\\mu = 0}(T \\geq c) \\leq \\alpha_0 Precisamos ent\u00e3o encontrar c tal que \\pi(\\mu) = P_{\\mu \\neq 0}(T \\ge c) seja maximado. Observamos que, quando \\mu = 0 , T \\sim N(0, \\sigma^2/2) \\rightarrow \\sqrt{2}T/\\sigma = Z \\sim N(0,1) . Logo P(|T| \\ge c) = P(|Z| \\ge \\sqrt{2}c/\\sigma) = 2(1 - \\Phi(\\sqrt{2}c/\\sigma)) \\leq \\alpha_0 Para maximizar \\pi(\\mu) em \\Omega_1 , observamos que \\pi(\\mu) decresce com c (os gr\u00e1ficos acima representam bem isso). Como queremos maximizar, gostar\u00edamos de tomar c o m\u00ednimo poss\u00edvel, restrito a 2(1 - \\Phi(\\sqrt{2}c/\\sigma)) \\leq \\alpha_0 \\rightarrow 1 - \\alpha_0/2 \\leq \\Phi(\\sqrt{2}c/\\sigma) como vimos acima. Estamos lidando com uma fun\u00e7\u00e3o invers\u00edvel, ent\u00e3o \\frac{\\sigma}{\\sqrt{2}}\\Phi^{-1}(1 - \\alpha_0/2) \\leq c O melhor valor de c que respeita essa condi\u00e7\u00e3o e maximiza a rela\u00e7\u00e3o \u00e9, portanto c = \\frac{\\sigma}{\\sqrt{2}}\\Phi^{-1}(1 - \\alpha_0/2) from scipy.stats import norm Lembre que \\alpha_0 indica o m\u00e1ximo de Erro I que aceitamos. alpha0 = 0.05 c = np . sqrt ( 73 ) / np . sqrt ( 2 ) * norm . ppf ( 1 - alpha0 / 2 ) print ( c ) 11.841167465893536 \u00c9 bem pr\u00f3ximo do gr\u00e1fico acima mostrado, quando testamos para c = 10 . t = np . arange ( - 20 , 20 , 0.1 ) X = norm ( loc = 0 , scale = np . sqrt ( 73 ) / np . sqrt ( 2 )) plt . plot ( t , X . pdf ( t )) plt . fill_between ( t [( t < - c )], X . pdf ( t [( t < - c )]), color = 'blue' ) plt . fill_between ( t [( t > c )], X . pdf ( t [( t > c )]), color = 'blue' ) plt . title ( 'Distribui\u00e7\u00e3o Normal e Regi\u00e3o de Rejei\u00e7\u00e3o' ) plt . show () Por exemplo vamos tirar duas amostras de nossa distribui\u00e7\u00e3o X1 , X2 = gold_df . Price . diff () . sample ( 2 ) T = np . abs ( X1 + X2 ) / 2 T >= c False Mas como escolher \\alpha_0 agora? Agora entra o conceito mais complexo, o do p-valor. Ele est\u00e1 associado \u00e0 ideia de escolher o menor \\alpha_0 poss\u00edvel, para que rejeitemos a hip\u00f3tese nula. Isso significa o seguinte: Queremos minimizar o Erro do Tipo I e rejeitar a Hip\u00f3tese Nula com os dados que obtivemos. Se o p-valor for muito alto, significa que o Erro do Tipo I \u00e9 grande se rejeitarmos a hip\u00f3tese nula. Voc\u00ea apostaria que podemos rejeitar a hip\u00f3tese nula nesse caso? Agora, se o p-valor for pequeno e rejeitarmos nossa hip\u00f3tese nula, o erro do tipo I vai ser pequeno, ent\u00e3o apostar que a hip\u00f3tese nula deva ser rejeitada \u00e9 mais confort\u00e1vel. Assim n\u00e3o escolhemos \\alpha_0 , s\u00f3 observamos seu menor valor e vemos se faz sentido. Em geral se p-valor < 0.05, as pessoas rejeitam a hip\u00f3tese nula. No nosso caso calcular o p-valor \u00e9 tranquilo. Para calcular o p-valor, precisamos dos dados . Queremos rejeitar a hip\u00f3tese nula, isto \u00e9, queremos que t \\geq c que \\alpha_0 seja o menor poss\u00edvel, onde t \u00e9 o valor observado de T . Vamos diminuindo \\alpha_0 e para cada \\alpha_0 podemos calcular c e verificamos se t \\ge c . Podemos fazer isso at\u00e9 que c = t , assim: t = \\frac{\\sigma}{\\sqrt{2}}\\Phi^{-1}(1 - \\alpha_0/2) \\rightarrow \\alpha_0 = 2\\left(1 - \\Phi\\left(\\frac{\\sqrt{2}}{\\sigma}t\\right)\\right) p_value = 2 * ( 1 - norm . cdf ( np . sqrt ( 2 ) / np . sqrt ( 73 ) * T )) print ( p_value ) 0.8477386286989927 Como o p-valor \u00e9 alto, n\u00e3o faz sentido rejeitar a hip\u00f3tese nula. Encerramos a atividade aqui!","title":"Exemplo"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#testes-de-razao-verossimilhanca","text":"S\u00e3o testes baseados na verossimilha\u00e7a do modelo f_n(x|\\theta) . Suponha que queremos testar a hip\u00f3tese: H_0: \\theta \\in \\Omega_0 H_1: \\theta \\in \\Omega_1 Vamos lembrar que a fun\u00e7\u00e3o de verossimilhan\u00e7a tende a ser mais alta pr\u00f3ximo do valor verdadeiro do par\u00e2metro. Com isso em mente, gostar\u00edamos de saber se a verossimilhan\u00e7a \u00e9 maior em \\Omega_0 ou em \\Omega_1 . Para isso, definimos a estat\u00edstica de raz\u00e3o de verossimilhan\u00e7a : \\Lambda(x) = \\frac{\\sup_{\\theta \\in \\Omega_0}f_n(x|\\theta)}{\\sup_{\\theta \\in \\Omega}f_n(x|\\theta)} Observe que o denominador \u00e9 o valor da fun\u00e7\u00e3o de verossimilhan\u00e7a no Estimador de M\u00e1xima Verossimilhan\u00e7a. Se o par\u00e2metro verdadeiro estiver em \\Omega_0 , o n\u00famerador deve ser mais alto em \\Omega_0 , ent\u00e3o a estat\u00edstica se aproxima de 1. Baseado nisso, o teste de raz\u00e3o de verossimilhan\u00e7a \u00e9: Rejeitamos H_0 se \\Lambda(x) \\le k , para algum k .","title":"Testes de Raz\u00e3o Verossimilhan\u00e7a"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#teorema_1","text":"Seja \\Omega \\in \\mathbb{R}^p aberto e suponha que H_0 seja \\theta_{i_1} = \\theta_{01}, ..., \\theta_{i_k} = \\theta_{0k} , onde \\theta = (\\theta_1, ..., \\theta_p) . Assuma que H_0 seja verdadeira e a fun\u00e7\u00e3o de verossimilhan\u00e7a satisfa\u00e7a as condi\u00e7\u00f5es para que o MLE seja assintoticamente normal e assintoticamente eficiente. Ent\u00e3o: -2\\log\\Lambda(x) \\overset{d}{\\to} \\chi^2(k) (converge em distribui\u00e7\u00e3o quando n \\to \\infty ). A demonstra\u00e7\u00e3o pode ser encontrada no StatLect","title":"Teorema:"},{"location":"infestatistica/TestingHypotheses/TestingHypotheses/#testes-nao-enviesados","text":"Um teste \u00e9 dito n\u00e3o enviesado se \\forall \\theta \\in \\Omega_0 e \\theta ' \\in \\Omega_1, \\pi(\\theta|\\delta) \\le \\pi(\\theta '|\\delta) N\u00e3o \u00e9 muito utilizado dado seu dif\u00edcil c\u00e1lculo num\u00e9rico e n\u00e3o traz resultados quem valem a pena.","title":"Testes n\u00e3o enviesados"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/","text":"Teste de Hip\u00f3teses II Nesse notebook veremos: Teste de Hip\u00f3teses Simples Hip\u00f3tese Alternativa Bilateral Teste T Comparando m\u00e9dias de duas Normais Comparando vari\u00e2ncias de duas Normais Teste de Hip\u00f3tese Simples O objetivo \u00e9 considerar se um vetor de observa\u00e7\u00f5es vem de uma entre duas observa\u00e7\u00f5es. Nesse caso o espa\u00e7o \\Omega \u00e9 formado por dois pontos, e n\u00e3o \u00e9 um espa\u00e7o de par\u00e2metros, mas espa\u00e7o de distribui\u00e7\u00f5es, em particular dessas duas distribui\u00e7\u00f5es. Isto \u00e9, vamos assumir que X = (X_1, ..., X_n) vem de f_0(x) ou f_1(x) . Assim \\Omega = \\{\\theta_0, \\theta_1\\} e \\theta = \\theta_i se os dados tem distribui\u00e7\u00e3o f_i(x), i = 0,1 . Vamos denotar: \\alpha(\\delta) = P(\\text{Rejeitar} H_0|\\theta = \\theta_0) = P(\\text{Erro I}) \\beta(\\delta) = P(\\text{N\u00e3o rejeitar} H_0|\\theta = \\theta_1) = P(\\text{Erro II}) Teorema Seja \\delta^* o procedimento de teste que n\u00e3o rejeita H_0 se af_0(x) > bf_1(x) e rejeita se af_0(x) < bf_1(x) . Ent\u00e3o, para todo outro procedimento de teste \\delta , a\\alpha(\\delta^*) + b\\beta(\\delta^*) \\le a\\alpha(\\delta) + b\\beta(\\delta) Queremos escolher um teste que minimize essa combina\u00e7\u00e3o linear a\\alpha(\\delta) + b\\beta(\\delta) . Claro que seria \u00f3timo ter esse erro zerado, mas sabemos que existe uma esp\u00e9cie de trade off entre esses erros. Esse teorema d\u00e1 o teste necess\u00e1rio para que isso aconten\u00e7a. Corol\u00e1rio Considere as hip\u00f3teses do teorema anterior, a > 0 e b > 0 . Defina estat\u00edstica de teste raz\u00e3o de verossimilhan\u00e7a : \\Lambda(x) = \\begin{cases} \\frac{f_0(x)}{f_1(x)}, \\text{ se } f_0(x) \\le f_1(x) \\\\ 1, \\text{ caso contr\u00e1rio }. \\end{cases} Defina o procedimento de teste \\delta : Rejeita H_0 se \\Lambda(x) > a/b . Ent\u00e3o o valor de af_0(x) + bf_1(x) \u00e9 m\u00ednimo. Lema Nayman-Pearson Suponha que \\delta ' tem a seguinte forma, para algum k > 0 : H_0 n\u00e3o \u00e9 rejeitada se f_1(x) < kf_0(x) e o \u00e9 quando f_1(x) > kf_0(x). Se \\delta \u00e9 outro procedimento de teste tal que \\alpha(\\delta) \\le \\alpha(\\delta ') , ent\u00e3o \\beta(\\delta) \\ge \\beta(\\delta '). Implementa\u00e7\u00e3o Vamos fazer uma simples implementa\u00e7\u00e3o de uso para esse tipo de problema. import numpy as np from scipy.stats import bernoulli , binom from scipy.optimize import brute Nesse caso, vamos fazer uma simples simula\u00e7\u00e3o, onde um par\u00e2metro de uma distribui\u00e7\u00e3o de Bernoulli pode ser p = 0.4 ou p = 0.6 . Vamos gerar essa amostra, mas sem de fato conhecer p verdadeiro. ro = np . random . RandomState ( 1000000 ) #random state p = ro . choice ([ 0.4 , 0.6 ]) Teremos uma amostra de tamanho n . n = 20 X = ro . binomial ( 1 , p , size = n ) Vamos utilizar o Lema Nayman-Pearson. O objetivo \u00e9 testar as seguintes hip\u00f3teses: H_0: p = 0.4 H_1: p = 0.6 Vamos fixar \\alpha_0 = 0.05 o tamanho do teste. Temos que, se y = \\sum_{i=1}^n x_i \\sim Binomial(n,p) , \\frac{f_1(x)}{f_0(x)} = \\frac{0.6^y0.4^{n-y}}{0.4^y0.6^{n-y}} = \\left(\\frac{3}{2}\\right)^y\\left(\\frac{2}{3}\\right)^{n-y} = \\left(\\frac{3}{2}\\right)^{2y - n} Assim: \\begin{split} 0.05 &= P(f_1(x) > kf_0(x)|p = 0.4) = P\\left(\\left(\\frac{3}{2}\\right)^{2y - n} > k\\right) \\\\ &= P\\left(2y - n > \\frac{\\log(k)}{\\log(3/2)}\\right) \\\\ &= P\\left(y > \\frac{\\log(k)}{2\\log(3/2)} + \\frac{n}{2}\\right), y \\sim Binomial(n,0.4) \\end{split} Isto \u00e9, preciso escolher k que satisfa\u00e7a essa rela\u00e7\u00e3o. Vamos calcular k numericamente utilizando um m\u00e9todo de otimiza\u00e7\u00e3o por bruta for\u00e7a (s\u00e3o poucas as op\u00e7\u00f5es). Como n\u00e3o queremos que seja marior do que 0.05, precisamos colocar peso para que n\u00e3o seja. Veja que existem v\u00e1rios valores de k que satisfazem isso. alpha0 = 0.05 Y = binom ( n = n , p = 0.4 ) func = lambda k , n : np . abs ( 0.95 - Y . cdf (( 1 / 2 ) * np . log ( k ) / np . log ( 3 / 2 ) + n / 2 )) + \\ 10 * ( 0.95 > Y . cdf (( 1 / 2 ) * np . log ( k ) / np . log ( 3 / 2 ) + n / 2 )) k = brute ( func , ranges = ( slice ( 1 , 20 , 1 ),), args = ( n ,))[ 0 ] k 6.0 Por esse motivo, vamos tomar k=6 . Pela Lema de Neyman Pearson, esse teste \u00e9 o que minimiza o Erro do Tipo II. Vamos ver se rejeitamos ou n\u00e3o a hip\u00f3tese nula baseado nos dados obtidos. f0 = lambda x : 0.4 ** ( sum ( x )) * 0.6 ** ( len ( x ) - sum ( x )) f1 = lambda x : 0.6 ** ( sum ( x )) * 0.4 ** ( len ( x ) - sum ( x )) if f1 ( X ) > k * f0 ( X ): print ( r 'Rejeitamos H0.' ) else : print ( r 'N\u00e3o rejeitamos H0.' ) N\u00e3o rejeitamos H0. Vamos ver quem \u00e9 p , ent\u00e3o. print ( 'O valor de p \u00e9 .... ' ) print ( p ) O valor de p \u00e9 .... 0.4 Fizemos bem em n\u00e3o rejeitar a hip\u00f3tese nula! Hip\u00f3tese Alternativa Bilateral Seja X = (X_1, ..., X_n) uma amostra aleat\u00f3ria de uma distribui\u00e7\u00e3o normal com m\u00e9dia \\mu desconhecida e vari\u00e2ncia \\sigma^2 conhecida e queremos testar a hip\u00f3tese H_0: \\mu = \\mu_0 H_1: \\mu \\neq \\mu_0 Como \\bar{X}_n \u00e9 um estimador consistente de \\mu , faz sentido rejeitar a hip\u00f3tese nula quando a m\u00e9dia amostral se afasta de \\mu_0 . Para isso, vamos escolher c_1, c_2 de forma que P(\\bar{X}_n \\leq c_1|\\mu = \\mu_0) + P(\\bar{X}_n \\geq c_2|\\mu = \\mu_0) = \\alpha_0 \\Rightarrow P\\left(Z \\leq \\sqrt{n}\\frac{c_1 - \\mu_0}{\\sigma}\\right) + P\\left(Z \\geq \\sqrt{n}\\frac{c_2 - \\mu_0}{\\sigma}\\right) = \\alpha_0 \\Rightarrow \\Phi\\left(\\sqrt{n}\\frac{c_1 - \\mu_0}{\\sigma}\\right) + 1 - \\Phi\\left(\\sqrt{n}\\frac{c_2 - \\mu_0}{\\sigma}\\right) = \\alpha_0 \\Rightarrow \\Phi\\left(\\sqrt{n}\\frac{c_1 - \\mu_0}{\\sigma}\\right) = \\alpha_1 \\text{ e } \\Phi\\left(\\sqrt{n}\\frac{c_2 - \\mu_0}{\\sigma}\\right) = 1 - \\alpha_2, \\text{ com } \\alpha_1 + \\alpha_2 = \\alpha_0 Observa\u00e7\u00e3o: \\bar{X}_n \\sim N(\\mu, \\sigma^2/n) \\Rightarrow Z = \\sqrt{n}\\frac{\\bar{X}_n - \\mu}{\\sigma} \\sim N(0,1) Observa\u00e7\u00e3o 2: No c\u00e1lculo substituimos \\mu por \\mu_0 , porque estamos \"condicionando\" no conhecimento deles serem iguais. Isto \u00e9, queremos que o tamanho do teste seja \\alpha_0 , lembrando que o tamanho do teste \u00e9 o supremo das probabilidades de se rejeitar a hip\u00f3tese nula quando ela \u00e9 verdadeira. Teste t Suponha que (X_1,...,X_n) \u00e9 uma amostra aleat\u00f3ria da distribui\u00e7\u00e3o N(\\mu,\\sigma^2) , com par\u00e2metros desconhecidos e queremos testar a hip\u00f3tese: H_0: \\mu \\le \\mu_0 \\implies \\Omega_0 = \\{(x,y) \\in \\mathbb{R}^2 | x \\le \\mu_0 \\text{ e } y > 0\\} H_1: \\mu > \\mu_0 \\implies \\Omega_1 = \\{(x,y) \\in \\mathbb{R}^2 | x > \\mu_0 \\text{ e } y > 0\\} Sabemos que U = n^{1/2}\\frac{\\bar{X}_n - \\mu_0}{\\sigma '} \u00e9 uma boa estat\u00edstica de teste e rejeitamos H_0 se U \\ge c . Essa estat\u00edstica \u00e9 interessante porque quando \\mu = \\mu_0, U \\sim t(n-1) . Por isso chamamos de testes t quando baseados na estat\u00edstica U . Podemos tamb\u00e9m inverter os sinais de desigualdade e rejeitar H_0 quando U \\le c . from pandas import DataFrame from scipy.stats import t import matplotlib.pyplot as plt import seaborn as sns sns . set () % matplotlib notebook mu0 = 10 # Vamos escolher mu e sigma de forma aleat\u00f3ria, mas n\u00e3o significa que \u00e9 uma vari\u00e1vel aleat\u00f3ria. n = 20 Distibui\u00e7\u00e3o de U Vamos gerar uma aproxima\u00e7\u00e3o para a distribui\u00e7\u00e3o de U para um determinado \\mu . U_values = {} for i in range ( 6 ): mu = ro . normal ( mu0 , 1 ) if i < 5 else 10 sigma = ro . exponential ( mu0 ) key = 'mu = {} , sigma = {} ' . format ( np . round ( mu , 2 ), np . round ( sigma , 2 )) U_values [ key ] = np . zeros ( 10000 ) for j in range ( 10000 ): X = ro . normal ( mu , sigma , size = n ) U = np . sqrt ( n ) * ( np . mean ( X ) - mu0 ) / np . std ( X , ddof = 1 ) U_values [ key ][ j ] = U U_values = DataFrame ( U_values ) fig , ax = plt . subplots ( figsize = ( 10 , 6 )) sns . kdeplot ( data = U_values , ax = ax ) ax . set_title ( 'Distribui\u00e7\u00e3o aproximada de U' ) plt . show () Teorema Seja c o 1 - \\alpha_0 quartil da distribui\u00e7\u00e3o t com n-1 graus de liberdade. Ent\u00e3o, segundo o teste citado acima, a fun\u00e7\u00e3o poder tem as seguintes propriedades: \\pi(\\mu, \\sigma^2|\\delta) = \\alpha_0 , quando \\mu = \\mu_0 . \\pi(\\mu, \\sigma^2|\\delta) < \\alpha_0 , quando \\mu < \\mu_0 . \\pi(\\mu, \\sigma^2|\\delta) > \\alpha_0 , quando \\mu > \\mu_0 . \\pi(\\mu, \\sigma^2|\\delta) \\to 0 , quando \\mu \\to -\\infty . \\pi(\\mu, \\sigma^2|\\delta) \\to 1 , quando \\mu \\to \\infty . O teste tamb\u00e9m \u00e9 n\u00e3o enviesado como consequ\u00eancia. P-valores para testes t Seja u a estat\u00edstica U quando observada. Seja T_{n-1}(\\cdot) a cdf da distribui\u00e7\u00e3o t com n-1 graus de liberdade. Ent\u00e3o o p-valor para H_0: \\mu \\leq \\mu_0 \u00e9 1 - T_{n-1}(u) , enquanto o p-valor para H_0: \\mu \\ge \\mu_0 \u00e9 T_{n-1}(u) . Distribui\u00e7\u00e3o t n\u00e3o central O objetivo \u00e9 encontrar a distribui\u00e7\u00e3o de U mesmo quando \\mu \\neq \\mu_0 . Seja W e Y vari\u00e1veis alet\u00f3rias independentes com distribui\u00e7\u00e3o N(\\psi, 1) e \\chi^2(m) , respectivamente. Ent\u00e3o X = \\frac{W}{\\left(\\frac{Y}{m}\\right)^{1/2}} tem distribui\u00e7\u00e3o t n\u00e3o central com m graus de liberdade e n\u00e3o centralidade \\psi . Denotaremos T_m(x|\\psi) a cdf dessa distribui\u00e7\u00e3o. Teorema (Fun\u00e7\u00e3o Poder) Seja X_1, ..., X_n amostra aleat\u00f3ria de N(\\mu,\\sigma^2) . A distribui\u00e7\u00e3o de U \u00e9 t n\u00e3o central com n-1 graus de liberdade e par\u00e2metro de n\u00e3o centralidade \\psi = n^{1/2}(\\mu - \\mu_0)/\\sigma ( Observe que isso ocorre porque dividimos o numerador e o denominador por \\sigma . Al\u00e9m disso, note que X n\u00e3o \u00e9 uma quantidade pivotal, dado que sua distribui\u00e7\u00e3o depende de par\u00e2metros desconhecidos ) Suponha que o procedimento \\delta rejeita H_0: \\mu \\le \\mu_0 se U \\ge c . Ent\u00e3o a fun\u00e7\u00e3o poder \u00e9 \\pi(\\mu,\\sigma^2|\\delta) = 1 - T_{n-1}(c,\\psi) Se \\delta ' rejeita H_0: \\mu \\ge \\mu_0 se U \\le c . Ent\u00e3o a fun\u00e7\u00e3o poder \u00e9 \\pi(\\mu,\\sigma^2|\\delta) = T_{n-1}(c,\\psi) from scipy.stats import nct #noncentral t dsitribution from matplotlib import animation from IPython.display import HTML import warnings warnings . filterwarnings ( 'ignore' ) n = 10 mu0 = 5 sigma = 2 psi = lambda mu : np . sqrt ( n ) * ( mu - mu0 ) / sigma X = nct ( df = n - 1 , nc = psi ( - 20 )) Vamos ver o que acontece quando variamos \\mu . Nesse caso -20 \\leq \\mu \\geq 20 . fig , ax = plt . subplots () x = np . linspace ( X . ppf ( 0.01 ), X . ppf ( 0.99 ), 100 ) line , = ax . plot ( x , X . pdf ( x ), 'r-' , lw = 5 , alpha = 0.6 ) ax . set_xlim (( - 60 , 60 )) ax . set_ylim (( 0 , 0.3 )) ax . set_title ( 't n\u00e3o central' ) def animate ( i , n ): x = np . linspace ( - 60 , 60 , 100 ) line . set_data ( x , nct . pdf ( x , df = n - 1 , nc = psi ( i - 20 ))) return line , HTML ( animation . FuncAnimation ( fig , animate , frames = 40 , interval = 100 , fargs = ( n ,), repeat = False ) . to_html5_video ()) Your browser does not support the video tag. Alternativa Bilateral Tome agora a hip\u00f3tese H_0: \\mu = \\mu_0 H_1: \\mu \\neq \\mu_0 Podemos usar a mesma estat\u00edstica U , mas agora que temos dois lados, vamos fazer o seguinte processo (vou construir de forma intuitiva, no livro tem uma formaliza\u00e7\u00e3o): O procedimento de teste \u00e9 do tipo: Rejeitamos H_0 se U \\le c_1 ou U \\ge c_2 . Vamos considerar c_1 = -c e c_2 = c , para simplificar. Seja \\alpha_0 o tamanho do teste, isto \u00e9, a probabilidade de rejeitarmos a hip\u00f3tese nula quando \\mu = \\mu_0 . Quando \\mu = \\mu_0 , U tem distribui\u00e7\u00e3o t com n-1 graus de liberdade. Assim: P(|U| \\ge c|\\mu = \\mu_0) = \\alpha_0 = P(U \\le -c) + P(U \\ge c) \\overset{simetria}{=} 2P( U \\ge c) = 2(1 - P(U \\le c)) n = 20 alpha0 = 0.05 c = t . ppf ( df = n - 1 , q = 1 - alpha0 / 2 ) X = t ( df = n - 1 ) x = np . arange ( - 5 , 5 , 0.1 ) plt . plot ( x , X . pdf ( x )) plt . fill_between ( x [( x < - c )], X . pdf ( x [( x < - c )]), color = 'blue' ) plt . fill_between ( x [( x > c )], X . pdf ( x [( x > c )]), color = 'blue' ) plt . title ( 'Distribui\u00e7\u00e3o de U e Regi\u00e3o de Rejei\u00e7\u00e3o' ) plt . show () Fun\u00e7\u00e3o Poder \\pi(\\mu,\\sigma^2,|\\delta) = T_{n-1}(-x|\\psi) + 1 - T_{n-1}(c|\\psi) P-valor Seja u o valor observado da vari\u00e1vel U . Vamos lembrar que o p-valor \u00e9 o menor tamanho \\alpha_0 tal que se rejeita a hip\u00f3tese com esse valor observado. Como s\u00f3 rejeitamos se: |u| \\ge c = T_{n-1}^{-1}(1 - \\alpha_0/2) \\implies \\alpha_0 \\ge 2 - 2T_{n-1}(|u|) Logo o p-valor \u00e9 2 - 2T_{n-1}(|u|) . Comparando m\u00e9dias de duas normais Assumimos que X = (X_1,...,X_m) \u00e9 uma amostra da distribui\u00e7\u00e3o normal com m\u00e9dia \\mu_1 e vari\u00e2ncia \\sigma^2 , enquanto Y = (Y_1, ..., Y_n) \u00e9 normal com m\u00e9dia \\mu_2 e vari\u00e2ncia \\sigma^2 . Estamos interessados no teste H_0: \\mu_1 \\le \\mu_2 H_1: \\mu_1 > \\mu_2 A fun\u00e7\u00e3o poder \u00e9 dada por \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) . A discuss\u00e3o quando as normais tem diferentes normais ser\u00e1 postergada. Defina S_x = \\sum_{i=1}^m (X_i - \\bar{X}_m)^2 S_y = \\sum_{i=1}^n (Y_i - \\bar{Y}_n)^2 U = \\frac{(m + n - 2)^{1/2}(\\bar{X}_m - \\bar{Y}_n)}{\\left(\\frac{1}{n} + \\frac{1}{m}\\right)^{1/2}(S_x^2 + S_y^2)^{1/2}} A distribui\u00e7\u00e3o: U \\sim t com m + n - 2 graus de liberdade, com par\u00e2metro de n\u00e3o centralidade \\psi= \\frac{\\mu_1 - \\mu_2}{\\sigma\\left(\\frac{1}{m} + \\frac{1}{n}\\right)^{1/2}} Note que se \\mu_1 = \\mu_2 , U \u00e9 uma distribui\u00e7\u00e3o t padr\u00e3o. Fun\u00e7\u00e3o Poder Considere o procedimento de teste \\delta que rejeite H_0 se U \\ge T_{m+n-2}^{-1}(1 - \\alpha_0) . Assim: \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) = \\alpha_0 , quando \\mu_1 = \\mu_2 . \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) < \\alpha_0 , quando \\mu_1 < \\mu_2 . \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) > \\alpha_0 , quando \\mu_1 > \\mu_2 . \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) \\to 0 , quando \\mu_1 - \\mu_2 \\to -\\infty . \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) \\to 1 , quando \\mu_1 - \\mu_2 \\to \\infty . Al\u00e9m do mais o teste \u00e9 n\u00e3o enviesado. P-valor Depois de termos observado as amostras, seja u a estat\u00edstica observada de U . O p-valor da hip\u00f3tese \u00e9 1 - T_{m+n-2}(u) . Equivalentemente com o teste t do item 3, podemos expressar tudo com a hip\u00f3tese bilateral e s\u00f3 altera o graude liberade quando comparado com o teste t anterior. Vari\u00e2ncias diferentes Raz\u00e3o entre as vari\u00e2ncias \u00e9 conhecida Suponha que se as vari\u00e2ncias de X e Y s\u00e3o \\sigma_1^2 e \\sigma_2^2 e que \\sigma^2_2 = k\\sigma^2_1, k > 0 . Ent\u00e3o podemos usar a estat\u00edstica U = \\frac{(m + n - 2)^{1/2}(\\bar{X}_m - \\bar{Y}_n)}{\\left(\\frac{1}{n} + \\frac{k}{m}\\right)^{1/2}(S_x^2 + \\frac{S_y^2}{k})^{1/2}} Problema de Behrens-Fisher Quando os 4 par\u00e2metros das normais s\u00e3o desconhecidos, t\u00e3o pouco a raz\u00e3o de vari\u00e2ncias, nem a estat\u00edstica de raz\u00e3o de verossimilhan\u00e7a tem distribui\u00e7\u00e3o conhecida. Algumas tentativas j\u00e1 foram feitas, como Welch e outros . Comparando vari\u00e2ncias de duas Normais Assumimos que X = (X_1,...,X_m) \u00e9 uma amostra da distribui\u00e7\u00e3o normal com m\u00e9dia \\mu_1 e vari\u00e2ncia \\sigma^2 , enquanto Y = (Y_1, ..., Y_n) \u00e9 normal com m\u00e9dia \\mu_2 e vari\u00e2ncia \\sigma^2 . Estamos interessados no teste H_0: \\sigma_1^2 \\le \\sigma_2^2 H_1: \\sigma_1^2 > \\sigma_2^2 A fun\u00e7\u00e3o poder \u00e9 dada por \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) . COnsidere S_x^2 e S_y^2 definidos anteriormente. Ent\u00e3o temos que S_x^2/(m-1) \u00e9 estimador para \\sigma_1^2 , enquanto S_y^2/(n-1) \u00e9 estimador para \\sigma_2^2 . Defina V = \\frac{S_x^2/(m-1)}{S_y^2/(n-1)} Rejeitaremos X_0 se V \\ge c , onde c ser\u00e1 escolhido para que esse teste tenha n\u00edvel de signific\u00e2ncia \\alpha_0 . Esse teste \u00e9 chamado de teste F, pois a distribui\u00e7\u00e3o de (\\sigma_1^2/\\sigma_2^2)V \u00e9 F com par\u00e2metros m-1 e n-1 . Em particular se \\sigma_1^2 = \\sigma_2^2 , V tem distribui\u00e7\u00e3o F. Onde a distribui\u00e7\u00e3o F \u00e9 descrita aqui . Fun\u00e7\u00e3o Poder Considere o procedimento de teste \\delta que rejeite H_0 se V \\ge F_{m-1,n-1}^{-1}(1 - \\alpha_0) . Assim: \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) = 1 - F_{m-1,n-1}(\\frac{\\sigma_2^2}{\\sigma_1^2}c) \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) = \\alpha_0 , quando \\sigma_1^2 = \\sigma^2_2 . \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) < \\alpha_0 , quando \\sigma_1^2 < \\sigma_2^2 . \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) > \\alpha_0 , quando \\sigma^2_1 > \\sigma_2^2 . \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) \\to 0 , quando \\sigma_1^2/\\sigma_2^2 \\to 0 . \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) \\to 1 , quando \\sigma_1^2/\\sigma_2^2 \\to \\infty . Al\u00e9m do mais o teste \u00e9 n\u00e3o enviesado. P-valor Depois de termos observado as amostras, seja v a estat\u00edstica observada de V . O p-valor da hip\u00f3tese \u00e9 1 - F_{m-1,n-1}(v) .","title":"Teste de Hip\u00f3teses II"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#teste-de-hipoteses-ii","text":"Nesse notebook veremos: Teste de Hip\u00f3teses Simples Hip\u00f3tese Alternativa Bilateral Teste T Comparando m\u00e9dias de duas Normais Comparando vari\u00e2ncias de duas Normais","title":"Teste de Hip\u00f3teses II"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#teste-de-hipotese-simples","text":"O objetivo \u00e9 considerar se um vetor de observa\u00e7\u00f5es vem de uma entre duas observa\u00e7\u00f5es. Nesse caso o espa\u00e7o \\Omega \u00e9 formado por dois pontos, e n\u00e3o \u00e9 um espa\u00e7o de par\u00e2metros, mas espa\u00e7o de distribui\u00e7\u00f5es, em particular dessas duas distribui\u00e7\u00f5es. Isto \u00e9, vamos assumir que X = (X_1, ..., X_n) vem de f_0(x) ou f_1(x) . Assim \\Omega = \\{\\theta_0, \\theta_1\\} e \\theta = \\theta_i se os dados tem distribui\u00e7\u00e3o f_i(x), i = 0,1 . Vamos denotar: \\alpha(\\delta) = P(\\text{Rejeitar} H_0|\\theta = \\theta_0) = P(\\text{Erro I}) \\beta(\\delta) = P(\\text{N\u00e3o rejeitar} H_0|\\theta = \\theta_1) = P(\\text{Erro II})","title":"Teste de Hip\u00f3tese Simples"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#teorema","text":"Seja \\delta^* o procedimento de teste que n\u00e3o rejeita H_0 se af_0(x) > bf_1(x) e rejeita se af_0(x) < bf_1(x) . Ent\u00e3o, para todo outro procedimento de teste \\delta , a\\alpha(\\delta^*) + b\\beta(\\delta^*) \\le a\\alpha(\\delta) + b\\beta(\\delta) Queremos escolher um teste que minimize essa combina\u00e7\u00e3o linear a\\alpha(\\delta) + b\\beta(\\delta) . Claro que seria \u00f3timo ter esse erro zerado, mas sabemos que existe uma esp\u00e9cie de trade off entre esses erros. Esse teorema d\u00e1 o teste necess\u00e1rio para que isso aconten\u00e7a.","title":"Teorema"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#corolario","text":"Considere as hip\u00f3teses do teorema anterior, a > 0 e b > 0 . Defina estat\u00edstica de teste raz\u00e3o de verossimilhan\u00e7a : \\Lambda(x) = \\begin{cases} \\frac{f_0(x)}{f_1(x)}, \\text{ se } f_0(x) \\le f_1(x) \\\\ 1, \\text{ caso contr\u00e1rio }. \\end{cases} Defina o procedimento de teste \\delta : Rejeita H_0 se \\Lambda(x) > a/b . Ent\u00e3o o valor de af_0(x) + bf_1(x) \u00e9 m\u00ednimo.","title":"Corol\u00e1rio"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#lema-nayman-pearson","text":"Suponha que \\delta ' tem a seguinte forma, para algum k > 0 : H_0 n\u00e3o \u00e9 rejeitada se f_1(x) < kf_0(x) e o \u00e9 quando f_1(x) > kf_0(x). Se \\delta \u00e9 outro procedimento de teste tal que \\alpha(\\delta) \\le \\alpha(\\delta ') , ent\u00e3o \\beta(\\delta) \\ge \\beta(\\delta ').","title":"Lema Nayman-Pearson"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#implementacao","text":"Vamos fazer uma simples implementa\u00e7\u00e3o de uso para esse tipo de problema. import numpy as np from scipy.stats import bernoulli , binom from scipy.optimize import brute Nesse caso, vamos fazer uma simples simula\u00e7\u00e3o, onde um par\u00e2metro de uma distribui\u00e7\u00e3o de Bernoulli pode ser p = 0.4 ou p = 0.6 . Vamos gerar essa amostra, mas sem de fato conhecer p verdadeiro. ro = np . random . RandomState ( 1000000 ) #random state p = ro . choice ([ 0.4 , 0.6 ]) Teremos uma amostra de tamanho n . n = 20 X = ro . binomial ( 1 , p , size = n ) Vamos utilizar o Lema Nayman-Pearson. O objetivo \u00e9 testar as seguintes hip\u00f3teses: H_0: p = 0.4 H_1: p = 0.6 Vamos fixar \\alpha_0 = 0.05 o tamanho do teste. Temos que, se y = \\sum_{i=1}^n x_i \\sim Binomial(n,p) , \\frac{f_1(x)}{f_0(x)} = \\frac{0.6^y0.4^{n-y}}{0.4^y0.6^{n-y}} = \\left(\\frac{3}{2}\\right)^y\\left(\\frac{2}{3}\\right)^{n-y} = \\left(\\frac{3}{2}\\right)^{2y - n} Assim: \\begin{split} 0.05 &= P(f_1(x) > kf_0(x)|p = 0.4) = P\\left(\\left(\\frac{3}{2}\\right)^{2y - n} > k\\right) \\\\ &= P\\left(2y - n > \\frac{\\log(k)}{\\log(3/2)}\\right) \\\\ &= P\\left(y > \\frac{\\log(k)}{2\\log(3/2)} + \\frac{n}{2}\\right), y \\sim Binomial(n,0.4) \\end{split} Isto \u00e9, preciso escolher k que satisfa\u00e7a essa rela\u00e7\u00e3o. Vamos calcular k numericamente utilizando um m\u00e9todo de otimiza\u00e7\u00e3o por bruta for\u00e7a (s\u00e3o poucas as op\u00e7\u00f5es). Como n\u00e3o queremos que seja marior do que 0.05, precisamos colocar peso para que n\u00e3o seja. Veja que existem v\u00e1rios valores de k que satisfazem isso. alpha0 = 0.05 Y = binom ( n = n , p = 0.4 ) func = lambda k , n : np . abs ( 0.95 - Y . cdf (( 1 / 2 ) * np . log ( k ) / np . log ( 3 / 2 ) + n / 2 )) + \\ 10 * ( 0.95 > Y . cdf (( 1 / 2 ) * np . log ( k ) / np . log ( 3 / 2 ) + n / 2 )) k = brute ( func , ranges = ( slice ( 1 , 20 , 1 ),), args = ( n ,))[ 0 ] k 6.0 Por esse motivo, vamos tomar k=6 . Pela Lema de Neyman Pearson, esse teste \u00e9 o que minimiza o Erro do Tipo II. Vamos ver se rejeitamos ou n\u00e3o a hip\u00f3tese nula baseado nos dados obtidos. f0 = lambda x : 0.4 ** ( sum ( x )) * 0.6 ** ( len ( x ) - sum ( x )) f1 = lambda x : 0.6 ** ( sum ( x )) * 0.4 ** ( len ( x ) - sum ( x )) if f1 ( X ) > k * f0 ( X ): print ( r 'Rejeitamos H0.' ) else : print ( r 'N\u00e3o rejeitamos H0.' ) N\u00e3o rejeitamos H0. Vamos ver quem \u00e9 p , ent\u00e3o. print ( 'O valor de p \u00e9 .... ' ) print ( p ) O valor de p \u00e9 .... 0.4 Fizemos bem em n\u00e3o rejeitar a hip\u00f3tese nula!","title":"Implementa\u00e7\u00e3o"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#hipotese-alternativa-bilateral","text":"Seja X = (X_1, ..., X_n) uma amostra aleat\u00f3ria de uma distribui\u00e7\u00e3o normal com m\u00e9dia \\mu desconhecida e vari\u00e2ncia \\sigma^2 conhecida e queremos testar a hip\u00f3tese H_0: \\mu = \\mu_0 H_1: \\mu \\neq \\mu_0 Como \\bar{X}_n \u00e9 um estimador consistente de \\mu , faz sentido rejeitar a hip\u00f3tese nula quando a m\u00e9dia amostral se afasta de \\mu_0 . Para isso, vamos escolher c_1, c_2 de forma que P(\\bar{X}_n \\leq c_1|\\mu = \\mu_0) + P(\\bar{X}_n \\geq c_2|\\mu = \\mu_0) = \\alpha_0 \\Rightarrow P\\left(Z \\leq \\sqrt{n}\\frac{c_1 - \\mu_0}{\\sigma}\\right) + P\\left(Z \\geq \\sqrt{n}\\frac{c_2 - \\mu_0}{\\sigma}\\right) = \\alpha_0 \\Rightarrow \\Phi\\left(\\sqrt{n}\\frac{c_1 - \\mu_0}{\\sigma}\\right) + 1 - \\Phi\\left(\\sqrt{n}\\frac{c_2 - \\mu_0}{\\sigma}\\right) = \\alpha_0 \\Rightarrow \\Phi\\left(\\sqrt{n}\\frac{c_1 - \\mu_0}{\\sigma}\\right) = \\alpha_1 \\text{ e } \\Phi\\left(\\sqrt{n}\\frac{c_2 - \\mu_0}{\\sigma}\\right) = 1 - \\alpha_2, \\text{ com } \\alpha_1 + \\alpha_2 = \\alpha_0 Observa\u00e7\u00e3o: \\bar{X}_n \\sim N(\\mu, \\sigma^2/n) \\Rightarrow Z = \\sqrt{n}\\frac{\\bar{X}_n - \\mu}{\\sigma} \\sim N(0,1) Observa\u00e7\u00e3o 2: No c\u00e1lculo substituimos \\mu por \\mu_0 , porque estamos \"condicionando\" no conhecimento deles serem iguais. Isto \u00e9, queremos que o tamanho do teste seja \\alpha_0 , lembrando que o tamanho do teste \u00e9 o supremo das probabilidades de se rejeitar a hip\u00f3tese nula quando ela \u00e9 verdadeira.","title":"Hip\u00f3tese Alternativa Bilateral"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#teste-t","text":"Suponha que (X_1,...,X_n) \u00e9 uma amostra aleat\u00f3ria da distribui\u00e7\u00e3o N(\\mu,\\sigma^2) , com par\u00e2metros desconhecidos e queremos testar a hip\u00f3tese: H_0: \\mu \\le \\mu_0 \\implies \\Omega_0 = \\{(x,y) \\in \\mathbb{R}^2 | x \\le \\mu_0 \\text{ e } y > 0\\} H_1: \\mu > \\mu_0 \\implies \\Omega_1 = \\{(x,y) \\in \\mathbb{R}^2 | x > \\mu_0 \\text{ e } y > 0\\} Sabemos que U = n^{1/2}\\frac{\\bar{X}_n - \\mu_0}{\\sigma '} \u00e9 uma boa estat\u00edstica de teste e rejeitamos H_0 se U \\ge c . Essa estat\u00edstica \u00e9 interessante porque quando \\mu = \\mu_0, U \\sim t(n-1) . Por isso chamamos de testes t quando baseados na estat\u00edstica U . Podemos tamb\u00e9m inverter os sinais de desigualdade e rejeitar H_0 quando U \\le c . from pandas import DataFrame from scipy.stats import t import matplotlib.pyplot as plt import seaborn as sns sns . set () % matplotlib notebook mu0 = 10 # Vamos escolher mu e sigma de forma aleat\u00f3ria, mas n\u00e3o significa que \u00e9 uma vari\u00e1vel aleat\u00f3ria. n = 20","title":"Teste t"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#distibuicao-de-u","text":"Vamos gerar uma aproxima\u00e7\u00e3o para a distribui\u00e7\u00e3o de U para um determinado \\mu . U_values = {} for i in range ( 6 ): mu = ro . normal ( mu0 , 1 ) if i < 5 else 10 sigma = ro . exponential ( mu0 ) key = 'mu = {} , sigma = {} ' . format ( np . round ( mu , 2 ), np . round ( sigma , 2 )) U_values [ key ] = np . zeros ( 10000 ) for j in range ( 10000 ): X = ro . normal ( mu , sigma , size = n ) U = np . sqrt ( n ) * ( np . mean ( X ) - mu0 ) / np . std ( X , ddof = 1 ) U_values [ key ][ j ] = U U_values = DataFrame ( U_values ) fig , ax = plt . subplots ( figsize = ( 10 , 6 )) sns . kdeplot ( data = U_values , ax = ax ) ax . set_title ( 'Distribui\u00e7\u00e3o aproximada de U' ) plt . show ()","title":"Distibui\u00e7\u00e3o de U"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#teorema_1","text":"Seja c o 1 - \\alpha_0 quartil da distribui\u00e7\u00e3o t com n-1 graus de liberdade. Ent\u00e3o, segundo o teste citado acima, a fun\u00e7\u00e3o poder tem as seguintes propriedades: \\pi(\\mu, \\sigma^2|\\delta) = \\alpha_0 , quando \\mu = \\mu_0 . \\pi(\\mu, \\sigma^2|\\delta) < \\alpha_0 , quando \\mu < \\mu_0 . \\pi(\\mu, \\sigma^2|\\delta) > \\alpha_0 , quando \\mu > \\mu_0 . \\pi(\\mu, \\sigma^2|\\delta) \\to 0 , quando \\mu \\to -\\infty . \\pi(\\mu, \\sigma^2|\\delta) \\to 1 , quando \\mu \\to \\infty . O teste tamb\u00e9m \u00e9 n\u00e3o enviesado como consequ\u00eancia.","title":"Teorema"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#p-valores-para-testes-t","text":"Seja u a estat\u00edstica U quando observada. Seja T_{n-1}(\\cdot) a cdf da distribui\u00e7\u00e3o t com n-1 graus de liberdade. Ent\u00e3o o p-valor para H_0: \\mu \\leq \\mu_0 \u00e9 1 - T_{n-1}(u) , enquanto o p-valor para H_0: \\mu \\ge \\mu_0 \u00e9 T_{n-1}(u) .","title":"P-valores para testes t"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#distribuicao-t-nao-central","text":"O objetivo \u00e9 encontrar a distribui\u00e7\u00e3o de U mesmo quando \\mu \\neq \\mu_0 . Seja W e Y vari\u00e1veis alet\u00f3rias independentes com distribui\u00e7\u00e3o N(\\psi, 1) e \\chi^2(m) , respectivamente. Ent\u00e3o X = \\frac{W}{\\left(\\frac{Y}{m}\\right)^{1/2}} tem distribui\u00e7\u00e3o t n\u00e3o central com m graus de liberdade e n\u00e3o centralidade \\psi . Denotaremos T_m(x|\\psi) a cdf dessa distribui\u00e7\u00e3o.","title":"Distribui\u00e7\u00e3o t n\u00e3o central"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#teorema-funcao-poder","text":"Seja X_1, ..., X_n amostra aleat\u00f3ria de N(\\mu,\\sigma^2) . A distribui\u00e7\u00e3o de U \u00e9 t n\u00e3o central com n-1 graus de liberdade e par\u00e2metro de n\u00e3o centralidade \\psi = n^{1/2}(\\mu - \\mu_0)/\\sigma ( Observe que isso ocorre porque dividimos o numerador e o denominador por \\sigma . Al\u00e9m disso, note que X n\u00e3o \u00e9 uma quantidade pivotal, dado que sua distribui\u00e7\u00e3o depende de par\u00e2metros desconhecidos ) Suponha que o procedimento \\delta rejeita H_0: \\mu \\le \\mu_0 se U \\ge c . Ent\u00e3o a fun\u00e7\u00e3o poder \u00e9 \\pi(\\mu,\\sigma^2|\\delta) = 1 - T_{n-1}(c,\\psi) Se \\delta ' rejeita H_0: \\mu \\ge \\mu_0 se U \\le c . Ent\u00e3o a fun\u00e7\u00e3o poder \u00e9 \\pi(\\mu,\\sigma^2|\\delta) = T_{n-1}(c,\\psi) from scipy.stats import nct #noncentral t dsitribution from matplotlib import animation from IPython.display import HTML import warnings warnings . filterwarnings ( 'ignore' ) n = 10 mu0 = 5 sigma = 2 psi = lambda mu : np . sqrt ( n ) * ( mu - mu0 ) / sigma X = nct ( df = n - 1 , nc = psi ( - 20 )) Vamos ver o que acontece quando variamos \\mu . Nesse caso -20 \\leq \\mu \\geq 20 . fig , ax = plt . subplots () x = np . linspace ( X . ppf ( 0.01 ), X . ppf ( 0.99 ), 100 ) line , = ax . plot ( x , X . pdf ( x ), 'r-' , lw = 5 , alpha = 0.6 ) ax . set_xlim (( - 60 , 60 )) ax . set_ylim (( 0 , 0.3 )) ax . set_title ( 't n\u00e3o central' ) def animate ( i , n ): x = np . linspace ( - 60 , 60 , 100 ) line . set_data ( x , nct . pdf ( x , df = n - 1 , nc = psi ( i - 20 ))) return line , HTML ( animation . FuncAnimation ( fig , animate , frames = 40 , interval = 100 , fargs = ( n ,), repeat = False ) . to_html5_video ()) Your browser does not support the video tag.","title":"Teorema  (Fun\u00e7\u00e3o Poder)"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#alternativa-bilateral","text":"Tome agora a hip\u00f3tese H_0: \\mu = \\mu_0 H_1: \\mu \\neq \\mu_0 Podemos usar a mesma estat\u00edstica U , mas agora que temos dois lados, vamos fazer o seguinte processo (vou construir de forma intuitiva, no livro tem uma formaliza\u00e7\u00e3o): O procedimento de teste \u00e9 do tipo: Rejeitamos H_0 se U \\le c_1 ou U \\ge c_2 . Vamos considerar c_1 = -c e c_2 = c , para simplificar. Seja \\alpha_0 o tamanho do teste, isto \u00e9, a probabilidade de rejeitarmos a hip\u00f3tese nula quando \\mu = \\mu_0 . Quando \\mu = \\mu_0 , U tem distribui\u00e7\u00e3o t com n-1 graus de liberdade. Assim: P(|U| \\ge c|\\mu = \\mu_0) = \\alpha_0 = P(U \\le -c) + P(U \\ge c) \\overset{simetria}{=} 2P( U \\ge c) = 2(1 - P(U \\le c)) n = 20 alpha0 = 0.05 c = t . ppf ( df = n - 1 , q = 1 - alpha0 / 2 ) X = t ( df = n - 1 ) x = np . arange ( - 5 , 5 , 0.1 ) plt . plot ( x , X . pdf ( x )) plt . fill_between ( x [( x < - c )], X . pdf ( x [( x < - c )]), color = 'blue' ) plt . fill_between ( x [( x > c )], X . pdf ( x [( x > c )]), color = 'blue' ) plt . title ( 'Distribui\u00e7\u00e3o de U e Regi\u00e3o de Rejei\u00e7\u00e3o' ) plt . show ()","title":"Alternativa Bilateral"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#funcao-poder","text":"\\pi(\\mu,\\sigma^2,|\\delta) = T_{n-1}(-x|\\psi) + 1 - T_{n-1}(c|\\psi)","title":"Fun\u00e7\u00e3o Poder"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#p-valor","text":"Seja u o valor observado da vari\u00e1vel U . Vamos lembrar que o p-valor \u00e9 o menor tamanho \\alpha_0 tal que se rejeita a hip\u00f3tese com esse valor observado. Como s\u00f3 rejeitamos se: |u| \\ge c = T_{n-1}^{-1}(1 - \\alpha_0/2) \\implies \\alpha_0 \\ge 2 - 2T_{n-1}(|u|) Logo o p-valor \u00e9 2 - 2T_{n-1}(|u|) .","title":"P-valor"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#comparando-medias-de-duas-normais","text":"Assumimos que X = (X_1,...,X_m) \u00e9 uma amostra da distribui\u00e7\u00e3o normal com m\u00e9dia \\mu_1 e vari\u00e2ncia \\sigma^2 , enquanto Y = (Y_1, ..., Y_n) \u00e9 normal com m\u00e9dia \\mu_2 e vari\u00e2ncia \\sigma^2 . Estamos interessados no teste H_0: \\mu_1 \\le \\mu_2 H_1: \\mu_1 > \\mu_2 A fun\u00e7\u00e3o poder \u00e9 dada por \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) . A discuss\u00e3o quando as normais tem diferentes normais ser\u00e1 postergada. Defina S_x = \\sum_{i=1}^m (X_i - \\bar{X}_m)^2 S_y = \\sum_{i=1}^n (Y_i - \\bar{Y}_n)^2 U = \\frac{(m + n - 2)^{1/2}(\\bar{X}_m - \\bar{Y}_n)}{\\left(\\frac{1}{n} + \\frac{1}{m}\\right)^{1/2}(S_x^2 + S_y^2)^{1/2}} A distribui\u00e7\u00e3o: U \\sim t com m + n - 2 graus de liberdade, com par\u00e2metro de n\u00e3o centralidade \\psi= \\frac{\\mu_1 - \\mu_2}{\\sigma\\left(\\frac{1}{m} + \\frac{1}{n}\\right)^{1/2}} Note que se \\mu_1 = \\mu_2 , U \u00e9 uma distribui\u00e7\u00e3o t padr\u00e3o.","title":"Comparando m\u00e9dias de duas normais"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#funcao-poder_1","text":"Considere o procedimento de teste \\delta que rejeite H_0 se U \\ge T_{m+n-2}^{-1}(1 - \\alpha_0) . Assim: \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) = \\alpha_0 , quando \\mu_1 = \\mu_2 . \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) < \\alpha_0 , quando \\mu_1 < \\mu_2 . \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) > \\alpha_0 , quando \\mu_1 > \\mu_2 . \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) \\to 0 , quando \\mu_1 - \\mu_2 \\to -\\infty . \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) \\to 1 , quando \\mu_1 - \\mu_2 \\to \\infty . Al\u00e9m do mais o teste \u00e9 n\u00e3o enviesado.","title":"Fun\u00e7\u00e3o Poder"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#p-valor_1","text":"Depois de termos observado as amostras, seja u a estat\u00edstica observada de U . O p-valor da hip\u00f3tese \u00e9 1 - T_{m+n-2}(u) . Equivalentemente com o teste t do item 3, podemos expressar tudo com a hip\u00f3tese bilateral e s\u00f3 altera o graude liberade quando comparado com o teste t anterior.","title":"P-valor"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#variancias-diferentes","text":"","title":"Vari\u00e2ncias diferentes"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#razao-entre-as-variancias-e-conhecida","text":"Suponha que se as vari\u00e2ncias de X e Y s\u00e3o \\sigma_1^2 e \\sigma_2^2 e que \\sigma^2_2 = k\\sigma^2_1, k > 0 . Ent\u00e3o podemos usar a estat\u00edstica U = \\frac{(m + n - 2)^{1/2}(\\bar{X}_m - \\bar{Y}_n)}{\\left(\\frac{1}{n} + \\frac{k}{m}\\right)^{1/2}(S_x^2 + \\frac{S_y^2}{k})^{1/2}}","title":"Raz\u00e3o entre as vari\u00e2ncias \u00e9 conhecida"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#problema-de-behrens-fisher","text":"Quando os 4 par\u00e2metros das normais s\u00e3o desconhecidos, t\u00e3o pouco a raz\u00e3o de vari\u00e2ncias, nem a estat\u00edstica de raz\u00e3o de verossimilhan\u00e7a tem distribui\u00e7\u00e3o conhecida. Algumas tentativas j\u00e1 foram feitas, como Welch e outros .","title":"Problema de Behrens-Fisher"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#comparando-variancias-de-duas-normais","text":"Assumimos que X = (X_1,...,X_m) \u00e9 uma amostra da distribui\u00e7\u00e3o normal com m\u00e9dia \\mu_1 e vari\u00e2ncia \\sigma^2 , enquanto Y = (Y_1, ..., Y_n) \u00e9 normal com m\u00e9dia \\mu_2 e vari\u00e2ncia \\sigma^2 . Estamos interessados no teste H_0: \\sigma_1^2 \\le \\sigma_2^2 H_1: \\sigma_1^2 > \\sigma_2^2 A fun\u00e7\u00e3o poder \u00e9 dada por \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) . COnsidere S_x^2 e S_y^2 definidos anteriormente. Ent\u00e3o temos que S_x^2/(m-1) \u00e9 estimador para \\sigma_1^2 , enquanto S_y^2/(n-1) \u00e9 estimador para \\sigma_2^2 . Defina V = \\frac{S_x^2/(m-1)}{S_y^2/(n-1)} Rejeitaremos X_0 se V \\ge c , onde c ser\u00e1 escolhido para que esse teste tenha n\u00edvel de signific\u00e2ncia \\alpha_0 . Esse teste \u00e9 chamado de teste F, pois a distribui\u00e7\u00e3o de (\\sigma_1^2/\\sigma_2^2)V \u00e9 F com par\u00e2metros m-1 e n-1 . Em particular se \\sigma_1^2 = \\sigma_2^2 , V tem distribui\u00e7\u00e3o F. Onde a distribui\u00e7\u00e3o F \u00e9 descrita aqui .","title":"Comparando vari\u00e2ncias de duas Normais"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#funcao-poder_2","text":"Considere o procedimento de teste \\delta que rejeite H_0 se V \\ge F_{m-1,n-1}^{-1}(1 - \\alpha_0) . Assim: \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) = 1 - F_{m-1,n-1}(\\frac{\\sigma_2^2}{\\sigma_1^2}c) \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) = \\alpha_0 , quando \\sigma_1^2 = \\sigma^2_2 . \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) < \\alpha_0 , quando \\sigma_1^2 < \\sigma_2^2 . \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) > \\alpha_0 , quando \\sigma^2_1 > \\sigma_2^2 . \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) \\to 0 , quando \\sigma_1^2/\\sigma_2^2 \\to 0 . \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) \\to 1 , quando \\sigma_1^2/\\sigma_2^2 \\to \\infty . Al\u00e9m do mais o teste \u00e9 n\u00e3o enviesado.","title":"Fun\u00e7\u00e3o Poder"},{"location":"infestatistica/TestingHypothesesII/TestingHypothesesII/#p-valor_2","text":"Depois de termos observado as amostras, seja v a estat\u00edstica observada de V . O p-valor da hip\u00f3tese \u00e9 1 - F_{m-1,n-1}(v) .","title":"P-valor"},{"location":"infestatistica/TestsUniformlyPoweful/TestsUniformlyPoweful/","text":"Testes Uniformemente mais Poderosos Estamos lidando com um teste de hip\u00f3teses com as vari\u00e1veis aleat\u00f3rias X_1, ..., X_n de uma distribui\u00e7\u00e3o parametrizada em \\theta desconhecido. H_0: \\theta \\in \\Omega_0 H_1: \\theta \\in \\Omega_1 Assumiremos que \\Omega_1 n\u00e3o \u00e9 conjunto unit\u00e1rio. Tamb\u00e9m suponha que o n\u00edvel de signific\u00e2ncia do teste seja \\alpha_0 , isto \u00e9 \\pi(\\theta|\\delta) \\le \\alpha_0, \\forall \\theta \\in \\Omega_0 . Segundo essas condi\u00e7\u00f5es, queremos encontrar o procedimento de teste \\delta que tem a menor probabilidade de erro do tipo II. TODO (Descrever os principais resultados)","title":"Testes Uniformemente mais Poderosos"},{"location":"infestatistica/TestsUniformlyPoweful/TestsUniformlyPoweful/#testes-uniformemente-mais-poderosos","text":"Estamos lidando com um teste de hip\u00f3teses com as vari\u00e1veis aleat\u00f3rias X_1, ..., X_n de uma distribui\u00e7\u00e3o parametrizada em \\theta desconhecido. H_0: \\theta \\in \\Omega_0 H_1: \\theta \\in \\Omega_1 Assumiremos que \\Omega_1 n\u00e3o \u00e9 conjunto unit\u00e1rio. Tamb\u00e9m suponha que o n\u00edvel de signific\u00e2ncia do teste seja \\alpha_0 , isto \u00e9 \\pi(\\theta|\\delta) \\le \\alpha_0, \\forall \\theta \\in \\Omega_0 . Segundo essas condi\u00e7\u00f5es, queremos encontrar o procedimento de teste \\delta que tem a menor probabilidade de erro do tipo II. TODO (Descrever os principais resultados)","title":"Testes Uniformemente mais Poderosos"},{"location":"infestatistica/UnbiasedEstimators/UnbiasedEstimators/","text":"Estimadores n\u00e3o enviesados Defini\u00e7\u00e3o Um estimador \\delta(X) \u00e9 dito n\u00e3o enviesado para g(\\theta) se E_{\\theta}[\\delta(X)] = g(\\theta) para todo valor de \\theta . O vi\u00e9s do estimador \u00e9 definido por E_{\\theta}[\\delta(X)] - g(\\theta) . Se \\delta \u00e9 um estimador com vari\u00e2ncia finita, ent\u00e3o: R(\\theta, \\delta) = Var(\\delta) + Vi\u00e9s(\\delta)^2 Estimador n\u00e3o enviesado para vari\u00e2ncia s^2 = \\hat{\\sigma}_1^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2 # Importando bibliotecas import numpy as np import pandas as pd from IPython.display import display , Math # Display latex import matplotlib.pyplot as plt % matplotlib inline np . random . seed ( 1000 ) # Garantindo reprodutibilidade Nota: Por que garantir reprodutibilidade? Reprodutibilidade \u00e9 a ideia de tornar o processo que foi feito por voc\u00ea reprodut\u00edvel por qualquer outra pessoa, para que ela possa obter os mesmos resultados seguindo os mesmos passos que voc\u00ea. Quando escolhemos um n\u00famero aleat\u00f3rio ( pseudoaleat\u00f3rio na verdade), ele muda de tempos em tempos. Mas isso vai tirar a ideia de \"garantir os mesmos resultados\". O resultado pode ser parecido, mas n\u00e3o exatamente igual. Isso \u00e9 muito importante no meio cient\u00edfico. Exemplo Vamos ver como se comporta esse estimador n\u00e3o viesado em uma popula\u00e7\u00e3o que representa o Brasil todo! Veja que eu n\u00e3o peguei dados online, porque quero TODA a popula\u00e7\u00e3o. Por isso vamos fazer uma simula\u00e7\u00e3o. A m\u00e9dia verdadeira da distribui\u00e7\u00e3o \u00e9 161,1cm e o desvio padr\u00e3o \u00e9 10cm. # Tamanho da popula\u00e7\u00e3o N = int ( 200e5 ) # Popula\u00e7\u00e3o gerada por simula\u00e7\u00e3o, usando a distribui\u00e7\u00e3o normal. population_height = pd . Series ( np . random . normal ( loc = 161.1 , scale = 10 , size = N )) Podemos ver a m\u00e9dia dessa popula\u00e7\u00e3o. population_height . mean () 161.10097546939974 O que a fun\u00e7\u00e3o var do pandas faz? Vamos comparar com o estimador trivial. ddof = 1 # Se ddof = 0, teremos a divis\u00e3o por N population_height . var ( ddof = ddof ) 100.02715920849081 Divindindo por N sigma_square_hat = (( population_height - population_height . mean ()) ** 2 ) . sum () / N sigma1_square_hat = (( population_height - population_height . mean ()) ** 2 ) . sum () / ( N - 1 ) display ( Math ( r '\\hat\\sigma^2 = {} ' . format ( sigma_square_hat ))) display ( Math ( r '\\hat\\sigma_1^2 = {} ' . format ( sigma1_square_hat ))) \\displaystyle \\hat\\sigma^2 = 100.02715420713488 \\displaystyle \\hat\\sigma_1^2 = 100.02715920849283 Estima\u00e7\u00e3o dos Par\u00e2metros Vamos supor que n\u00e3o conhecemos os par\u00e2metros da nossa popula\u00e7\u00e3o e podemos conhecer apenas uma amostra aleat\u00f3ria dela. Vamos fazer 500 dessas simula\u00e7\u00f5es number_simulations = 500 sample_size = 30 sample = pd . DataFrame ( population_height . sample ( n = number_simulations * sample_size , replace = True , random_state = 100 ), columns = [ 'height' ]) reshape = sample . to_numpy () . reshape (( - 1 , 30 )) samples = pd . DataFrame ( reshape , columns = range ( 0 , 30 )) Vamos estimar a m\u00e9dia com a m\u00e9dia amostral que \u00e9 n\u00e3o viesada tamb\u00e9m! Al\u00e9m disso ela \u00e9 o MLE. Estamos estimado para cada amostra a m\u00e9dia! Se fizermos uma m\u00e9dia das m\u00e9dias, veremos que ela chegar\u00e1 pr\u00f3ximo a m\u00e9dia verdadeira. estimated_mean = samples . mean ( axis = 1 ) #Axis = 1 faz a m\u00e9dia por linha. estimated_mean_of_means = estimated_mean . expanding () . mean () fig , ax = plt . subplots ( figsize = ( 8 , 5 )) ax . plot ( estimated_mean_of_means , label = 'Valor estimado' ) ax . hlines ( population_height . mean (), xmin = 0 , xmax = number_simulations , color = 'grey' , linestyle = '--' , alpha = 0.8 , label = 'Valor verdadeiro' ) ax . grid ( alpha = 0.4 ) ax . set_title ( 'Estimado a m\u00e9dia verdadeira' ) plt . show () Vamos comparar os estimadores para a vari\u00e2ncia, o viesado e o n\u00e3o viesado. df = pd . DataFrame ({ 'enviesado (dividido por n)' : samples . var ( ddof = 0 , axis = 1 ) . expanding () . mean (), 'nao_viesado (dividido por n - 1)' : samples . var ( ddof = 1 , axis = 1 ) . expanding () . mean (), 'verdadeiro' : pd . Series ( population_height . var ( ddof = 0 ), index = samples . index )}) ax = df . plot () ax . set_title ( 'Estimadores para a vari\u00e2ncia' ) ax . grid ( alpha = 0.4 ) plt . show () # Comparar com Conscist\u00eancia","title":"Estimadores n\u00e3o enviesados"},{"location":"infestatistica/UnbiasedEstimators/UnbiasedEstimators/#estimadores-nao-enviesados","text":"","title":"Estimadores n\u00e3o enviesados"},{"location":"infestatistica/UnbiasedEstimators/UnbiasedEstimators/#definicao","text":"Um estimador \\delta(X) \u00e9 dito n\u00e3o enviesado para g(\\theta) se E_{\\theta}[\\delta(X)] = g(\\theta) para todo valor de \\theta . O vi\u00e9s do estimador \u00e9 definido por E_{\\theta}[\\delta(X)] - g(\\theta) . Se \\delta \u00e9 um estimador com vari\u00e2ncia finita, ent\u00e3o: R(\\theta, \\delta) = Var(\\delta) + Vi\u00e9s(\\delta)^2","title":"Defini\u00e7\u00e3o"},{"location":"infestatistica/UnbiasedEstimators/UnbiasedEstimators/#estimador-nao-enviesado-para-variancia","text":"s^2 = \\hat{\\sigma}_1^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2 # Importando bibliotecas import numpy as np import pandas as pd from IPython.display import display , Math # Display latex import matplotlib.pyplot as plt % matplotlib inline np . random . seed ( 1000 ) # Garantindo reprodutibilidade","title":"Estimador n\u00e3o enviesado para vari\u00e2ncia"},{"location":"infestatistica/UnbiasedEstimators/UnbiasedEstimators/#nota-por-que-garantir-reprodutibilidade","text":"Reprodutibilidade \u00e9 a ideia de tornar o processo que foi feito por voc\u00ea reprodut\u00edvel por qualquer outra pessoa, para que ela possa obter os mesmos resultados seguindo os mesmos passos que voc\u00ea. Quando escolhemos um n\u00famero aleat\u00f3rio ( pseudoaleat\u00f3rio na verdade), ele muda de tempos em tempos. Mas isso vai tirar a ideia de \"garantir os mesmos resultados\". O resultado pode ser parecido, mas n\u00e3o exatamente igual. Isso \u00e9 muito importante no meio cient\u00edfico.","title":"Nota: Por que garantir reprodutibilidade?"},{"location":"infestatistica/UnbiasedEstimators/UnbiasedEstimators/#exemplo","text":"Vamos ver como se comporta esse estimador n\u00e3o viesado em uma popula\u00e7\u00e3o que representa o Brasil todo! Veja que eu n\u00e3o peguei dados online, porque quero TODA a popula\u00e7\u00e3o. Por isso vamos fazer uma simula\u00e7\u00e3o. A m\u00e9dia verdadeira da distribui\u00e7\u00e3o \u00e9 161,1cm e o desvio padr\u00e3o \u00e9 10cm. # Tamanho da popula\u00e7\u00e3o N = int ( 200e5 ) # Popula\u00e7\u00e3o gerada por simula\u00e7\u00e3o, usando a distribui\u00e7\u00e3o normal. population_height = pd . Series ( np . random . normal ( loc = 161.1 , scale = 10 , size = N )) Podemos ver a m\u00e9dia dessa popula\u00e7\u00e3o. population_height . mean () 161.10097546939974 O que a fun\u00e7\u00e3o var do pandas faz? Vamos comparar com o estimador trivial. ddof = 1 # Se ddof = 0, teremos a divis\u00e3o por N population_height . var ( ddof = ddof ) 100.02715920849081 Divindindo por N sigma_square_hat = (( population_height - population_height . mean ()) ** 2 ) . sum () / N sigma1_square_hat = (( population_height - population_height . mean ()) ** 2 ) . sum () / ( N - 1 ) display ( Math ( r '\\hat\\sigma^2 = {} ' . format ( sigma_square_hat ))) display ( Math ( r '\\hat\\sigma_1^2 = {} ' . format ( sigma1_square_hat ))) \\displaystyle \\hat\\sigma^2 = 100.02715420713488 \\displaystyle \\hat\\sigma_1^2 = 100.02715920849283","title":"Exemplo"},{"location":"infestatistica/UnbiasedEstimators/UnbiasedEstimators/#estimacao-dos-parametros","text":"Vamos supor que n\u00e3o conhecemos os par\u00e2metros da nossa popula\u00e7\u00e3o e podemos conhecer apenas uma amostra aleat\u00f3ria dela. Vamos fazer 500 dessas simula\u00e7\u00f5es number_simulations = 500 sample_size = 30 sample = pd . DataFrame ( population_height . sample ( n = number_simulations * sample_size , replace = True , random_state = 100 ), columns = [ 'height' ]) reshape = sample . to_numpy () . reshape (( - 1 , 30 )) samples = pd . DataFrame ( reshape , columns = range ( 0 , 30 )) Vamos estimar a m\u00e9dia com a m\u00e9dia amostral que \u00e9 n\u00e3o viesada tamb\u00e9m! Al\u00e9m disso ela \u00e9 o MLE. Estamos estimado para cada amostra a m\u00e9dia! Se fizermos uma m\u00e9dia das m\u00e9dias, veremos que ela chegar\u00e1 pr\u00f3ximo a m\u00e9dia verdadeira. estimated_mean = samples . mean ( axis = 1 ) #Axis = 1 faz a m\u00e9dia por linha. estimated_mean_of_means = estimated_mean . expanding () . mean () fig , ax = plt . subplots ( figsize = ( 8 , 5 )) ax . plot ( estimated_mean_of_means , label = 'Valor estimado' ) ax . hlines ( population_height . mean (), xmin = 0 , xmax = number_simulations , color = 'grey' , linestyle = '--' , alpha = 0.8 , label = 'Valor verdadeiro' ) ax . grid ( alpha = 0.4 ) ax . set_title ( 'Estimado a m\u00e9dia verdadeira' ) plt . show () Vamos comparar os estimadores para a vari\u00e2ncia, o viesado e o n\u00e3o viesado. df = pd . DataFrame ({ 'enviesado (dividido por n)' : samples . var ( ddof = 0 , axis = 1 ) . expanding () . mean (), 'nao_viesado (dividido por n - 1)' : samples . var ( ddof = 1 , axis = 1 ) . expanding () . mean (), 'verdadeiro' : pd . Series ( population_height . var ( ddof = 0 ), index = samples . index )}) ax = df . plot () ax . set_title ( 'Estimadores para a vari\u00e2ncia' ) ax . grid ( alpha = 0.4 ) plt . show () # Comparar com Conscist\u00eancia","title":"Estima\u00e7\u00e3o dos Par\u00e2metros"}]}